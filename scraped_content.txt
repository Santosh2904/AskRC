URL: https://rc-docs.northeastern.edu/en/latest/connectingtocluster/index.html#

Connecting To Cluster - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Connecting To Cluster
¶
The following sections on this page will guide you on how to connect to the cluster using command-line access using the terminal and the web interface of Open OnDemand. Guides are provided if your personal computer is running Windows, macOS, or Linux.
Using the Terminal
¶
You connect to the HPC using a
secure shell
program to initiate an SSH session to
sign in to the HPC.
Connecting with Mac
Using the Terminal to connect to Discovery.
Learn more »
Connecting on Mac
Connecting with Windows
Using MobaXTerm to connect to Discvoery.
Learn more »
Connecting on Windows
Connecting with Linux
Using the Terminal to connect to Discovery.
Learn more »
Connecting on Linux
Using Open OnDemand
¶
To learn more about connecting to the cluster using Open OnDemand, please see the Research Computing website on utilizing
Open OnDemand
.
Next
Connecting on Mac
Previous
Home
Copyright © 2024, RC
Made with
Furo
On this page
Connecting To Cluster
Using the Terminal
Using Open OnDemand

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html

Running Jobs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Running Jobs
¶
Running jobs efficiently is at the heart of a successful experience with our High-Performance Computing (HPC) cluster. This section outlines the essential aspects of executing various types of jobs, understanding the queuing system, and troubleshooting.
Understanding the Queuing System
Learn how our job scheduler manages the distribution of computational tasks.
Learn more »
Understanding the Queuing System
Job Scheduling Policies and Priorities
Gain insights into how jobs are prioritized and scheduled within the system.
Learn more »
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Run jobs interactively or submit batch scripts for later execution.
Learn more »
Interactive and Batch Mode
Running Jobs with job-assist
SLURM job submissions with job-assist command.
Learn more »
Running Jobs with job-assist
Running jobs smoothly is key to productive research and development. Explore these topics to understand and make the most of our robust computing infrastructure. If you need assistance, our support team is available at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy computing!
Next
Understanding the Queuing System
Previous
Connecting on Linux
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/index.html

Working with GPUs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Working with GPUs
¶
Harnessing the power of Graphics Processing Units (GPUs) can significantly accelerate your computations on our High-Performance Computing (HPC) cluster. This section provides insights into using GPUs effectively, from understanding their capabilities to best practices for optimization.
GPUs on our HPC
Overview of the GPUs on the HPC.
Learn more »
GPUs on the HPC
GPU Access
How to utilize GPUs interactively (srun) and passively (sbatch).
Learn more »
GPU Access
GPU Job Submission
Using CUDA and building deep learning environments.
Learn more »
GPU Job Submission
Access to the Multi-GPU Partition
Get access to the Multi-GPU partition.
Learn more »
Access to Multi-GPU Partition
Research Computing strives to provide an environment where innovation thrives. Explore these topics to understand and leverage our resources to their fullest. Our support team is available at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
if you need assistance.
Next
GPUs on the HPC
Previous
Running Jobs with job-assist
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html

Data Management - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Data Management
¶
Data management is a critical aspect of utilizing our High-Performance Computing (HPC) environment. This section provides comprehensive guidance on handling data within the cluster, storage options, and secure data transfers.
Data Storage Options
Selecting the appropriate storage option can significantly impact your work’s efficiency.
Learn more »
Data Storage Options
Transfer Data
Transferring data securely and efficiently is fundamental.
Learn more »
Transfer Data
Using Globus
Globus offers a user-friendly interface for transferring large datasets.
Learn more »
Using Globus
Efficient data management is key to a productive experience in the HPC environment. Explore the topics above to gain insights, follow best practices, and utilize our state-of-the-art tools. For further assistance, please contact our support team at
rchelp
@
northeastern
.
edu
or refer to our comprehensive
Frequently Asked Questions (FAQs)
section.
Happy data handling!
Next
Data Storage Options
Previous
Access to Multi-GPU Partition
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/index.html

Software Overview - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Software Overview
¶
Cluster Software
¶
System-wide Software
Cluster’s nuts and bolts.
Learn more »
System-wide Software
Package Managers
Allow you to build specific environments.
Learn more »
Package Managers
From Source
Using make or cmake.
Learn more »
From Source
The cluster offers you many options for working with software. Two of the easiest and most convenient ways are using the
module
command on the
command line
and the
interactive-ood-apps
web portal.
See also
More about using module.
If you need a specific software package, first check to see if it is already available through one of the preinstalled modules on the cluster. The Research Computing team adds new modules regularly, so use the
module
avail
command to view the most up-to-date list.
Requesting Software Installation Assistance
¶
If the software you need is not a module on the cluster, cannot be installed via Spack, and is not available through another way of self-installation (e.g.,
make
), please submit a
ServiceNow software request ticket
.
Note
Some packages might not be able to be installed on the cluster due to hardware incompatibility issues.
Following these steps, users should be able to install most software packages on their own. However, always refer to the specific software’s documentation, as steps can vary. If you run into issues or need help, please contact support.
Next
System-wide Software
Previous
Using Globus
Copyright © 2024, RC
Made with
Furo
On this page
Software Overview
Cluster Software
Requesting Software Installation Assistance

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html

Slurm - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Slurm
¶
Slurm (Simple Linux Utility for Resource Management) is an open-source, highly configurable, fault-tolerant, and adaptable workload manager, used extensively in High Performance Computing (HPC) environments.
Slurm is designed to accommodate the complex needs of large-scale computational workloads by efficiently distributing and managing tasks across clusters comprising thousands of nodes, offering seamless control over resources, scheduling, and job queuing.
You can also use Slurm on the Discovery cluster for functionalities such as
Slurm Jobs Array
,
Monitoring and Managing Jobs
, and check the
Query Partitions: sinfo
.
Slurm Commands
Basic Slurm commands that are used for running, monitoring, and canceling jobs.
Monitoring and Managing Jobs
Learn the advanced usage and explanation of
squeue
,
scancel
, and
sinfo
for monitoring jobs.
Slurm Jobs Array
An introduction and use cases for Slurm job arrays for launching a large series of jobs.
Next
Slurm Commands
Previous
CMake
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/classroom/index.html

Classroom Resources - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Classroom Resources
¶
High-Performance Computing (HPC) is not only for researchers; it is an integral part of modern education. In this section, we offer guidance for instructors and students who want to use our HPC resources in the classroom.
Course guide
Read our guide on teaching or taking a course on the NU HPC.
Learn more »
Course Guide
Courses Cheetsheet
A one-page guide to common unix commands, OOD troubleshooting tips, and more.
Learn more »
Courses Cheatsheet
CPS Class Instructions
Instructions for opening a jupyterlab session for students and instructors in CPS
Learn more »
CPS Class Instructions
Leveraging HPC in the classroom can enrich the learning experience, allowing students to tackle real-world problems. Explore this section to understand how you can make the most of our HPC resources for educational purposes. For further assistance, please contact our educational support team at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy teaching and learning!
Next
Course Guide
Previous
Slurm Jobs Array
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/containers/index.html

Containers on HPC - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Containers on HPC
¶
A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. (
Reference
)
Containers allow reproducibility and portability in scientific workflows.
They ensure that scientific experiments can be reproduced on any system, such as our laptop, our friend’s laptop, HPC as well as public cloud-based environments. Containers achieve this by encapsulating the entire software environment of the specific analysis or simulation. This eliminates the “it works on my machine” problem by providing a consistent runtime environment across various platforms.
This consistency also allows for more collaboration, allowing researchers to share their work without encountering compatibility issues, thereby speeding up scientific workflows.
Containers also simplify software dependency issues by isolating software and its dependencies from the underlying HPC environment. This reduces project conflicts and makes managing different software versions much more accessible.
We recommend using containers on the HPC because they provide isolation from the HPC environment and complete control of the software environment to the researcher. Many technologies exist that could be used for containerization, including Docker, Podman, and Singularity/Apptainer. On Discovery, we use Singularity as the containerization engine, however, Singularity can work with containers built with Docker or Podman.
Singularity on Discovery
See how to use containers on HPC
Singularity on Discovery
Next
Singularity on Discovery
Previous
CPS Class Instructions
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/index.html

Best Practices - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Best Practices
¶
Utilizing High-Performance Computing (HPC) resources effectively requires adherence to best practices. In this section, you will find essential guidance for managing your directory storage, checkpointing jobs, and cluster usage.
Home Directory Storage Quota
Learn about storage quotas and how to manage your home directory effectively.
Learn more »
Home Directory Storage Quota
Work Directory Storage Quota
Learn about storage quotas and how to manage your work directory effectively.
Learn more »
Work Directory Storage Quota
Scratch Directory Purge
Learn how to use /scratch effectively, before and after a purge.
Learn more »
Scratch Directory Purge
Checkpointing Jobs
Discover how to save progress within long-running jobs, preventing loss of data.
Learn more »
Checkpointing Jobs
Shell Environment on the Cluster
Best practices for using the shell environment on Discovery.
Learn more »
Shell Environment on the Cluster
Cluster Usage
Guidelines about where tasks should be performed and the bots that monitor them.
Learn more »
Cluster Usage
Applying these best practices can greatly enhance your HPC experience. They ensure efficient use of resources, contribute to robust job execution, and facilitate optimal performance. If you need additional assistance, our support team is reachable at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy computing!
Next
Home Directory Storage Quota
Previous
Singularity on Discovery
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/faqs-new.html

Frequently Asked Questions (FAQs) - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Frequently Asked Questions (FAQs)
¶
Below are some common questions and answers regarding our HPC cluster and its operation.
- My OOD job shutsdown/completes quickly before/during/after starting? OR
- When I try and launch an OOD job, I get: “Disk quota exceeded” error. OR
- My OOD Desktop job starts and quickly moves to
Completed
without starting..
There are two main things to check:
Check that the memory available in
/home/$USER
is less than the quota limit.
There are storage quotas on
$HOME
directories. (See
Home Directory Storage Quota
for more details). OOD applications run from the
$HOME
directory and it is important that there is enough space in
$HOME
for the OOD application to run.
In the terminal, run
du
-shc
.[^.]*
*
/home/$USER
, which should have output similar to the following:
[
<username>@<host>
~
]
$
du
-shc
.
[
^.
]
*
*
/home/
$USER
/
39M
.git
106M
discovery-examples
41K
README.md
3
.3M
software-installation
147M
total
Based on the output, we can check which files are consuming how much memory, and which files can be removed to free up memory.
Check if there are other scripts running at startup (in
.bashrc
), or an anaconda or other environments loading at startup.
We recommend loading other scripts/environments after your main environment has been loaded. We recommend activating any conda environments
after
the main environment has been loaded. If there are other scripts/environments you’d like to run, we recommend creating a separate bash script and sourcing it once the main environment has been loaded. For more details on the
.bashrc
file and best practices, see
About your .bashrc file
.
Can I install my software on the HPC cluster?
Yes, you can. Please follow the guidelines in the
Package Managers
and
From Source
sections. If you encounter any issues, contact Research Computing. To contact RC, see
getting-help
.
How to Resolve the “Error: C++17 standard requested but CXX17 is not defined” When Installing a Package in R?
Ensure that you have the GCC version 11.1.0 module loaded. Run the following command
module
load
gcc/11.1.0
Later create a .R/Makevars file to add compiler flags using following commands.
mkdir
-p
~/.R
nano
~/.R/Makevars
Next, make sure to add the following flags in the file.
CXX17
=
g
++
-
std
=
c
++
17
-
fPIC
CXX17STD
=
-
std
=
c
++
17
PKG_CXXFLAGS
=
-
std
=
c
++
17
-
fPIC
PKG_LIBS
=
-
fPIC
Now try to install the desired package.
If you are using a different version of GCC, adjust the module load command accordingly (e.g., module load gcc/9.3.0).
Previous
Glossary
Copyright © 2024, RC
Made with
Furo

================================================================================

