URL: https://github.com/northeastern-rc/rc-public-documentation

GitHub - northeastern-rc/rc-public-documentation
Skip to content
Navigation Menu
Toggle navigation
Sign in
Product
GitHub Copilot
Write better code with AI
Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
All features
Documentation
GitHub Skills
Blog
Solutions
By company size
Enterprises
Small and medium teams
Startups
By use case
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
White papers, Ebooks, Webinars
Customer Stories
Partners
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
Advanced Security
Enterprise-grade security features
GitHub Copilot
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our
documentation
.
Cancel
Create saved search
Sign in
Sign up
Reseting focus
You signed in with another tab or window.
Reload
to refresh your session.
You signed out in another tab or window.
Reload
to refresh your session.
You switched accounts on another tab or window.
Reload
to refresh your session.
Dismiss alert
northeastern-rc
/
rc-public-documentation
Public
Notifications
You must be signed in to change notification settings
Fork
6
Star
6
rc-docs.northeastern.edu/
6
stars
6
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
16
Pull requests
5
Discussions
Actions
Projects
0
Security
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Projects
Security
Insights
northeastern-rc/rc-public-documentation
master
Branches
Tags
Go to file
Code
Folders and files
Name
Name
Last commit message
Last commit date
Latest commit
History
1,713 Commits
.README_images
.README_images
.github/
ISSUE_TEMPLATE
.github/
ISSUE_TEMPLATE
bin
bin
container
container
docs
docs
.gitignore
.gitignore
.pre-commit-config.yaml
.pre-commit-config.yaml
.readthedocs.yaml
.readthedocs.yaml
README.md
README.md
View all files
Repository files navigation
README
Research Computing Public Documentation
This repository contains the source files for the Research Computing documentation.
HPC users can submit issues and bugs in the documentation
here
.
Basic Development Workflow
Following are the basic steps for contributing to the RC documentation.
In the terminal, go to the folder where we want to clone the repo.
Use the following command:
git clone https://github.com/northeastern-rc/rc-public-documentation.git0
This should result in an output similar to following:
Cloning into
'
rc-public-documentation
'
...
remote: Enumerating objects: 9694, done.
remote: Counting objects: 100% (2891/2891), done.
remote: Compressing objects: 100% (733/733), done.
remote: Total 9694 (delta 2413), reused 2371 (delta 2158), pack-reused 6803
Receiving objects: 100% (9694/9694), 91.99 MiB
|
10.76 MiB/s, done.
Resolving deltas: 100% (6479/6479), done.
`
If we already have the documentation project on our computer, we run the following commands in the terminal.
git checkout master
git pull
This should bring the local copy of the project to the same state as the remote copy (master). We should get a message similar to the following
Already on
'
master
'
Your branch is up to date with
'
origin/master
'
.
or
Already up to date.
Now, to do our work, we’ll create a new branch from master and switch to this new branch, using the following command:
git checkout -b new-branch-name
We can replace the
new-branch-name
with any appropriate name.
We can use the command
git branch
to check if we are on the branch that we just created, which we should be working on, which should result in the following, (with the
*
next to the name of the branch we are working on)
*
new-branch-name
master
We can now make all the changes we want to make for the specific small task/edit we want to do.
Once done with the change, use the following commands to add and commit the changes, with a relevant commit message.
git add
.
git commit -m “This is the commit message”
`
(A commit message summarizes what the change is about. Notice the
.
at the end of
git add
, which tells to stage all the modified/new files.)
To preview documentation changes on our local machine, we can build and run the docker container (
instructions here
) and navigate to the corresponding page.
Now, the changes are ready to be pushed to remote. To push, use the following command:
git push
The above command should create a pull request (PR) on Github. Add reviewers to your PR for reviewing the changes. Once the review is complete someone can merge the PR, completing the process.
Advanced Development
Clone repo:
git clone git@github.com:northeastern-rc/rc-public-documentation.git
Then, either open PyCharm and create python environment (some find easier to create environment using PyCharm):
OR, do so via the command-line:
cd
rc-public-documentation

python3.11 -m venv .venv
source
venv/bin/activate
pip install --upgrade pip
Next, for those using Mac, install pre-commit via
brew
:
brew install pre-commit
For those on windows, use
pip
:
pip install pre-commit
Then, install
pre-commit
for RTD development:
$ pre-commit install
pre-commit installed at .git/hooks/pre-commit

pip install -r docs/requirements.txt
Install Dependencies
:
Launch VS-Code and open the project folder.
Install
myst-plugin-install
; also, install directly from VS-Code by searching Extensions for
myst
.
You can use the
amazing markdown lint VS Code extension
developed by David Anson.  Here are the steps:
Press
F1
to open the VS Code Command Palette.
Type
ext install markdownlint
to find the extension
Press Enter or click the cloud icon to install it
Restart Visual Studio Code if prompted
Or for package development:
pip install myst-parser
npm install -g myst-cli
Open Visual Studio Code
Press
Ctrl+P
/
Ctrl+P
/
⌘P
to open the Quick Open dialog
Type
ext install markdownlint
to find the extension
Click the
Install
button, then the
Enable
button
You can also create a separate Window pane to preview your Markdown.  To do this:
Press F1 to bring up the VS Code Command Palette.
Type “mark” to narrow down the list of commands to “markdown” commands.
Click “Markdown: Open Preview to the Side as shown here:
Markdown Preview
Additional
Code Spell Checker
Markdown helper
emojisense
markdown-all-in-one
https://sphinx-design.readthedocs.io/en/latest/dropdowns.html
Contributing
We welcome all contributions!
See the
Contributing Guide
for more details.
About
rc-docs.northeastern.edu/
Resources
Readme
Activity
Custom properties
Stars
6
stars
Watchers
11
watching
Forks
6
forks
Report repository
Releases
5
v4.0.0
Latest
Sep 25, 2023
+ 4 releases
Packages
0
No packages published
Contributors
23
+ 9 contributors
Languages
Dockerfile
61.2%
Shell
38.8%
Footer
© 2024 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/connectingtocluster/linux.html

Connecting on Linux - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Connecting on Linux
¶
Linux computers come with a Secure Shell (SSH) program called Terminal that you use to connect to the HPC using SSH. If you need to use software that uses a GUI, such as Matlab or Maestro, make sure to use the -Y option in the second step below using X11 forwarding.
Connecting to the Cluster on Linux
¶
Search for the Terminal application from the launcher menu.
At the prompt, type
ssh
<username>@login.discovery.neu.edu
, where
<username>
is your Northeastern username. If you need to use X11 forwarding, type
ssh
-Y
<username>@login.discovery.neu.edu
.
Type your Northeastern password and press
Enter
.
You are now connected to Discovery at a login node.
Passwordless SSH In Linux
¶
You must set up passwordless ssh to ensure that GUI-based applications launch without issues. Please make sure that your keys are added to the authorized.key file in your
~/.ssh
directory. This needs to be done anytime you regenerate your SSH keys. If you are having an issue opening an application that needs X11 forwarding, such as MATLAB or Schrodinger, and you recently regenerated your keys, make sure to add your keys to the authorized.key file.
Note
Ensure you’re on your local computer for steps 1 through 4—type
exit
to return to your local computer if connected to the cluster.
Open the Terminal application and type
cd
~/.ssh
. This moves you to the ssh folder on your local computer.
Type
ssh-keygen
-t
rsa
to generate two files,
id_rsa
and
id_rsa.pub
.
Press
Enter
on all the prompts (do not generate a passphrase).
Type
ssh-copy-id
-i
~/.ssh/id_rsa.pub
<yourusername>@login.discovery.neu.edu
to copy
id_rsa.pub
to your
/home/.ssh
folder on Discovery. You can enter your NU password if prompted. This copies the token from
id_rsa.pub
file to the
authorized_keys
file, which will either be generated or appended if it already exists.
Connect to Discovery via
ssh
<yourusername>@login.discovery.neu.edu
. You should now be connected without having to enter your password.
Now on the cluster,
Type
cd
~/.ssh
to move to your ssh folder.
Type
ssh-keygen
-t
rsa
to generate your key files.
Press Enter on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type
Y
.
Type
cat
id_rsa.pub
>>
authorized_keys
. This adds the contents of your public key file to a new line in the
~/.ssh/authorized_keys
file.
X11 on Linux
¶
To use X11 forwarding, from the Terminal application, log in using the -Y option (
ssh
-Y
<yourusername>@login.discovery.neu.edu
).
Tip
If you used the -Y option to enable X11 forwarding on your Linux machine, you can test to see if it is working by typing
xeyes
. This will run a small program that makes a pair of eyes appear to follow your cursor.
Next
Running Jobs
Previous
Connecting on Windows
Copyright © 2024, RC
Made with
Furo
On this page
Connecting on Linux
Connecting to the Cluster on Linux
Passwordless SSH In Linux
X11 on Linux

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/index.html

NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
HPC Documentation
¶
A dedicated platform for researchers, scholars, and students to access, learn, and leverage our state-of-the-art HPC resources.
Open OnDemand (OOD)
OOD provides interactive software (e.g, Rstudio, Jupyterlab, VSCode) connected to the cluster.
Learn more »
https://rc.northeastern.edu/ood/
Classroom Integration
Request use of the HPC for your class, including CPU, GPU, and storage.
Learn more »
Classroom Resources
Best Practices
Learn to optimize your projects by following some best practices.
Learn more »
Best Practices
The Northeastern University HPC Cluster is a high-powered, multi-node, parallel computing system designed to meet the computational and data-intensive needs of various academic and industry-oriented research projects. The cluster incorporates cutting-edge computing technologies and robust storage solutions, providing an efficient and scalable environment for large-scale simulations, complex calculations, artificial intelligence, machine learning, big data analytics, bioinformatics, and more.
Whether you are a seasoned user or just beginning your journey into high-performance computing, our portal offers comprehensive resources, including in-depth guides, tutorials, best practices, and troubleshooting tips. Furthermore, the platform provides a streamlined interface to monitor, submit, and manage your computational jobs on the HPC cluster.
We invite you to explore the resources, tools, and services available through the portal. Join us as we endeavor to harness the power of high-performance computing, enabling breakthroughs in research and fostering innovation at Northeastern University.
Note
Access to the HPC Cluster is subject to university policy and guidelines. Please ensure that you read and understand these guidelines before using the facility. If you require assistance or have any questions, please do not hesitate to contact our dedicated support team.
Next
Connecting To Cluster
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/classroom/cheatsheet.html

Courses Cheatsheet - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Courses Cheatsheet
¶
Download
Courses
Cheatsheet
Next
CPS Class Instructions
Previous
Course Guide
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/faqs-new.html

Frequently Asked Questions (FAQs) - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Frequently Asked Questions (FAQs)
¶
Below are some common questions and answers regarding our HPC cluster and its operation.
- My OOD job shutsdown/completes quickly before/during/after starting? OR
- When I try and launch an OOD job, I get: “Disk quota exceeded” error. OR
- My OOD Desktop job starts and quickly moves to
Completed
without starting..
There are two main things to check:
Check that the memory available in
/home/$USER
is less than the quota limit.
There are storage quotas on
$HOME
directories. (See
Home Directory Storage Quota
for more details). OOD applications run from the
$HOME
directory and it is important that there is enough space in
$HOME
for the OOD application to run.
In the terminal, run
du
-shc
.[^.]*
*
/home/$USER
, which should have output similar to the following:
[
<username>@<host>
~
]
$
du
-shc
.
[
^.
]
*
*
/home/
$USER
/
39M
.git
106M
discovery-examples
41K
README.md
3
.3M
software-installation
147M
total
Based on the output, we can check which files are consuming how much memory, and which files can be removed to free up memory.
Check if there are other scripts running at startup (in
.bashrc
), or an anaconda or other environments loading at startup.
We recommend loading other scripts/environments after your main environment has been loaded. We recommend activating any conda environments
after
the main environment has been loaded. If there are other scripts/environments you’d like to run, we recommend creating a separate bash script and sourcing it once the main environment has been loaded. For more details on the
.bashrc
file and best practices, see
About your .bashrc file
.
Can I install my software on the HPC cluster?
Yes, you can. Please follow the guidelines in the
Package Managers
and
From Source
sections. If you encounter any issues, contact Research Computing. To contact RC, see
getting-help
.
How to Resolve the “Error: C++17 standard requested but CXX17 is not defined” When Installing a Package in R?
Ensure that you have the GCC version 11.1.0 module loaded. Run the following command
module
load
gcc/11.1.0
Later create a .R/Makevars file to add compiler flags using following commands.
mkdir
-p
~/.R
nano
~/.R/Makevars
Next, make sure to add the following flags in the file.
CXX17
=
g
++
-
std
=
c
++
17
-
fPIC
CXX17STD
=
-
std
=
c
++
17
PKG_CXXFLAGS
=
-
std
=
c
++
17
-
fPIC
PKG_LIBS
=
-
fPIC
Now try to install the desired package.
If you are using a different version of GCC, adjust the module load command accordingly (e.g., module load gcc/9.3.0).
Previous
Glossary
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/slurmguide/index.html

Slurm - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Slurm
¶
Slurm (Simple Linux Utility for Resource Management) is an open-source, highly configurable, fault-tolerant, and adaptable workload manager, used extensively in High Performance Computing (HPC) environments.
Slurm is designed to accommodate the complex needs of large-scale computational workloads by efficiently distributing and managing tasks across clusters comprising thousands of nodes, offering seamless control over resources, scheduling, and job queuing.
You can also use Slurm on the Discovery cluster for functionalities such as
Slurm Jobs Array
,
Monitoring and Managing Jobs
, and check the
Query Partitions: sinfo
.
Slurm Commands
Basic Slurm commands that are used for running, monitoring, and canceling jobs.
Monitoring and Managing Jobs
Learn the advanced usage and explanation of
squeue
,
scancel
, and
sinfo
for monitoring jobs.
Slurm Jobs Array
An introduction and use cases for Slurm job arrays for launching a large series of jobs.
Next
Slurm Commands
Previous
CMake
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/interactiveandbatch.html

Interactive and Batch Mode - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Interactive and Batch Mode
¶
In our High-Performance Computing (HPC) environment, users can run jobs in two primary modes: Interactive and Batch. This page provides an in-depth guide to both, assisting users in selecting the appropriate mode for their specific tasks.
Interactive Jobs:
srun
Command
¶
The
srun
command is used to submit an interactive job, which runs in a shell terminal. This method is useful when you want to test a short computation or run an interactive session like a shell, Python, or an R terminal.
To start an
srun
session, the following syntax is used from the login node:
srun
[
options
]
[
command
]
The following is an example of an
srun
job running on 1 node, with 1 task:
srun
-N
1
-n
1
-p
short
--pty
bash
n,
--ntasks=<number>
: specify the number of tasks
N,
--nodes=<minnodes[-maxnodes]>
: specify the number of nodes
p,
--partition=<partition-name>
: specify a partition for the job to run on
To see all options for
srun
, please refer to
srun manual
from SchedMD.
Examples Using
srun
¶
You can tailor your request to fit both the needs of the job and the partition limits if you’re familiar with the available
hardware
and
partitions
on Discovery.
To request one node and one task for 30 minutes with X11 forwarding (check that you have
X11 forwarding
setup) on the short partition, type:
srun
--partition
=
short
--nodes
=
1
--ntasks
=
1
--x11
--mem
=
2G
--time
=
00
:30:00
--pty
/bin/bash
To request one node, with 10 tasks and 2 CPUs per task (a total of 20 CPUs), 40 GB of memory, for one hour on the express partition, type:
srun
--partition
=
short
--nodes
1
--ntasks
10
--cpus-per-task
2
--pty
--mem
=
40G
--time
=
01
:00:00
/bin/bash
To request 2 nodes, each with 10 tasks per node and 2 CPUs per task (a total of 40 CPUs), 80 GB of memory, for one hour on the express partition, type:
srun
--partition
=
short
--nodes
=
2
--ntasks
10
--cpus-per-task
2
--pty
--mem
=
80G
--time
=
01
:00:00
/bin/bash
To allocate a GPU node, you should specify the
gpu
partition and use the
--gres
option:
srun
--partition
=
gpu
--nodes
=
1
--ntasks
=
1
--gres
=
gpu:1
--mem
=
2Gb
--time
=
01
:00:00
--pty
/bin/bash
Batch Jobs:
sbatch
Command
¶
The
sbatch
command is used to submit a job script for passive execution. The script includes the
SBATCH
directives that control the job parameters (e.g., number of nodes, CPUs per task, job name). A node is a single machine in the cluster allocated for computation, while a task is a unit of parallel work required from within the job. Not all programs are optimized to run on more than one node. We recommmend testing if increasing the number of tasks decreases job runtime prior to testing if increasing the number of nodes decreases runtime.
Important
Remember for all requests to the scheduler, the more resources requested the longer your job may sit in the queue waiting for the allocation of those resources.
To submit the batch jobs, the following is run from the login node:
sbatch
[
options
]
<script_file>
An example sbatch script for a job utilizing 2 nodes and 16 tasks per node:
Note
Only use more than 1 node if your program is optimized for multi-nodal execution.
#!/bin/bash
#SBATCH -J MyJob                            # Job name
#SBATCH -N 2                                # Number of nodes
#SBATCH -n 16                               # Number of tasks
#SBATCH -o output_%j.txt                    # Standard output file
#SBATCH -e error_%j.txt                     # Standard error file
#SBATCH
[email protected]
# Email
#SBATCH --mail-type=ALL                     # Type of email notifications
# Your program/command here
./my_program
To submit this job script, save it as
my_job.sh
and run:
sbatch
my_job.sh
For more information on the
SBATCH
directives that can be used in the script, please refer to the
sbatch manual
from Schedmd.
Request a specific amount of memory in the job script if calculations require more than the default 2GB per allocated code. The example script below requests 100GB of memory (
--mem=100G
). Use one capital letter to abbreviate the unit of memory (i.e., kilo
K
, mega
M
, giga
G
, and tera
T
) with the
--mem=
option, as that is what Slurm expects to see:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=4:00:00
#SBATCH --job-name=MyJobName
#SBATCH --mem=100G
#SBATCH --partition=short
# <commands to execute>
Next
Running Jobs with job-assist
Previous
Job Scheduling Policies and Priorities
Copyright © 2024, RC
Made with
Furo
On this page
Interactive and Batch Mode
Interactive Jobs:
srun
Command
Examples Using
srun
Batch Jobs:
sbatch
Command

================================================================================

URL: https://rc.northeastern.edu

Research Computing at Northeastern University
Faculty
Faculty
Staff
Staff
Students
Students
GET SUPPORT
SUPPORT
RC Services
About
Cluster Maintenance
Compute
Data Management Plan for Funding Agencies
Data Storage Options
Getting Access
Partitions
Team
News
Status Updates
Support
Getting Help
Open OnDemand (OOD)
Discovery Courses FAQ
Service Catalog
Training
Policies
General Policies
Research Projects Storage Space Policy
Scratch Space Policy
Acknowledgements
Documentation
Select Page
Research Computing at Northeastern University
Connecting the research community at Northeastern University with high performance computing solutions.
Discovery Announcements:
Intermittent Connection Issues with Open onDemand Applications
Tuesday, October 22, 2024; 12:30 p.m. ET
The Research Computing team is currently investigating intermittent connection issues with several Open onDemand applications.
If you have any questions, please
submit a ticket
, drop by our
Office Hours, or Schedule a Consultation
.
Scratch Purge Scheduled for T
uesday, November 5, 2024
To ensure adequate resources for all users going into the new semester, the Research Computing team is planning to continue its periodic practice to purge
/scratch space
. T
he next purge is planned for Tuesday, November 5, 2024
. As a reminder, if you have any data on /scratch that you would like to retain, please move your data into persistent storage, like /work. Research Computing also provides archival storage options in the form of Disk and Tape media. You can find more information about storage option on the
Research Projects Storage Space Policy
page.
If you need to expand the amount of storage you have available in /work, please submit a
Storage Space Extension Request
. If you have any questions, please
submit a ticket
, drop by our
Office Hours, or Schedule a Consultation
.
Learn About Discovery
Discovery is a high performance computing (HPC) resource for the Northeastern University research community. The Discovery cluster is located in the Massachusetts Green High Performance Computing Center (MGHPCC) in Holyoke, MA. MGHPCC is a 90,000 square-foot, 15 megawatt research computing and data center facility that houses computing resources for five institutions: Northeastern, BU, Harvard, MIT, and UMass.
The Discovery cluster provides access to over 50,000 CPU cores and over 525 GPUs to all Northeastern faculty and students free of charge. Hardware currently available for research consists of a combination of Intel Xeon (Cascadelake, Skylake, Broadwell, Haswell, Sandybridge, and Ivybridge) and AMD (Zen, Zen2) CPU microarchitectures. Additionally, a selection of NVIDIA Pascal (P100), Volta (V100), Turing (T4), Ampere (A100), and Hopper (H100) GPUs. Discovery is connected to the university network over 10 Gbps Ethernet (GbE) for high-speed data transfer, and Discovery provides 6 PB of available storage on a high-performance file system. Compute nodes are connected with either 10 GbE or high data rate InfiniBand (200 Gbps or 100 Gbps), supporting all types and scales of computational workloads.
Hardware
Software
Open OnDemand
Data Management
Connecting You to the Power of Discovery
As a researcher at Northeastern University, you can take advantage of the comprehensive research computing offerings and services available to you—including access to centralized high performance computing (HPC) clusters, storage, visualization, software, high-level technical and scientific consultations, documentation, and training.
Research Computing Office Hours
RC Office Hours are a great way for you to connect with the RC team for short (~10-15 min) consultations. Office Hours are held every Wednesday from 3 – 4 p.m. ET and Thursday from 11 a.m. – 12 p.m. ET. All current or prospective Discovery users are welcome to join anytime during these hours.
Join Wednesday Office Hours : 3 - 4 p.m. ET
Join Thursday Office Hours : 11 a.m. - 12 p.m. ET
Research Computing News at Northeastern
RC Team
Get to Know the Research Computing Student Assistants
RC Team
Get to know the Research Computing Co-ops
Read More Blogs
Latest Publications
RC Team
Get to Know the Research Computing Team
View More News posts
How Can Research Computing Support You?
Accelerate your research at any stage by leveraging our online user guides, hands-on training sessions, and one-on-one guidance.
Documentation
Training
Consultations & Office Hours
Contact Us
Subscribe
Close
Subscribe By Email
Get every new post delivered right to your inbox.
Your Email
Leave this field blank
This form is protected by reCAPTCHA and the Google
Privacy Policy
and
Terms of Service
apply.

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/multigpu-partition-access.html

Access to Multi-GPU Partition - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Access to Multi-GPU Partition
¶
The
multigpu
partition in the HPC cluster allows users extensive parallel processing on multiple GPUs. This setup is ideal for applications that require significant computational power, such as deep learning, scientific simulations, and large-scale data analysis.
Please follow these steps to apply for the Multi-GPU Partition.
Step 1 - Submitting the Access to MultiGPU Partition Request
¶
Please use the partition application form here:
HPC Partition Request
Select under “Partition Type” -
Multigpu -
Partition
(Testing
Access)
Fill in the number of GPUs and the type of GPU you’d like to test on. Please also provide a short description of your expected testing workload.
Note
Please consider the following while requesting GPUs for testing:
Testing should represent your planned work but doesn’t need full production runs.
Use scaled-down versions of your jobs for timing data when possible.
Ensure your test cases cover a range of processor/GPU counts to measure scaling accurately.
Consider testing with different problem sizes to understand how scaling efficiency changes with workload.
If you need to use A100s in your workflow, consider testing your code on V100 GPUs:
V100 testing is often sufficient to demonstrate scaling.
If a job scales well on V100s, it will also scale on A100s.
This approach conserves A100 resources for all the users on the cluster.
Only use A100s for testing if your job requires their capabilities or memory that V100s cannot provide.
Submit the form.
The Research Computing team will need an estimate of the duration needed for testing (preferably less than 24 hrs.)
The Research Computing team will contact you with more details for accessing a multi-GPU node.
Step 2 - Testing the Code on the Temporary Reservation
¶
To check your reservation
scontrol
show
reservation
=
<reservation_name>
#<reservation_name> will be in the details provided by the RC team.
To run an
Interactive Job
, for example, using V100-sxm2 with 4 GPUs, use the command below
srun
-p
reservation
--reservation
=
<reservation_name>
--gres
=
gpu:v100-sxm2:4
--time
=
24
:00:00
-N
1
--pty
/bin/bash
Note
Your local machine must remain active for successful job execution. Be aware of the following risks:
Network disconnections have the potential to interrupt the job.
Computer sleep/hibernation can break the connection.
Losing RDP session (e.g., timeout, local reboot) stops GUI-dependent processes.
Power outages or system updates can cause unexpected disconnects.
Recommendations:
Use a stable network connection.
Disable sleep mode on your local machine during testing.
Implement job checkpointing where possible.
Monitor job status regularly.
To run
Non-interactive job
, you can use the following command
1- Create a script
nano
my_job.sh
# You can use any text editor you prefer, not just nano. Choose the editor you're most comfortable with for modifying files.
2- Add reservation and job details
#!/bin/bash
#SBATCH --job-name=MyJob             # Job name
#SBATCH -p reservation
#SBATCH --reservation=<reservation_name>
#SBATCH --gres=gpu:v100-sxm2:4
#SBATCH --nodes=1                    # Number of nodes
#SBATCH --ntasks=4                   # Number of tasks
#SBATCH --cpus-per-task=2            # Number of CPUs per task
#SBATCH --mem=8G                     # Total CPU memory (not GPU memory)
#SBATCH --time=02:00:00              # Time limit hh:mm:ss
#SBATCH --output=output_%j.txt       # Standard output and error log
#SBATCH --error=error_%j.txt         # Standard error log
# Load any required modules
module
load
python
#example
# Run your program
srun
python
your_script.py
#example
# You can use the editor of your choice to edit and save the file on the cluster.
# Following commands are for the editor 'Nano', and can be used to write this script to disk
# Ctrl+x to save the file
# press 'Y' to save the changes
# press enter to complete saving the file to the disk
3- Submit the job
sbatch
job_script.sh
4-You can monitor the status of your job using the squeue command:
squeue
-u
<your_username>
#To cancel a job
scancel
<job_id>
# where <job_id> is the ID of the Job we are canceling
Perform testing on 1,2,..(4,8) GPUs. Record runtimes for each test (calculate efficiency).
Important
This multi-GPU setup is intended for research workflows only. For course-related multi-GPU needs, please refer to the course request form. Instructors should submit those requests directly through the appropriate channels.
Step 3 - Post-Testing Application
¶
Ensure your
Scaling efficiency
is adequate (generally over 0.5) when using the maximum GPUs selected on the
multigpu
partition⁠. If needed, please consult with the Research Computing (RC) team for guidance and support throughout the process⁠.
Re-enter the partition application
form
and select
Multigpu
-
Partition
(Post
Testing)
under “Partition Type.”
Please fill out the form with your test results and submit it.
After the RC team reviews and approves your application, you will be granted access to the
multigpu
partition.
Next
Data Management
Previous
GPU Job Submission
Copyright © 2024, RC
Made with
Furo
On this page
Access to Multi-GPU Partition
Step 1 - Submitting the Access to MultiGPU Partition Request
Step 2 - Testing the Code on the Temporary Reservation
Step 3 - Post-Testing Application

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Core

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Backfilling

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-InfiniBand-IB

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Login-Node

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/shell_environment.html

Shell Environment on the Cluster - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Shell Environment on the Cluster
¶
The Discovery Shell Environment and
.bashrc
¶
Discovery uses a Linux-based operating system (CentOS), where the shell program interfaces with the user. Bash (Bourne Again Shell) is one of the most popular shell implementations and the default shell on Discovery.
The shell script
.bashrc
is used by
bash
to initialize your shell environment. For example, it is typically used to define aliases, functions, and load modules. Note that environment variables settings (such as
PATH
) generally go in the
.bash_profile
or
.profile
files. Your
.bashrc
,
.bash_profile
, and
.profile
files live in your
$HOME
directory. You can change your .bashrc with a text editor like
nano
.
Caution
Making edits to your
.bashrc
file can result in many issues. Some changes may prevent you from launching apps or executing commands. Modifying your
PATH
variable may result in the inability to use basic shell commands (such as
cd
or
ls
) if not done correctly.
Before making changes to your
.bashrc
file, make a backup of the default
.bashrc
file, so you can restore it if necessary. If you need help with editing
.bashrc
, reach out to
rchelp
@
northeastern
.
edu
or
schedule a consultation with a staff member
who can help suggest edits and troubleshoot any issues you might be having.
About your .bashrc file
¶
When your account is created, you have a default
.bashrc
file in your home directory. See the figure below for an example of a default
.bashrc
file.
# .bashrc
# Source global definitions
if
[
-f
/etc/bashrc
]
;
then
.
/etc/bashrc
fi
# Uncomment the following line if you don't like SystemCTL's auto-paging feature:
# export SYSTEMD_PAGER=
# User specific aliases and functions
Important
We recommend keeping
.bashrc
unchanged when using Discovery. You can source environment shell scripts or load modules directly inside your job instead. This approach can prevent some runtime errors from loading incompatible modules, setting environment variables incorrectly, or mixing multiple software and Conda environments.
Editing your
.bashrc
file
¶
The basic workflow for editing your
.bashrc
file is to sign in to Discovery, go to your
$HOME
directory, open the file in a text editor on the command line, make your edits, save the file, sign out of Discovery, then sign back in again. Your changes will take effect when you sign back in again.
Example procedure for editing your
.bashrc
file:
Sign in to Discovery.
(Optional) Type
pwd
to ensure you are in your
/home
directory.
(Optional) Type
ls
-a
to view the contents of your
/home
directory, including hidden files. Your
.bashrc
file is hidden (hidden files are preceded by a
.
). Using the
-a
option with
ls
displays hidden files.
(Recommended) Type
cp
.bashrc
.bashrc-default
to make a copy of your
.bashrc
file called
.bashrc-default
.
Type
nano
.bashrc
to open your
.bashrc
file in the nano text editor.
Type the edits that you want to make to your file.
Save the file and exit the editor.
Sign out of Discovery and sign back in for the changes to take effect.
Sourcing a Shell script example
¶
A safe alternative to
.bashrc
is to source a shell script inside your runtime job environment. Below is an example script to load an Anaconda module and source a Conda environment, which will be used inside the slurm script.
Create a shell script
myenv.bash
:
#!/bin/bash
module
load
anaconda3/2021.05
module
load
cuda/11.1
source
activate
pytorch_env_training
Then, source the shell script inside your sbatch slurm script (see
Batch Jobs: sbatch Command
):
#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --time=01:00:00
#SBATCH --job-name=gpu_run
#SBATCH --mem=4GB
#SBATCH --ntasks=1
#SBATCH --output=myjob.%j.out
#SBATCH --error=myjob.%j.err
source
myenv.bash
python
<myprogram>
Next
Cluster Usage
Previous
Checkpointing Jobs
Copyright © 2024, RC
Made with
Furo
On this page
Shell Environment on the Cluster
The Discovery Shell Environment and
.bashrc
About your .bashrc file
Editing your
.bashrc
file
Sourcing a Shell script example

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Partition

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Node

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/systemwide/matlab.html

Using MATLAB - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Using MATLAB
¶
See also
Using Module
and
using-ood
.
MATLAB is available on the cluster as a module and as an interactive app on Open OnDemand. You can also download MATLAB for your computer through the
Northeastern portal on the MATLAB website
. Note that the procedures detailed below pertain specifically to using MATLAB on the cluster, and not to using MATLAB on your computer.
Installing MATLAB toolboxes
¶
Use the following procedure if you need to install a MATLAB toolbox:
Download the toolbox from its source website.
Connect to the cluster.
Create a directory in your /home directory. We recommend creating a directory called
matlab
by typing:
mkdir /home/
/matlab  #where
is your username
Go to the directory you just created by typing:
cd /home/
/matlab
Unzip the toolbox file by typing:
unzip
Load MATLAB by typing:
module load matlab
Start MATLAB by typing:
matlab
Add the toolbox to your PATH by typing:
addpath(‘/home/
/matlab/
’) #where
is the name of the toolbox you just unzipped
If this is a toolbox you want to use more than once, you should save it to your path by typing:
savepath()
You can now use the toolbox within MATLAB. When you are done, type
quit
.
Using MATLAB Parallel Server
¶
MATLAB on OOD
Configuration of MATLAB client on the HPC
The cluster has MATLAB Parallel Server installed and this section details an example of how you can set up and use the MATLAB Parallel Computing Toolbox on the HPC. This walkthrough uses MATLAB R2023a launched as an interactive app on the
Open OnDemand
web portal. There are several parts to this walkthrough so we suggest that you read it through completely before starting. The parameters presented represent only one scenario and the results may differ when you run the examples.
Go to
http://ood.discovery.neu.edu
. If prompted, sign in with your cluster username and password.
Click
Interactive Apps
, and select
MATLAB
.
Select
MATLAB version R2023a
or newer, and set the time for the length of your job, the number of cpus to be 8, and the memory to at least 16 GB. Click
Launch
.
If necessary, adjust the
Compression
and
Image Quality
, and then click
Launch MATLAB
.
Once MATLAB is open, click on the Home tab, click Parallel > Discover Clusters… to discover the profile. This will open a window where you should select ‘on your network’ and click next and then select ‘Discovery’ and click finished. Note, this is valid for R2023a and newer. Create a cluster profile to run parallel jobs by running
configCluster
in the Command Window. Note:
configCluster
should only be called once on the HPC.
Now MATLAB is configured to have jobs submitted to the HPC cluster and not run in the current session.
MATLAB on a Local Machine
Installation and Configuration of MATLAB on a Local Machine
MATLAB can be configured on your local machine to submit jobs to the HPC cluster. To complete this, you will need to download the following directory from Github:
MATLAB Desktop Parallel Toolbox Setup
.
In your desktop MATLAB instance, run:
>>
userpath
and move the compressed directory you downloaded from Github to that location and unzip the directory. To configure MATLAB to run parallel jobs on the cluster, run the following in the Command Window:
>>
configCluster
Note
This only needs to be called once per version of MATLAB you are using to run jobs on the HPC cluster.
Submitting jobs to the HPC cluster will require SSH credentials and you will be prompted for your username and password or a private SSH key. The username and/or location of the private key will be saved for future session in MATLAB.
Jobs will now default to running on the HPC cluster rather than submitting to your local machine.
To submit jobs to your local machine instead of the HPC cluster, run the following:
>>
% Get a handle to the local resources
>>
c
=
parcluster
(
'local'
);
Configuring Jobs
¶
Prior to submitting jobs to the HPC, various parameters can be assigned and adjusted for your job such as partition, email, job walltime, etc.
To get a handle on the cluster and the current configurations please run the following command in the Command Window:
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
A required field that needs to be set is the memory per CPU core:
>>
% Specify memory to use, per core (default: 4gb)
>>
c
.
AdditionalProperties
.
MemPerCPU
=
'6gb'
;
You need to save the changes after modifying the AdditionalProperties:
>>
c
.
saveProfile
To view the values of the currently saved configuration, use the following command in the Command Window:
>>
% To view current properties
>>
c
.
AdditionalProperties
If you need to unset a saved parameter, you can do the following in the Command Window:
>>
% Turn off email notifications
>>
c
.
AdditionalProperties
.
EmailAddress
=
''
;
>>
c
.
saveProfile
The following are optional fields that can be set and adjusted for the parcluster and include the following items.
Specifying a constaint such as a
CPU constraint
:
>>
% Specify a constraint
>>
c
.
AdditionalProperties
.
Constraint
=
'feature-name'
;
Setting an email to receive updates on the SLURM job:
>>
% Request email notification of job status
>>
c
.
AdditionalProperties
.
EmailAddress
=
'
[email protected]
'
;
Setting the number of GPUs used and the type of
GPU
:
>>
% Specify number of GPUs (default: 0)
>>
c
.
AdditionalProperties
.
GPUsPerNode
=
1
;
>>
c
.
AdditionalProperties
.
GPUCard
=
'gpu-card'
;
Note
If you need a GPU, you need to make sure the partition you are submitting the job to is the
gpu
or
multigpu
partition or a private partition that has GPUs available.
Selecting the
partition
you want the job to run on the HPC cluster:
>>
% Specify the partition
>>
c
.
AdditionalProperties
.
Partition
=
'partition-name'
;
Setting the number of cores for the job:
>>
% Specify cores per node (default: 0)
>>
c
.
AdditionalProperties
.
ProcsPerNode
=
4
;
Setting exclusive use of the node:
>>
% Set node exclusivity (default: false)
>>
c
.
AdditionalProperties
.
RequireExclusiveNode
=
true
;
Note
The exclusive node option can increase your wait time in the queue so only use if required.
If you are running the job on a reservation that has been setup:
>>
% Use reservation
>>
c
.
AdditionalProperties
.
Reservation
=
'reservation-name'
;
Setting the time for the job to run on the HPC cluster:
>>
% Specify the wall time (e.g., 1 day, 5 hours, 30 minutes)
>>
c
.
AdditionalProperties
.
WallTime
=
'1-05:30'
;
Note
You need to make sure the time fits within the max time allowed on the
partition
you are submitting to or the jobs will not run.
Using the MATLAB Client Interactively on the HPC Cluster
¶
If you want to run an interactive pool job on the cluster we can use a
parpool
like above:
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
>>
% Open a pool of 64 workers on the cluster
>>
pool
=
c
.
parpool
(
64
);
Instead of the open instance of MATLAB running this code, this will be run across the HPC cluster. Run the following code:
>>
% Run a parfor over 1000 iterations
>>
parfor
idx
=
1
:
1000
a
(
idx
)
=
rand
;
end
When you no longer need the pool, you can delete it and free up the resources with:
>>
% Delete the pool
>>
pool
.
delete
Using the MATLAB with a Batch Job on the HPC Cluster
¶
You can use the
batch
command to submit passively running jobs to the HPC cluster from MATLAB. This command will return a job object that can be used to access the output of the submitted batch job.
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
>>
% Submit job to query where MATLAB is running on the cluster
>>
job
=
c
.
batch
(@
pwd
,
1
,
{},
'CurrentFolder'
,
'.'
);
>>
% Query job for state
>>
job
.
State
>>
% If state is finished, fetch the results
>>
job
.
fetchOutputs
{:}
>>
% Delete the job after results are no longer needed
>>
job
.
delete
To see the jobs that have been completed or still are running, you can call
parcluster
to return the cluster object that stores this information. Run the following command in the Command Window:
>>
c
=
parcluster
;
>>
jobs
=
c
.
Jobs
>>
>>
% Get a handle to the second job in the list
>>
job2
=
c
.
Jobs
(
2
);
Once you locate the job, you can retrieve the results with
fetchOutputs
if you are using it interactively in the Command Window or you can use
load
if you are using it in a MATLAB script.
>>
% Fetch all results from the second job in the list
>>
job2
.
fetchOutputs
{:}
Parallel MATLAB Batch Job
¶
The
batch
command can also submit parallel workflows to the HPC cluster. As an example, save the following code to a MATLAB script called
parallel_example.m.
function
[sim_t, A] = parallel_example
(
iter
)
if
nargin
==
0
iter
=
8
;
end
disp
(
'Start sim'
)
t0
=
tic
;
parfor
idx
=
1
:
iter
A
(
idx
)
=
idx
;
pause
(
2
)
idx
end
sim_t
=
toc
(
t0
);
disp
(
'Sim completed'
)
save
RESULTS
A
end
When you submit the job using the
batch
command, you will need to also specify the MATLAB Pool argument:
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
>>
% Submit a batch pool job using 4 workers for 16 simulations
>>
job
=
c
.
batch
(@
parallel_example
,
1
,
{
16
},
'Pool'
,
4
,
...
'CurrentFolder'
,
'.'
);
>>
% View current job status
>>
job
.
State
>>
% Fetch the results after a finished state is retrieved
>>
job
.
fetchOutputs
{:}
ans
=
8.8872
This example took 8.89 seconds to run utilizing four workers. These jobs will always request N+1 number of CPUs for the task since 1 worker is required to manage the batch job and the pool of workers.
We can run the same simulation but increase the size of the Pool. We will retrieve the results later so we will keep the submitted job id as a reference.
Note
Be careful with increasing the numbers of workers for your job because there can be a point where the returns on performance will diminish.
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
>>
% Submit a batch pool job using 8 workers for 16 simulations
>>
job
=
c
.
batch
(@
parallel_example
,
1
,
{
16
},
'Pool'
,
8
,
...
'CurrentFolder'
,
'.'
);
>>
% Get the job ID
>>
id
=
job
.
ID
id
=
4
>>
% Clear job from workspace (as though MATLAB exited)
>>
clear
job
You need to get a handle of the cluster with the
parcluster
command and then you can use the
findJob
method to obtain information about the job:
>>
% Get a handle to the cluster
>>
c
=
parcluster
;
>>
% Find the old job
>>
job
=
c
.
findJob
(
'ID'
,
4
);
>>
% Retrieve the state of the job
>>
job
.
State
ans
=
finished
>>
% Fetch the results
>>
job
.
fetchOutputs
{:};
ans
=
4.7270
This job ran for 4.73 seconds using 8 workers in MATLAB. You can run the code with different numbers of works to determine the ideal number of works for the job.
You can also use the MATLAB GUI to obtain the results from the job using the Job Monitor found in the Parallel dropdown in the Home tab and select Monitor Jobs.
Function
Description
Applies Only to Desktop
clusterFeatures
List of cluster features/constraints
clusterGpuCards
List of cluster GPU cards
clusterPartitionNames
List of cluster partition/queue names
disableArchiving
Modify file archiving to resolve file mirroring issue
true
fixConnection
Reestablish cluster connection (e.g., after reconnection of VPN)
true
willRun
Explain why job is queued
Debugging
¶
You can debug MATLAB errors with the following commands depending if it is a serial or parallel job. If the job is a serial submission, you can use the
getDebugLog
method to view the error log that is created. In the Command Window:
>>
c
.
getDebugLog
(
job
.
Tasks
)
If you submit a Pool job, you need to specify the job object:
>>
c
.
getDebugLog
(
job
)
When troubleshooting a job, having the SLURM job ID can be helpful to understand what might have occurred in the job. You can obtain this by calling `getTaskSchedulerIDs() in the Command Window:
>>
job
.
getTaskSchedulerIDs
()
ans
=
25539
To Learn More
¶
To learn more about the MATLAB Parallel Computing Toolbox, checkout these resources:
Parallel Computing Overview
Parallel Computing Documentation
Parallel Computing Coding Examples
Parallel Computing Tutorials
Parallel Computing Videos
Parallel Computing Webinars
Next
Package Managers
Previous
Using R
Copyright © 2024, RC
Made with
Furo
On this page
Using MATLAB
Installing MATLAB toolboxes
Using MATLAB Parallel Server
Configuring Jobs
Using the MATLAB Client Interactively on the HPC Cluster
Using the MATLAB with a Batch Job on the HPC Cluster
Parallel MATLAB Batch Job
Debugging
To Learn More

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/fromsource/makefile.html

Make - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Make
¶
Important
Be sure to refer to the installation instructions provided with software being installed. If the software requires additional dependencies not installed on the system, they might need to installed and added to your
PATH
similarly.
If you want to use
make
to add software locally to your path, you must first download the software package from its source (e.g., its webpage or GitHub) and unpack it or unzip it if need be. Then, you must set the installation path to a directory with write access on the cluster, such as your home directory or your
/work
.
Note
You can use
./configure
to specify the installation path (e.g.,
./configure
--prefix=${HOME}/software
).
After setting the installation path, compile the code via
make
and then install the software using
make
install
.
Makefile Example: Installing FFTW Library
¶
Even without root access, you can install software system-wide by installing it in your home directory. Let us continue with the FFTW library as an example.
Download the FFTW tarball. Here, we download version 3.3.9:
cd
~
wget
http://www.fftw.org/fftw-3.3.9.tar.gz
Extract the tarball:
tar
xzf
fftw-3.3.9.tar.gz
Move into the directory:
cd
fftw-3.3.9
Configure the build process, specifying the prefix as a location in your home directory. This location is where the library will be installed. Note that the specified directory should be in your
PATH
to ensure system-wide accessibility.
./configure
--prefix
=
$HOME
/fftw
Compile the software:
make
-j
8
Instead of the default
make
install
, which typically requires root access for system-wide installation, you can
make
install
with the prefix configuration to install the software in your home directory.
make
install
The FFTW library should now be installed in the
fftw
directory in your home directory.
Important
Include the location of the software in your
PATH
to access directly. This can be done by adding the following line to your
~/.bashrc
:
export
PATH
=
$HOME
/fftw/bin:
$PATH
This puts the program in your path so that the system can find the FFTW library binaries when called.
Note
You need to source your profile or restart your shell for these changes to take effect, which is done as follows:
source
~/.bashrc
Makefile Example: Installing LAMMPS
¶
See also
CMake Example: Parallel LAMMPS Build
Note
There are no configure options used and the information is stored within the makefiles
mylammps/MAKE
in the
make.serial
and
make.mpi
files.
The following instructions to build LAMMPS using make.
To allocate an interactive job on compute node type:
srun
-N
1
-n
28
--constraint
=
ib
--pty
/bin/bash
Load the following modules required for building LAMMPS:
module
load
openmpi/4.0.5
module
load
python/3.6.6
module
load
gcc/9.2.0
Change the directory to the
src
directory using the command
cd
/path/to/mylammps/src
Use the following command to build serial version or the MPI version of LAMMPS depending on the requirement. This will generate
lmp_serial
binary for a serial build and
lmp_mpi
for an MPI build.
make
serial
make
mpi
Now you can start running the program, using
./lmp_serial
or
mpirun
-n
1
./lmp_mpi
-h
.
Next
CMake
Previous
From Source
Copyright © 2024, RC
Made with
Furo
On this page
Make
Makefile Example: Installing FFTW Library
Makefile Example: Installing LAMMPS

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Container

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Scaling-efficiency

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Home-Directory

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Job-Script

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Fair-Share-Allocation

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/conda.html

Conda - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
See also
Should I use Anaconda or Miniconda
?
Conda
¶
Conda
is an open-source environment and package manager.
Miniconda
is a free installer for Conda and Python and comes with a few other packages.
Anaconda
is also a package manager that has a much larger number of packages pre-installed.
Managing Conda Environments
¶
Creating Environments
¶
Note
We recommend avoiding building Conda environments in your
/home
, for its space quota. Instead, Use
/work
, which can be requested by PIs for groups in need of space
/work
.
See also
Learn about storage options
and
Submit New Storage Space request
.
Installing local virtual environment using Conda is recommended on the cluster. You can have multiple environments with different packages for each, which allows project’s environments to be independent of others. You only have to load the
anaconda3
module.
From the login node, log-in to a compute node.
Request one node on the
short
partition with 1 CPU core. Then, load the
anaconda3/2022.05
module.
¶
1
srun
--partition
=
short
--nodes
=
1
--cpus-per-task
=
1
--pty
/bin/bash
2
module
load
anaconda3/2022.05
To create a new Conda environment where
<environment-name>
is the path and name. You can see a list of your existing environments with
conda
env
list
.
conda
create
--prefix
=
/<path>/<environment-name>
python
=
3
.11
anaconda
Attention
Do NOT automatically initialize conda on startup, as it sometimes interferes with other environments on the HPC. If you have previously set conda to initialize on startup, remove the conda initialization script from the
.bashrc
file. See
Conda and .bashrc
for more details.
Follow the prompts to complete the Conda install, then activate the environment.
source
activate
/<path>/<environment-name>
Your command line prompt will then include the path and name of environment.
(
/<path>/<environment-name>
)
[
<username>@c2001
dirname
]
$
Tip
The
conda
config
--set
env_prompt
'({name})
'
command modifies your
.condarc
to show only the environment, which displays as follows:
(
<environment-name>
)
[
<username>@c2000
dirname
]
$
With your Conda environment activated you can install a specific package with
conda
install
[
packagename
]
To deactivate the current active Conda environment
conda
deactivate
To delete a Conda environment and all of its related packages, run:
conda
remove
-n
yourenvironmentname
--all
Listing Environments
¶
You can view the environments you’ve created in your home directory by using the following command
conda
env
list
# conda environments:
#
MlGenomics
$HOME
/.conda/envs/MlGenomics
base
$HOME
/miniconda3
To list the software packages within a specific environment, use
conda
list
--name
env_name
If you’ve created an environment in a different location, you can still list its packages using:
conda
list
--prefix
/path/to/env
Exporting Environment
¶
For ensuring reproducibility, it’s recommended to export a list of all packages and versions in an environment to an environment file. This file can then be used to recreate the exact environment on another system or by another user. It also serves as a record of the software environment used for your analysis.
Removing Environments
¶
When you need to remove an environment located in your home directory, execute:
conda
env
remove
--name
env_name
For environments located elsewhere, you can remove them using:
rm
-rf
/path/to/env
Clean Conda Environment
¶
To remove packages that are no longer used by any environment and any downloaded tarballs stored in the conda package cache, run:
conda
clean
--all
By following these guidelines, you can efficiently manage your Conda environments and packages, ensuring reproducibility and a clean system.
Using Miniconda
¶
This procedure assumes that you have not installed Miniconda. If you need to update Miniconda, do not follow the installation procedure. Use
conda
update
. This procedure uses the Miniconda3 version with Python version 3.8 in step 2, although there are other versions you can install (e.g., 3.9 or 3.11).
Installing Miniconda
¶
Attention
Make sure to log on to a compute node.
srun
--partition
=
short
--nodes
=
1
--cpus-per-task
=
1
--pty
/bin/bash
Download Miniconda, check the hash key, and install as follows:
wget
--quiet
https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sha256sum
Miniconda3-latest-Linux-x86_64.sh
bash
Miniconda3-latest-Linux-x86_64.sh
-b
-p
<dir>
Where
<dir>
is the full path to your desired installation directory (e.g.,
/work/mygroup/miniconda3
).
Activate the base Miniconda environment
source
<dir>/bin/activate
You can now create a new environment with this command where we are using python version 3.8:
conda
create
--name
my-py38env
python
=
3
.8
Type
y
if asked to proceed with the installation.
Now you can activate your new environment
conda
activate
my-py38env
To deactivate the environment, type
conda
deactivate
. You can type this command again to deactivate the base Miniconda environment.
Conda and
.bashrc
¶
In addition to editing your
.bashrc
file as outlined in the example above, programs you install can also modify your
.bashrc
file. For example, if you follow the procedure outlined in
Using Miniconda
, there may be a section added to your
.bashrc
file (if you didn’t use the
-b
batch option) that automatically loads your conda environment every time you sign in to Discovery. See the figure below for an example of this:
# .bashrc
# Source global definitions
if
[
-f
/etc/bashrc
]
;
then
.
/etc/bashrc
fi
# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=
# User specific aliases and functions
# >>> conda initialize >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup
=
"
$(
'/home/$USER/miniconda3/bin/conda'
'shell.bash'
'hook'
2
>
/dev/null
)
"
if
[
$?
-eq
0
]
;
then
eval
"
$__conda_setup
"
else
if
[
-f
"/home/
$USER
/miniconda3/etc/profile.d/conda.sh"
]
;
then
.
"/home/
$USER
/miniconda3/etc/profile.d/conda.sh"
else
export
PATH
=
"/home/
$USER
/miniconda3/bin:
$PATH
"
fi
fi
unset
__conda_setup
# <<< conda initialize <<<
You should not modify this section in the
.bashrc
file directly. If it was changed, remove this section manually using a file editor.
Caution
We recommend removing the conda initialization section from your
.bashrc
as it may interfere with the correct startup environment when using Open OnDemand apps. You should always load your Conda environment after your job has already started.
If you need help with your
.bashrc
file or would like it restored to its default, reach out to the RC team at
mailto:rchelp
@
northeastern
.
edu
, and we can provide you with
a new default
.bashrc
file and help troubleshoot issues with the file.
Conda Best Practices
¶
See also
Best practices for home storage:
Conda
.
Your
~/.conda
may get very large if you install multiple packages and create many virtual Conda environments. Make sure to clean the Conda cache and clean unused packages with:
conda
clean
--all
.
Clean unused Conda environments by first listing the environments with:
conda
env
list
, and then removing unused ones:
conda
env
remove
--name
<environment-name>
.
You can build Conda environments in different locations to save space on your home directory (see
Data Storage Options
). You can use the
--prefix
flag when building your environment. For example:
conda
create
myenv
--prefix=/work/<mygroup>/<mydirectory>
.
Another recommended step is to update your Conda version (possible only when using Miniconda):
conda
update
conda
-y
Next
Spack
Previous
Package Managers
Copyright © 2024, RC
Made with
Furo
On this page
Conda
Managing Conda Environments
Creating Environments
Listing Environments
Exporting Environment
Removing Environments
Clean Conda Environment
Using Miniconda
Installing Miniconda
Conda and
.bashrc
Conda Best Practices

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/classroom/index.html

Classroom Resources - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Classroom Resources
¶
High-Performance Computing (HPC) is not only for researchers; it is an integral part of modern education. In this section, we offer guidance for instructors and students who want to use our HPC resources in the classroom.
Course guide
Read our guide on teaching or taking a course on the NU HPC.
Learn more »
Course Guide
Courses Cheetsheet
A one-page guide to common unix commands, OOD troubleshooting tips, and more.
Learn more »
Courses Cheatsheet
CPS Class Instructions
Instructions for opening a jupyterlab session for students and instructors in CPS
Learn more »
CPS Class Instructions
Leveraging HPC in the classroom can enrich the learning experience, allowing students to tackle real-world problems. Explore this section to understand how you can make the most of our HPC resources for educational purposes. For further assistance, please contact our educational support team at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy teaching and learning!
Next
Course Guide
Previous
Slurm Jobs Array
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Resource-Reservation

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/spack.html

Spack - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Spack
¶
Research Computing recommends using
Spack
to conveniently install software packages locally to your path. Please refer to the
Spack documentation
for the latest information about the
packages
that Spack contains. To use Spack, you first need to copy it to your
/home
directory or a
/work
directory, then you need to add it to your local environment.
Note
Spack software installations are part of your research and should preferably be stored in your PI’s
/work
directory.
Install Spack
¶
These instructions will demonstrate how to install Spack in your
/home
(
non-shared
) or
/work
(
shared
) directory and then how to add Spack to your local environment while on a compute node, so you have access to the Spack commands (steps 4-5).
Non-shared
Copy Spack’s Git repository to ‘$HOME’
git
clone
-c
feature.manyFiles
=
true
https://github.com/spack/spack.git
Shared
Copy Spack’s Git repository to
/work
and modify directory permissions to give write access to the members of your PI’s
/work
.
cd
/work/<PI-Project-Dir>
git
clone
-c
feature.manyFiles
=
true
https://github.com/spack/spack.git
chmod
-R
775
spack/
Install a software using Spack
¶
Request a compute node interactively:
srun
-p
short
--pty
-N
1
-n
28
/bin/bash
. While building the software Spack will attempt to run
make
in parallel. Hence, you need to request a compute node with multiple cores. This
srun
request is for 28 cores on one node (
-N
1
-n
28
).
Any module that is required for your software installation needs to be in your
$PATH
prior to adding Spack to your local environment. For example, to use a newer version of python for compatibility with Spack, type:
module
load
python/3.8.1
.
Add Spack to your local environment, so you can use the Spack commands. If Spack has been installed on
$HOME
:
For
Spack
on
$HOME
export
SPACK_ROOT
=
/home/<yourusername>/spack
.
$SPACK_ROOT
/share/spack/setup-env.sh

For
Spack
on
/work/<PI-Project-Dir>
export
SPACK_ROOT
=
/work/<PI-Project-Dir>/spack
.
$SPACK_ROOT
/share/spack/setup-env.sh
After you have the Spack commands in your environment, type
spack
help
to ensure Spack is loaded in your environment and to see the commands you can use with Spack. You can also type
spack
list
to see all the software that you can install with Spack, but note this command can take a few moments to populate the list.
To check your spack version:
spack
--version
.
To see information about a specific software package, including options and dependencies:
spack
info
<software
name>
. Make sure to note the options and/or dependencies that you want to add or not add before installing the software.
To install a software package plus any dependencies or options:
spack
install
<software
name>
+<any
dependencies
or
options>
;
you can specify
-<any
dependencies
or
options>
. You can also list
+
or
-
different options and dependencies within the same line. Do
not put a space between each option/dependency that you list.
To view information about your installed software packages:
spack
find
<software
package
name>
or
spack
info
<software
package
name>
.
To Install a specific version of the software:
spack
install
<softwarename@version>
.
When you have installed a software package, you can add it to the module system by executing this command:
.
$SPACK_ROOT/share/spack/setup-env.sh
Example: Installing LAMMPS
¶
This section details how to install the LAMMPS application with the
KOKKOS and User-reaxc packages using Spack. This example assumes that
you do not have any previous versions of LAMMPS installed. If you have
any previous versions of LAMMPS, you must uninstall them before using
this procedure. To see if you have any previous versions of LAMMPS,
type
spack
find
lammps
. If you do have a previous version, you will
need to uninstall LAMMPS by typing
spack
uninstall
--dependents
lammps
. Then, you can follow the instructions below. Note that the
installation can take about two hours to complete. As part of the
procedure, we recommend that you initiate a
tmux session
so that
you can have the installation running as a separate process if you
need to do other work on Discovery. If you decide to use tmux, make
note of the compute node number (compute node numbers start with c or
d with four numbers, such as c0123) to make it easier to check on the
progress of the installation.
If LAMMPS has a dependency on a specific
gcc
compiler, then do the following before starting the installation procedure. This will update the
compilers.yaml
file located in
$HOME/.spack/linux
.
cd
$HOME/.spack/linux/
Open
compilers.yaml
and copy-paste a
compiler
entry at the end of the file.
Edit ‘spec’ and ‘path’ to indicate the version of the GCC compiler that is required for installation.
For
example:
spec:
gcc@
=
8
.1.0
paths:
cc:
/shared/centos7/gcc/8.1.0/bin/gcc
cxx:
/shared/centos7/gcc/8.1.0/bin/g++
f77:
/shared/centos7/gcc/8.1.0/bin/gfortran
fc:
/shared/centos7/gcc/8.1.0/bin/gfortran
The
compilers.yaml
file should now have the desired
gcc
version as its latest
compiler
entry.
Assuming that Spack has already been installed at a desired location. For installing gpu-supported LAMMPS, request a GPU node for 8 hours:
srun
--partition
=
gpu
--nodes
=
1
--ntasks
=
14
--pty
--gres
=
gpu:1
--mem
=
16GB
--time
=
08
:00:00
/bin/bash
Load compatible CUDA, GCC, and Python modules and activate Spack from the installed location.
module
load
cuda/10.2
gcc/8.1.0
python/3.8.1
export
SPACK_ROOT
=
/work/<PI-Project-Dir>/spack
.
$SPACK_ROOT
/share/spack/setup-env.sh
(Optional) Initiate a
tmux
session:
Start a tmux session:
tmux
.
List tmux sessions:
tmux
ls
Detach from tmux session:
Ctrl+b
d
Attach to tmux session:
tmux
attach-session
-t
0
Exit a tmux session:
Ctrl+d
Type:
spack
install
lammps
+asphere
+body
+class2
+colloid
+compress
+coreshell
+cuda
\
cuda_arch
=
70
+cuda_mps
+dipole
+granular
+kokkos
+kspace
+manybody
+mc
+misc
+molecule
\
+mpiio
+peri
+python
+qeq
+replica
+rigid
+shock
+snap
+spin
+srd
+user-reaxc
+user-misc
Type
spack
find
LAMMPS
to view your installed software package.
Type
spack
load
lammps
.
Next
From Source
Previous
Conda
Copyright © 2024, RC
Made with
Furo
On this page
Spack
Install Spack
Install a software using Spack
Example: Installing LAMMPS

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Concurrency

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Module

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/datamanagement/transferringdata.html

Transfer Data - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Transfer Data
¶
The HPC has a dedicated transfer node that you must use to transfer data to and from the cluster. You cannot transfer data from any other node or the HPC to your local machine. The node name is
<username>@xfer.discovery.neu.edu:
where
<username>
is your Northeastern username to login into the transfer node.
You can also transfer files using Globus. This is highly recommended if you need to transfer large amounts of data. See
Using Globus
for more information.
If you are transferring data from different directories on the HPC, you need to use a compute node (see
Interactive Jobs: srun Command
or
Batch Jobs: sbatch Command
) with SCP, rsync, or the copy command to complete these tasks. You should use the
--constraint=ib
flag (see
hardware-overview
) to ensure the fastest data transfer rate.
Caution
The
/scratch
space is for temporary file storage only. It is not backed up. If you have directed your output files to
/scratch
, you should transfer your data from
/scratch
to another location as soon as possible. See
Data Storage Options
for more information.
Transfer via Terminal
¶
SCP
You can use
scp
to transfer files/directories to and from your local machine and the HPC. As an example, you can use this command to transfer a file to your
/scratch
space on the HPC from your local machine:
scp
<filename>
<username>@xfer.discovery.neu.edu:/scratch/<username>
where
<filename>
is the name of the file in your current directory you want to transfer, and
<username>
is your Northeastern username. So that you know, this command is run on your local machine.
If you want to transfer a directory in your
/scratch
called
test-data
from the HPC to your local machine’s current working directory, an example of that command would be:
scp
-r
<username>@xfer.discovery.neu.edu:/scratch/<username>/test-data
.
where
-r
flag is for the recursive transfer because it is a directory. So that you know, this command is run on your local machine.
Rsync
You can use the
rsync
command to transfer data to and from the HPC and local machine. You can also use
rsync
to transfer data from different directories on the cluster.
The syntex of
rsync
is
rsync
[
options
]
<source>
<destination>
An example of using rsync to transfer a directory called
test-data
in your current working directory on your local machine to your
/scratch
on the HPC is
rsync
-av
test-data/
<username>@xfer.discovery.neu.edu:/scratch/<username>
where this command is run on your local machine in the directory that contains
test-data
.
Similarly,
rsync
can be used to copy from the current working directory on the HPC to your current working directory on your local machine:
rsync
-av
<username>@xfer.discovery.neu.edu:/scratch/<username>/test-data
.
where this command is run on your local machine in the current directory that you want to save the directory
test-data
.
You can also use rsync to copy data from different directories on the HPC:
srun
--partition
=
short
--nodes
=
1
--ntasks
=
1
--time
=
01
:05:00
--constraint
=
ib
--pty
/bin/bash
rsync
-av
/scratch/<username>/source_folder
/home/<username>/destination_folder
sbatch
You can use a sbatch job to complete data transfers by submitting the job to the HPC queue. An example of using
rsync
through a sbatch script is as follows:
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --time=0:05:00
#SBATCH --job-name=DataTransfer
#SBATCH --mem=2G
#SBATCH --partition=short
#SBATCH --constraint=ib
#SBATCH -o %j.out
#SBATCH -e %j.err
rsync
-av
/scratch/<username>/source_folder
/home/<username>/destination_folder
where we are transferring the data from
source_folder
to the
destination_folder
.
SSHFS
If you want to use
sshfs
, use it with the dedicated transfer node
xfer.discovery.neu.edu
. It will not work on the login or compute nodes. On a Mac, you will also have to install macFUSE and sshfs (please refer to
macFUSE
) to use the
sshfs
command.
Use this syntax to perform file transfers with
sshfs
:
sshfs
<username>@xfer.discovery.neu.edu:</your/remote/path>
<your/local/path>
-<options>
For example, this will mount a directory in your
/scratch
named
test-data
to a local directory on your machine
~/mount_point
:
sshfs
<username>@xfer.discovery.neu.edu:/scratch/<username>/test-data
~/mount_point
You can interact with the directory from your GUI or use the terminal to perform tasks on it.
Transfer via GUI Application
¶
OOD’s File Explorer
You can use OOD’s File Explorer application to transfer data from different directories on the HPC and also to transfer data to and from your local machine to the HPC. For more information to complete this please see
file-explorer
.
MobaXterm
You can use MobaXterm to transfer data to and from the HPC. Please check out
MobaXterm
to download MobaXterm.
Open MobaXterm.
Click
Session
, then select
SFTP
.
In the
Remote host
field, type
xfer.discovery.neu.edu
In the
Username
field, type your Northeastern username.
In the
Port
field, type 22.
In the
Password
box, type your Northeastern password and click
OK
. Click
No
if prompted to save your password.
You will now be connected to the transfer node and can transfer files through MobaXterm. Please refer to
MobaXterm
for further information.
FileZilla
You can use FileZilla to transfer data to and from the HPC. Please check out
FileZilla
to download.
Open FileZilla.
In the
Host
field, type
sftp://xfer.discovery.neu.edu
In the
Username
field, type your Northeastern username.
In the
Password
field, type your Northeastern password.
In the
Port
field, type 22.
You will now be connected to the transfer node and can transfer files through FileZilla. Please refer to
FileZilla
for further information.
Next
Using Globus
Previous
Data Storage Options
Copyright © 2024, RC
Made with
Furo
On this page
Transfer Data
Transfer via Terminal
Transfer via GUI Application

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/systemwide/modules.html

Using Module - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Using Module
¶
Note
Some modules conflict, resulting in the software behaving differently than expected. Also, if there are multiple software versions and you load more than one version of the software, only the latest version will be used. Use
module
list
to view the modules loaded into your path.
The ‘modules’ tool is widely used for managing application environments on High-Performance Computing (HPC) systems. This page will provide an overview of ‘modules’, instructions on using them, best practices, use cases, and more.
The module system on the cluster includes many commonly used scientific software packages that you can load into your path when you need them and unload when you no longer need them. In essence, ‘modules’ handle environment variables like
PATH
and
LD_LIBRARY_PATH
to avoid conflicts between software applications.
Use the
module
avail
command to show a list of the most currently available software on the cluster.
Tip
The
which
<target>
prints the path of executable
<target>
in your path (e.g.,
which
python
prints the
python
that will execute if called).
Module commands
¶
The following are common module commands helpful in interacting with software packages on the cluster.
List of common module commands, where
<module>
is the name of the target module.
¶
Module Command
Function
module
avail
Displays a list of all available modules.
module
list
Shows a list of currently loaded modules.
module
show
<module>
View the details of a software package.
module
load
<module>
Loads a specific module.
module
unload
<module>
Unloads a specific module.
module
purge
Unloads all loaded modules.
module
swap
<module1>
<module2>
Replaces
module1
with
module2
.
Caution
module
purge
unloads all modules from your environment, including the default module
discovery/2019-02-21
. This module sets the HTTP proxy needed to access the internet from nodes. If you accidentally purge this module, it automatically reloads by logging out and then back in. You can also load it manually
module
load
.
Module show example
¶
Before loading a module, type
module
show
<name
of
module>
to see if there are any dependencies or commands that you need to execute
before loading the module. Sometimes, a module might depend on loading other modules to work as expected. While modules are convenient for loading software on the cluster, scientific software can come with many packages and dependencies. In addition to the module, you will need to look over other ways to load the cluster’s software.
See also
Software Overview
for installing software on the cluster.
Here is an example of using
module
show
to show details for the Amber software package.
$
module
show
amber
Command-line output.
¶
/shared/centos7/modulefiles/amber/18-mpi:

module-whatis     loads the modules environment for Amber 18 MPI parallel executable
                  on CPU nodes.

Please load the following modules:
module load openmpi/3.1.2
module load amber/18-mpi
module load python/2.7.15

setenv            AMBER_HOME /shared/centos7/amber/amber18-cpu
prepend-path      PYTHONPATH /shared/centos7/amber/amber18-cpu/lib/python2.7/site-packages
prepend-path      PATH /shared/centos7/amber/amber18-cpu/bin
prepend-path      LD_LIBRARY_PATH /shared/centos7/amber/amber18-cpu/lib
prepend-path      C_INCLUDE_PATH /shared/centos7/amber/amber18-cpu/include
prepend-path      CPLUS_INCLUDE_PATH /shared/centos7/amber/amber18-cpu/include
Module load and unload example
¶
The software module
stata/15
was loaded and unloaded in the following code snippet. After each,
module
list
displays loaded modules showing whether or not STATA was loaded.
Loading Stata version 15.
¶
$
module
load
stata/15
$
module
list
Currently
Loaded
Modulefiles:
1
)
discovery/2019-02-21
2
)
stata/15
Unloading Stata version 15.
¶
$
module
unload
stata/15
$
module
list
Currently
Loaded
Modulefiles:
1
)
discovery/2019-02-21
Launching applications via X11 Forwarding
¶
If you are attempting to open a GUI-based software application that  uses X11 forwarding to display, such as MATLAB or Maestro, and you get an error such as
Error:
unable
to
open
display
localhost:19.0
, this is most likely due to an issue with passwordless SSH. See
connect-to-cluster
for tips and troubleshooting information opening applications that use X11 forwarding.
Advanced Module Usage
¶
Explain how to use modules in job scripts or interactive sessions, using module save and module restore to manage module collections and other advanced topics.
Creating Module Files
¶
See also
installing-your-own-module
.
Best Practices Using Modules
¶
Only load the modules you need: Unnecessary modules can cause conflicts.
Know module hierarchies: Some modules might only become available after loading another module.
Always load a specific module version: This avoids problems if the default version changes.
Common Issues and Troubleshooting Modules
¶
Cover common issues users might face while using ‘modules’, like conflicts, missing modules, or unexpected behavior, and provide troubleshooting tips.
Use-Cases for Modules
¶
Provide several examples of how to use ‘modules’ for various use cases. This could include:
Loading modules for a specific software stack for a project.
Swapping compiler modules to test code compatibility.
Using module collections to switch between different project environments easily.
Next
Using MPI
Previous
System-wide Software
Copyright © 2024, RC
Made with
Furo
On this page
Using Module
Module commands
Module show example
Module load and unload example
Launching applications via X11 Forwarding
Advanced Module Usage
Creating Module Files
Best Practices Using Modules
Common Issues and Troubleshooting Modules
Use-Cases for Modules

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/gpuoverview.html

GPUs on the HPC - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
GPUs on the HPC
¶
This page covers the
Graphics Processing Unit (GPU)
resources available on the
cluster
.
See also
GPU Hardware details for different types of GPUs available on Discovery.
The
gpu
partition
is the general GPU resource for HPC users looking to use a GPU;
multigpu
is the alternative, where more than one GPU are accessible.
See also
Learn more about partitions.
Anyone with a cluster account has access to the
gpu
partition. However, you must submit a
ServiceNow ticket
requesting temporary access to
multigpu
provided sufficient need and preparation.
Note
When working with shared computational resources, it is important to remember not to leave the jobs idle.
Name
Requires Approval?
Time in Hours (Default/Max)
Submitted Jobs
GPU per Job Limit
User Limit (No. GPUs)
gpu
No
4/8
4/100
1
4
multigpu
Yes
4/24
8/100
8
8
Important
Consider the compatibility of the GPU, as some programs do not work on the older k40m or k80 GPUs.
Execute the following command to display the
non-Kepler
GPUs that are available:
sinfo -p gpu –Format=nodes,cpus,memory,features,statecompact,nodelist,gres
This indicates the state (idle or not) of gpu-types and could be helpful to find one that is
idle
. However, the command does not give real-time information of the state and should be used carefully.
Next
GPU Access
Previous
Working with GPUs
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/datamanagement/index.html

Data Management - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Data Management
¶
Data management is a critical aspect of utilizing our High-Performance Computing (HPC) environment. This section provides comprehensive guidance on handling data within the cluster, storage options, and secure data transfers.
Data Storage Options
Selecting the appropriate storage option can significantly impact your work’s efficiency.
Learn more »
Data Storage Options
Transfer Data
Transferring data securely and efficiently is fundamental.
Learn more »
Transfer Data
Using Globus
Globus offers a user-friendly interface for transferring large datasets.
Learn more »
Using Globus
Efficient data management is key to a productive experience in the HPC environment. Explore the topics above to gain insights, follow best practices, and utilize our state-of-the-art tools. For further assistance, please contact our support team at
rchelp
@
northeastern
.
edu
or refer to our comprehensive
Frequently Asked Questions (FAQs)
section.
Happy data handling!
Next
Data Storage Options
Previous
Access to Multi-GPU Partition
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/classroom/cps_ood.html

CPS Class Instructions - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
CPS Class Instructions
¶
Important
The following instructions will only work for CPS classes.
These instructions describe the process of opening a CPS JupyterLab environment on the
Open OnDemand (OOD)
on the cluster web portal and accessing class
/courses
folders.
Note
Due to problems with launching OOD on
Safari
, we recommend using
Google Chrome
,
Mozilla Firefox
or
Microsoft Edge
browsers instead for best experience.
Open JupyterLab For Classes
¶
Important
The class instructor needs to fill in the: [Discovery Classroom Use Request] You will only be able to find your class resources if a request was already made.
In a web browser, go to
http://ood.discovery.neu.edu
. Login with your NU credentials.
Under the
Courses
menu, select your Class Name (For example:
ALY6080 JupyterLab
).
Select the default options and click
Launch
. Wait until the session is successfully created and ready to be launched (turns green).
For more control of the session, modify
Time
for the session time (in hours),
Memory
to get more memory in GB, and the
Working Directory
where JupyterLab will launch.
Note
If
Working Directory
is left blank, the session will launch in the main class folder (in this example
/courses/ALY6080.202335/data
). Alternatively, start the session directly from your personal working directory by entering:
/courses/ALY6080.202335/students/[username]
, where
[username]
is your username on Discovery. The instructions below assume the field is left blank.
courses/
└── ALY6080.202335
    ├── data
    ├── staff
    │   ├── doug
    │   ├── evelyn
    │   └── frank
    └── students
        │── alice
        │── bab
        └── cindy
The above file tree represents the folder structure for the
ALY6080.202335
course.
Click
Connect to Jupyter
to open JupyterLab. This will open a JupyterLab interface in another tab.
Caution
Select
Cancel
when prompted with the
Build Recommended
option. The package jupyterlab-dash does not require a build, and will not work when build is enabled.
Access Class Directories
¶
After you are connected to a CPS JupyterLab session on OOD, you can access any shared class directories and your private class directory.
You can navigate between the class folders using the left menu. The files in the shared class directory are read ony and the students do not have permissions to edit or remove the files. Your instructor may use the shared class directory to share files related to the classwork
Navigate to your personal class directory under:
/courses/[coursename.termcode]/students/<your
username>
. Now you can create and edit Jupyter notebook files here.
Open a new Python notebook session from the Launcher menu by clicking the
Python 3 (ipykernel)
.
A new file will be created inside your directory called
Untitled.ipynb
. You can rename it by right-clicking on it and using the rename option.
This Python notebook has ready-to-use Python packages needed for your class.
Note
Permission Denied errors:
Do not attempt to create, edit or write files that are outside your personal student directory. Most “Permission Denied” errors are due to directories or files having read-only access permissions.
Next
Containers on HPC
Previous
Courses Cheatsheet
Copyright © 2024, RC
Made with
Furo
On this page
CPS Class Instructions
Open JupyterLab For Classes
Access Class Directories

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/clusterusage.html

Cluster Usage - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Cluster Usage
¶
When using the cluster, it is important to use the appropriate resources for different tasks.
Login vs. Compute Node
¶
Once you have logged into the cluster, it is important to run CPU-intensive activities on compute nodes by submitting a slurm job.
See also
Batch Jobs: sbatch Command
and
Interactive Jobs: srun Command
for more information on creating a slurm job.
Performing CPU-intensive activities on the login nodes is detrimental to the performance of for all cluster users and it will not provide the best performance for the tasks you are trying to accomplish.
Conversely, if you allocate CPU or GPU resources through a slurm job, it is important to use them or end your job, as other users may be waiting for the resources to be freed.
Important
There are bots monitoring the usage of the login nodes and compute nodes that identify inappropriate resource usage, alerting both RC and the user in question via email.
Transferring Data
¶
If you are attempting to transfer data, we have a dedicated transfer node that you should use.
See also
Transfer Data
.
Next
Glossary
Previous
Shell Environment on the Cluster
Copyright © 2024, RC
Made with
Furo
On this page
Cluster Usage
Login vs. Compute Node
Transferring Data

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Job-Priority

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/containers/singularity.html

Singularity on Discovery - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Singularity on Discovery
¶
Singularity is the container runtime engine for the Discovery cluster and is installed as a module. With Singularity, you can run existing containers or pull and run your own custom container.
How to run a container with Singularity
¶
You can run a container image with the
run
or
exec
commands after moving to a compute node and loading the singularity module.
To see available singularity versions run this in the command line: module avail singularity
Important
The
--bind
or
-B
flag
It’s important to mount the directories in the Discovery cluster to the container image so you can access necessary input data and to output the results of your software to the directory you specify.
For example, this command will allow you to access your directories on /work and /scratch while running the container.
-B “/work:/work,/scratch:/scratch”
Example using an image already located on the file system
¶
We have several container images located in /shared/container_repository
Here’s an
srun
example using a singularity image from the /shared/container_repository
srun
--pty
/bin/bash
module
load
singularity/3.10.3
singularity
run
-B
"/work:/work"
/shared/container_repository/star/star_2.7.8a.sif

Singularity>

Singularity>
STAR
Usage:
STAR
[
options
]
...
--genomeDir
/path/to/genome/index/
--readFilesIn
R1.fq
R2.fq
Spliced
Transcripts
Alignment
to
a
Reference
(
c
)
Alexander
Dobin,
2009
-2020

STAR
version
=
2
.7.8a
STAR
compilation
time,server,dir
=
Wed
Nov
15
00
:19:03
UTC
2023
f81ff285a72e:/opt/STAR-2.7.8a/source
For
more
details
see:
<https://github.com/alexdobin/STAR>
<https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf>

To
list
all
parameters,
run
STAR
--help
An example sbatch script
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=01:00:00
#SBATCH --partition=short
#SBATCH --job-name=star
#SBATCH --output=star.out
#SBATCH --error=star.err
#SBATCH --ntasks=1
module
load
singularity/3.10.3
cd
/work/mygroup

singularity
exec
-B
"/work:/work"
/shared/container_repository/star/star_2.7.8a.sif
STAR
\
--genomeDIR
/path/to/genome
\
--readFilesIn
R1.fq
R2.fq
Pull an image from a URL
¶
with the command
singularity
pull
you can pull an existing container image from the repositories listed below:
Docker Hub
Singularity Hub
Nvidia Container Repository
Important
The Importance of Tags and Scientific Reproducibility
Container images are cataloged using “tags”. The tag can indicate the version of the image build or some other aspect of the image. Common tags include “latest” and will pull the most recent version of the image. Care should be taken to distinguish the container version from the version of the software installed, as the same tag on a container image (i.e., “latest”) will pull different versions of software over time. We recommend pulling specific versions of images to better track the version used.
Example pulling Spades image from Dockerhub
¶
We’ll use a compute node with the InfiniBand network to pull the image. The process of pulling an image and converting it to singularity format (.sif) will take longer for large images.
More about the spades metagenome assembler image can be found
here
.
srun
--constraint
=
ib
-p
short
--pty
/bin/bash
module
load
singularity/3.10.3
cd
/work/groupname/container_images
mkdir
-p
cache
tmp
export
SINGULARITY_CACHEDIR
=
$(
pwd
)
/cache
export
SINGULARITY_TMPDIR
=
$(
pwd
)
/tmp
singularity
pull
spades_3.15.5.sif
docker://staphb/spades:3.15.5
# This will pull an image which will be named spades_3.15.5.sif
# There will be several warnings (warn xattr{}) which can be ignored
# Test the container with singularity run remember to use the "-B" flag to bind any directories
singularity
run
spades_3.15.5.sif
Singularity>
spades.py
--test
This container image can now be used as in the srun and sbatch examples above.
Note
Pulling container images to the Discovery cluster can contribute to your storage use in /home. If a cache and tmp directory aren’t created and exported as in the example above, these items will be deposited in a hidden directory in your home at .singularity.
Please check your home storage usage regularily to stay below the quota.
See
Home Directory Storage Quota
for more.
Next
Best Practices
Previous
Containers on HPC
Copyright © 2024, RC
Made with
Furo
On this page
Singularity on Discovery
How to run a container with Singularity
Example using an image already located on the file system
Pull an image from a URL
Example pulling Spades image from Dockerhub

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/systemwide/mpi.html

Using MPI - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Using MPI
¶
Messaging Passing Interface (MPI)
is a standardized and portable message-passing system designed to function on a wide variety of parallel computing architectures. It provides a library of functions that enable a program to distribute a computational task across multiple nodes in a cluster.
There are multiple implementations of MPI including
OpenMPI
(Open Source Message Passing Interface),
MPICH
, and
Intel MPI
. OpenMPI is a widely used MPI implementation in the HPC community and the one we will be working with through our documentation.
Getting Started with MPI
¶
To get started with MPI on a Slurm-based HPC cluster, you should have:
Basic knowledge of Linux/Unix commands
Familiarity with a programming language supported by MPI (e.g., C, C++, FORTRAN) if you are developing a program
Understand how to load MPI module on the HPC
Understand how to compile your source code and run the binaries (compiled languages) or to run the interpreted language with MPI
MPI libraries on Discovery
¶
There are many versions of OpenMPI, MVAPICH, and MPICH that are available on the HPC as modules compiled with different compilers and additional libraries and features. To see them, use the
module
avail
openmpi
,
module
avail
mpich
, and
module
avail
mvapich
respectively.
Use the
module
show
command to view information about the compilers you need to use with these libraries and if they support InfiniBand (IB) or not. For example,
module
show
openmpi/4.1.0-zen2-gcc10.1
.
Output for
module
show
openmpi/4.1.0-zen2-gcc10.1
/shared/centos7/modulefiles/openmpi/4.1.0-zen2-gcc10.1:

module-whatis
Loads
the
executables,
libraries
and
headers
for
OpenMPI
v.
4
.1.1.
Built
using
Intel
2021
compilers
on
AMD
EPYC
architecture
(
zen2
)
.

Please
note
-
this
MPI
module
supports
communication
through
the
HDR200
InfiniBand
network
by
using
the
Mellanox
(
OFED
5
.3
)
UCX
(
1
.10.1
)
framework
with
cross
platform
unified
API.
To
make
sure
InfiniBand
is
being
used,
make
sure
to
compile
and
run
your
applications
using
this
module
only
on
AMD
EPYC
architectures
(
zen2
)
.

To
allocate
the
zen2
arch
compute
node,
add
the
following
flag
to
your
SLURM
command:
--constraint
=
zen2
For
more
details:
https://rc-docs.northeastern.edu/en/latest/hardware/hardware_overview.html

To
use
the
module,
type:
module
load
gcc/10.1.0
module
load
openmpi/4.1.0-zen2-gcc10.1


conflict
openmpi
prepend-path
PATH
/shared/centos7/openmpi/4.1.0-zen2-gcc10.1/bin
prepend-path
MANPATH
/shared/centos7/openmpi/4.1.0-zen2-gcc10.1/share/man
prepend-path
LD_LIBRARY_PATH
/shared/centos7/openmpi/4.1.0-zen2-gcc10.1/lib
prepend-path
CPATH
/shared/centos7/openmpi/4.1.0-zen2-gcc10.1/include
prepend-path
LIBRARY_PATH
/shared/centos7/openmpi/4.1.0-zen2-gcc10.1/lib
setenv
OMPI_MCA_btl
^vader,tcp,openib,uct
Running a MPI Program
¶
The following is a basic slurm script for running an MPI program with annotations:
#!/bin/bash
#SBATCH --job-name=test_job         # Set the job name
#SBATCH --output=res_%j.out         # Set the output file name (%j expands to jobId)
#SBATCH --ntasks=4                  # Request 4 tasks
#SBATCH --time=01:00:00             # Request 1 hour runtime
#SBATCH --mem-per-cpu=2000          # Request 2000MB memory per CPU
module
load
openmpi/4.0.5
# Load the necessary module(s)
mpirun
-n
4
./your_program
# Run your MPI executable
Note
For MPI tasks,
--ntasks=X
is used, where
X
requests the number of cpu cores for tasks.
This script specifies that it needs 4 tasks (i.e., CPU cores), a maximum of 10 minutes of runtime, and 2000MB of memory per CPU. It then loads the OpenMPI module and runs the MPI program using mpirun.
Tip
Best practice for writing your sbatch script is including the versions of the modules you are loading to ensure you always have your expected environment on the HPC.
OpenMPI Tuning for Performance Optimization
¶
OpenMPI provides a variety of environment variables that can be used to optimize the runtime characteristics of your MPI program for maximum performance. For instance, you can specify which network interfaces to use by setting the
OMPI_MCA_btl
variable:
export
OMPI_MCA_btl
=
self,vader,tcp
Tip
You can also include or exclude certain network interfaces by setting the
OMPI_MCA_btl_tcp_if_include
or
OMPI_MCA_btl_tcp_if_exclude
variables.
Also you can check if certain MPI modules already have certain
OMPI_MCA_btl
set by using the
module
show
command and looking for the
setenv
options listed.
In addition, OpenMPI lets you control the placement of processes on nodes, which can be critical for performance. The
--map-by
and
--bind-to
options dictate how processes are mapped to hardware resources and how they are bound to those resources, respectively.
Remember, optimizing for performance often requires a thorough understanding of your application, your hardware, and MPI.
Troubleshooting and Debugging MPI Programs
¶
Debugging MPI programs can be challenging due to their parallel nature. Fortunately, OpenMPI provides several tools and techniques to help with this.
One useful feature is verbose error reporting. To enable this, set the
OMPI_MCA_mpi_abort_print_stack
to
1
:
export
OMPI_MCA_mpi_abort_print_stack
=
1
If you have a parallel debugger such as TotalView or DDT, you can use it with OpenMPI using the
mpiexec
command with the
-tv
or
-debug
options, respectively.
Finally, remember to check your slurm job output files for any error messages or abnormal output. Sometimes, the issue may be with how you are running your job rather than with your MPI program itself.
Benchmarking OpenMPI Performance
¶
Benchmarking is a method used to measure the performance of a system or one of its components under different conditions. For MPI, benchmarks can be used to measure its communication and computation efficiency on different high-performance computing (HPC) systems. By comparing these benchmarks, you can identify potential bottlenecks and areas for improvement to optimize the performance of MPI.
Tools for Benchmarking
¶
There are several tools available for benchmarking MPI, including the following:
HPC Challenge (HPCC)
:
This benchmark suite measures a range of metrics, including latency and bandwidth, as well as floating-point computation performance.
Intel MPI Benchmarks (IMB)
:
A suite of benchmarks provided by Intel specifically for MPI. It includes a set of MPI-1 and MPI-2 function benchmarks and measures point-to-point communication, MPI data types, collective communication, and more.
OSU Micro-Benchmarks (OSU-MB)
:
A lightweight set of benchmarks designed to measure latency, bandwidth, and other performance metrics for various MPI functions.
To use these tools, you generally need to download and compile them, and then run them using a slurm job script.
Developing with MPI
¶
Hello world program
¶
The fundamental concept in MPI is the communicator, which defines a group of processes that can send messages to each other. By default, all processes belong to the
MPI_COMM_WORLD
communicator. Here is a simple C++ program that using MPI:
#include
<mpi.h>
#include
<stdio.h>
int
main
(
int
argc
,
char
**
argv
)
{
MPI_Init
(
NULL
,
NULL
);
// Get the number of processes
int
world_size
;
MPI_Comm_size
(
MPI_COMM_WORLD
,
&
world_size
);
// Get the rank of the process
int
world_rank
;
MPI_Comm_rank
(
MPI_COMM_WORLD
,
&
world_rank
);
printf
(
"Hello world from processor %d out of %d processors
\n
"
,
world_rank
,
world_size
);
MPI_Finalize
();
}
In this code,
MPI_Init
initializes the MPI environment,
MPI_Comm_size
gets the number of processes,
MPI_Comm_rank
gets the rank (ID) of the process, and
MPI_Finalize
ends the MPI environment. In the C/C++ language, the
#include
<mpi.h>
header file needs to be added to compile MPI code.
To understand how to run an MPI program, let us write a simple program that prints a
"Hello,
World!"
message from each process.
First, create a file called
hello_world.c
in your preferred text editor and add the following code:
#include
<mpi.h>
#include
<stdio.h>
int
main
(
int
argc
,
char
**
argv
)
{
MPI_Init
(
NULL
,
NULL
);
int
world_rank
;
MPI_Comm_rank
(
MPI_COMM_WORLD
,
&
world_rank
);
int
world_size
;
MPI_Comm_size
(
MPI_COMM_WORLD
,
&
world_size
);
printf
(
"Hello, World! I am process %d out of %d
\n
"
,
world_rank
,
world_size
);
MPI_Finalize
();
return
0
;
}
This program initializes the MPI environment, gets the rank of the process, gets the total number of processes, and then prints a message. Finally, it finalizes the MPI environment.
Next, compile the program using the
mpicc
command, which is a wrapper for the C compiler that includes the OpenMPI libraries:
mpicc
hello_world.c
-o
hello_world
Where
-o
is the output flag, naming the executable
hello_world
. If omitted (i.e.,
mpicc
helloworld_c
would generate a compiled executable named
a.out
by default).
Finally, create a slurm job script to run the program:
#!/bin/bash
#SBATCH --job-name=hello_world
#SBATCH --output=result.txt
#SBATCH --ntasks=4
#SBATCH --time=10:00
#SBATCH --mem-per-cpu=2000
module
load
openmpi/4.0.5
mpirun
-n
4
./hello_world
Submit this script to slurm with the
sbatch
command:
sbatch
job_script.sh
You should see output in the
result.txt
file that shows
"Hello,
World!"
messages from each process.
MPI Communication: Send and Receive Operations
¶
MPI allows processes to communicate by sending and receiving messages. These messages can contain any type of data. Here is a simple example of using
MPI_Send
and
MPI_Recv
to send a number from one process to another:
#include
<mpi.h>
#include
<stdio.h>
int
main
(
int
argc
,
char
**
argv
)
{
MPI_Init
(
NULL
,
NULL
);
int
world_rank
;
MPI_Comm_rank
(
MPI_COMM_WORLD
,
&
world_rank
);
int
number
;
if
(
world_rank
==
0
)
{
number
=
-1
;
MPI_Send
(
&
number
,
1
,
MPI_INT
,
1
,
0
,
MPI_COMM_WORLD
);
}
else
if
(
world_rank
==
1
)
{
MPI_Recv
(
&
number
,
1
,
MPI_INT
,
0
,
0
,
MPI_COMM_WORLD
,
MPI_STATUS_IGNORE
);
printf
(
"Process 1 received number %d from process 0
\n
"
,
number
);
}
MPI_Finalize
();
}
MPI Monte Carlo
¶
A key aspect of using OpenMPI is the ability to implement parallel algorithms, which can significantly speed up computation. Here is an example of a parallel version of the Monte Carlo method for estimating the number
\(\pi\)
:
#include
<mpi.h>
#include
<stdio.h>
#include
<stdlib.h>
#include
<time.h>
int
main
(
int
argc
,
char
**
argv
)
{
MPI_Init
(
NULL
,
NULL
);
int
world_rank
;
MPI_Comm_rank
(
MPI_COMM_WORLD
,
&
world_rank
);
srand
(
time
(
NULL
)
*
world_rank
);
// Ensure random numbers on all processes
int
local_count
=
0
;
int
global_count
=
0
;
int
flip
=
1
<<
24
;
double
x
,
y
,
z
;
// Calculate hits within circle locally
for
(
int
i
=
0
;
i
<
flip
;
i
++
)
{
x
=
(
double
)
rand
()
/
(
double
)
RAND_MAX
;
y
=
(
double
)
rand
()
/
(
double
)
RAND_MAX
;
z
=
sqrt
((
x
*
x
)
+
(
y
*
y
));
if
(
z
<=
1.0
)
{
local_count
++
;
}
}
// Combine all local sums into the global sum
MPI_Reduce
(
&
local_count
,
&
global_count
,
1
,
MPI_INT
,
MPI_SUM
,
0
,
MPI_COMM_WORLD
);
// Process 0 calculates pi and prints the result
if
(
world_rank
==
0
)
{
double
pi
=
((
double
)
global_count
/
(
double
)(
flip
*
world_rank
))
*
4.0
;
printf
(
"The estimated value of pi is %f
\n
"
,
pi
);
}
MPI_Finalize
();
}
In this code, each process performs its own Monte Carlo simulation and then combines its results with those from other processes using the
MPI_Reduce
function.
Using OpenMPI with Python’s mpi4py
¶
mpi4py is a Python package that provides bindings to the MPI standard. It allows Python programs to take advantage of the distributed memory model and scale across multiple nodes of a high performance computing cluster, just like MPI.
The mpi4py package has been designed to be as close as possible to the MPI standard, providing Python developers with a familiar and straightforward interface to MPI.
In this program,
process
0
sends the number
-1
to
process
1
, which receives it and prints it.
To install mpi4py inside of a conda environment:
srun
-n
4
--pty
/bin/bash
module
load
anaconda3/2022.05
mkdir
-p
/path/to/mpi4py_env
conda
create
--prefix
=
/path/to/mpi4py_env
-y
source
activate
/path/to/mpi4py_env
conda
install
-c
conda-forge
mpi4py
In your preferred text editor, write a file
hello.py
that contains the following:
from
mpi4py
import
MPI
comm
=
MPI
.
COMM_WORLD
rank
=
comm
.
Get_rank
()
if
rank
==
0
:
print
(
'Hello from the master process'
)
else
:
print
(
f
'Hello from process
{
rank
}
'
)
This program gets the communicator for the current process, obtains the rank of the process, and then prints a message. If the rank is 0, the process is the master, otherwise, it is a worker.
You can run this program using the
mpirun
command:
srun
-n
4
--pty
/bin/bash
mpirun
-np
4
python
hello_world.py
This will run the program on 4 processes.
Just like in MPI, mpi4py allows you to perform point-to-point communication using the
send
and
recv
methods, and collective communication using methods like
bcast
(broadcast),
gather
, and
reduce
.
Here is an example of point-to-point communication:
from
mpi4py
import
MPI
comm
=
MPI
.
COMM_WORLD
rank
=
comm
.
Get_rank
()
if
rank
==
0
:
data
=
{
'a'
:
1
,
'b'
:
2
,
'c'
:
3
}
comm
.
send
(
data
,
dest
=
1
)
else
:
data
=
comm
.
recv
(
source
=
0
)
print
(
f
'Received data
{
data
}
at process
{
rank
}
'
)
In this program, the master process sends a dictionary to a specific process and that process receives the dictionary.
Note
Anything greater than rank 2 will make this program hang.
Writing Efficient MPI Code
¶
Efficiency and scalability are crucial when writing MPI code. Here are some tips to follow:
Overlap Computation and Communication:
Whenever possible, organize your code so that computation can occur while communication is ongoing. This will reduce the waiting time for communication to complete.
Minimize Communication:
Communication is often the bottleneck in parallel programs. Therefore, design your algorithms to minimize the amount of data that needs to be sent between processes.
Use Collective Operations:
MPI provides collective operations like
MPI_Bcast
and
MPI_Reduce
. These operations are often optimized for the underlying hardware and should be used whenever possible.
Use Non-Blocking Operations:
MPI also provides non-blocking versions of its send and receive functions. These functions (
MPI_Isend
and
MPI_Irecv
) return immediately, allowing the program to continue executing while communication is happening in the background.
Getting Help with MPI
¶
For assistance with getting started with using MPI or troubleshooting using MPI libraries on Discovery, reach out to us at
rchelp
@
northeastern
.
edu
or
schedule a consultation
with one of our team members.
Next
Using R
Previous
Using Module
Copyright © 2024, RC
Made with
Furo
On this page
Using MPI
Getting Started with MPI
MPI libraries on Discovery
Running a MPI Program
OpenMPI Tuning for Performance Optimization
Troubleshooting and Debugging MPI Programs
Benchmarking OpenMPI Performance
Tools for Benchmarking
Developing with MPI
Hello world program
MPI Communication: Send and Receive Operations
MPI Monte Carlo
Using OpenMPI with Python’s mpi4py
Writing Efficient MPI Code
Getting Help with MPI

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/index.html

Running Jobs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Running Jobs
¶
Running jobs efficiently is at the heart of a successful experience with our High-Performance Computing (HPC) cluster. This section outlines the essential aspects of executing various types of jobs, understanding the queuing system, and troubleshooting.
Understanding the Queuing System
Learn how our job scheduler manages the distribution of computational tasks.
Learn more »
Understanding the Queuing System
Job Scheduling Policies and Priorities
Gain insights into how jobs are prioritized and scheduled within the system.
Learn more »
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Run jobs interactively or submit batch scripts for later execution.
Learn more »
Interactive and Batch Mode
Running Jobs with job-assist
SLURM job submissions with job-assist command.
Learn more »
Running Jobs with job-assist
Running jobs smoothly is key to productive research and development. Explore these topics to understand and make the most of our robust computing infrastructure. If you need assistance, our support team is available at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy computing!
Next
Understanding the Queuing System
Previous
Connecting on Linux
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/gpujobsubmission.html

GPU Job Submission - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
GPU Job Submission
¶
Using CUDA
¶
There are several versions of CUDA Toolkits available on the HPC, including. Use the
module
avail
command to check for the latest software versions on the cluster.
$
module
avail
cuda
-------------------------------
/shared/centos7/modulefiles
-------------------------------
cuda/10.0
cuda/10.2
cuda/11.1
cuda/11.3
cuda/11.7
cuda/12.1
cuda/9.1
cuda/10.1
cuda/11.0
(
default
)
cuda/11.2
cuda/11.4
cuda/11.8
cuda/9.0
cuda/9.2
To see details on a specific CUDA toolkit version, use
module
show
(e.g.,
module
show
cuda/11.4
).
To add CUDA to your path, use
module
load
(e.g.,
module
load
cuda/11.4
adds CUDA 11.4).
Note
Executing
nvidia-smi
(i.e., NVIDIA System Management Interface) on a GPU node displays the CUDA driver information and monitor the GPU device.
GPUs for Deep Learning
¶
See also
Deep learning frameworks tend to cost storage that can quickly surpass
Home Directory Storage Quota
: follow best practices for
Conda environments
.
Select the tab with the desire deeplearning framework.
Important
Each tab assumes you are on a GPU node before with CUDA 11.8 and anaconda modules loaded as done above.
PyTorch
The following example demonstrates how to build PyTorch inside a conda virtual environment for CUDA version 12.1.
PyTorch’s installation steps for Python 3.10 and Cuda 12.1:
¶
srun
--partition
=
gpu
--nodes
=
1
--gres
=
gpu:v100-sxm2:1
--cpus-per-task
=
2
--mem
=
10GB
--time
=
02
:00:00
--pty
/bin/bash
module
load
anaconda3/2022.05
cuda/12.1
conda
create
--name
pytorch_env
-c
conda-forge
python
=
3
.10
-y
source
activate
pytorch_env
conda
install
jupyterlab
-y
pip3
install
torch
torchvision
torchaudio
Now, let us check the installation:
python
-c
'import torch; print(torch.cuda.is_available())'
If CUDA is detected by PyTorch, you should see the result,
True
.
If you want to use an older version of CUDA, here is the following example that demonstrates how to build PyTorch inside a conda virtual environment for CUDA version 11.8.
PyTorch’s installation steps for Python 3.10 and Cuda 11.8:
¶
srun
--partition
=
gpu
--nodes
=
1
--gres
=
gpu:v100-sxm2:1
--cpus-per-task
=
2
--mem
=
10GB
--time
=
02
:00:00
--pty
/bin/bash
module
load
anaconda3/2022.05
cuda/11.8
conda
create
--name
pytorch_env
-c
conda-forge
python
=
3
.10
-y
source
activate
pytorch_env
conda
install
jupyterlab
-y
pip
install
torch
==
2
.1.0
torchvision
==
0
.16.0
torchaudio
==
2
.1.0
--index-url
https://download.pytorch.org/whl/cu118
Now, let us check the installation:
python
-c
'import torch; print(torch.cuda.is_available())'
If CUDA is detected by PyTorch, you should see the result,
True
.
See also
PyTorch documentation
for the most up-to-date instructions and for different CUDA versions.
TensorFlow
Here are steps for installing CUDA 12.1 with the latest version of TensorFlow (TF).
See also
Compatibility of CUDA and TensorFlow versions
, and
detailed installation instructions
.
For the latest installation, use the TensorFlow pip package, which includes GPU support for CUDA-enabled devices:
Tensorflow’s installation steps for Python 3.9 and Cuda 12.1:
¶
srun
-p
gpu
--gres
=
gpu:v100-pcie:1
--pty
/bin/bash
module
load
anaconda3/2022.05
module
load
cuda/12.1
conda
create
--name
TF_env
python
=
3
.9
-y
source
activate
TF_env
pip
install
--upgrade
pip
pip
install
tensorflow
[
and-cuda
]
pip
install
jupyterlab
Verify the installation:
python
-c
"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
# True
Note
Ignore the
Warning
messages that get generated after executing the above commands.
Next
Access to Multi-GPU Partition
Previous
GPU Access
Copyright © 2024, RC
Made with
Furo
On this page
GPU Job Submission
Using CUDA
GPUs for Deep Learning

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/understandingqueuing.html

Understanding the Queuing System - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Understanding the Queuing System
¶
The queuing system in a high-performance computing (HPC) environment manages and schedules computing tasks. Our HPC cluster uses the Slurm Workload Manager as our queuing system. This section aims to help you understand how the queuing system works and how to interact effectively.
Introduction to Queuing Systems
¶
The Slurm scheduler manages jobs in the queue. When you submit a job, it gets placed in the queue. The scheduler then assigns resources to the job when they become available, according to the job’s priority and the available resources.
Job Submission and Scheduling
¶
Jobs are submitted to the queue via a script specifying the resources required (e.g., number of CPUs, memory, and GPUs) and the commands to be executed. Once submitted, the queuing system schedules the job based on the resources requested, the current system load, and scheduling policies.
Scheduling Policies**
¶
Our cluster uses a fair-share scheduling policy. This means that usage is tracked for each user or group, and the system attempts to balance resource allocation over time. If a user or group has been using many resources, their job priority may be temporarily reduced to allow others to use the system. Conversely, users or groups that have used fewer resources will have their jobs prioritized.
The following policies ensure fair use of the cluster resources:
Single job size
: The maximum number of nodes a single job depends on the partition (see
partition-names
).
Run time limit
: The maximum run time for a job depends on the partition (see
partition-names
).
Priority decay
: If a job remains in the queue without running for an extended period, its priority may slowly decrease.
Job Priority**
¶
Several factors determine job priority:
Fair-share
: This is based on the historical resource usage of your group. The more resources your group has used, the lower your job’s priority becomes, and vice versa.
Job size
: Smaller jobs (regarding requested nodes) typically have higher priority.
Queue wait time
: The longer a job has been in the queue, the higher its priority becomes.
Job States
¶
Each job in the queue has a state. The main job states are:
Pending (PD): The job is waiting for resources to become available.
Running (R): The job is currently running.
Completed (CG): The job has been completed successfully.
A complete list of job states can be found in the Slurm documentation.
Monitoring the Queue**
¶
You can use the following commands to interact with the queue:
squeue
: Displays the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. For example, to display your jobs:
squeue
-u
your_username
scontrol
: Used to view and modify Slurm configuration and state. For example, to show the details of a specific job:
scontrol
show
job
your_job_id
Tips for Efficient Queue Usage**
¶
Request only the resources you need: Overestimating your job’s requirements can result in longer queue times.
Break up large jobs: Large jobs tend to wait in the queue longer than small jobs. Break up large jobs into smaller ones.
Use idle resources: Sometimes, idle resources can be used. If your job is flexible regarding start time and duration, you can use the
--begin
and
--time
options to take advantage of these idle resources.
Next
Job Scheduling Policies and Priorities
Previous
Running Jobs
Copyright © 2024, RC
Made with
Furo
On this page
Understanding the Queuing System
Introduction to Queuing Systems
Job Submission and Scheduling
Scheduling Policies**
Job Priority**
Job States
Monitoring the Queue**
Tips for Efficient Queue Usage**

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Job-Dependency

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/workquota.html

Work Directory Storage Quota - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Work Directory Storage Quota
¶
Every space in work has a quota for TB and inodes (file count). It is necessary to stay below the quota to be able to use the space effectively.
We recommend the following practices to ensure effective use of your space in
/work
Check your usage
regularily to avoid hitting the quota limit in the middle of running jobs, which will disrupt their execution.
Compress directories that are not used frequently or for projects that have been completed.
When the output of a script has been generated and you wish to keep the intermediate files or intermediate outputs you can tar the directory. For large numbers of files or big files this can take time. We recommend running taring or compressing in an sbatch script. This can also be done as part of your sbatch script that generated the intermediate outputs.
#!/bin/bash
#SBATCH --job-name=tarit
#SBATCH --nodes=1
#SBATCH --ntasks=4
#SBATCH --time=24:00:00
#SBATCH -p short
# Commands to execute
tar
cvxf
project_1_output.tar.gz
/work/full/path/to/directory
Delete no-longer needed intermediate outputs. This can also be done as part of the sbatch script to streamline the process of generating output.
Request additional space in
/work
. Please review our
storage policies
for the relevant costs associated, if applicable.
Next
Scratch Directory Purge
Previous
Home Directory Storage Quota
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Queue

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/checkpointing.html

Checkpointing Jobs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Checkpointing Jobs
¶
The complexity of HPC systems can introduce unpredictable hardware or software component behavior, leading to job failures. Applying fault tolerance techniques to your HPC workflows can make your jobs more resilient to crashes, partition time limits, and hardware failures.
The Checkpointing technique
¶
Checkpointing is a fault tolerance technique based on the Backward Error Recovery (BER) technique, designed to overcome “fail-stop” failures (interruptions during the execution of a job).
To implement checkpointing:
Use data redundancy to create checkpoint files, saving all necessary calculation state data. Checkpoint files are generally created at constant intervals during the run.
If a failure occurs, start from an error-free state, check for consistency, and restore the algorithm to the previous error-free state.
Checkpointing allows you to:
Create resilient workflows in the event of faults.
Overcome most scheduler resource time limitations.
Implement an early error detection approach by inspecting intermediate results.
Checkpointing types
¶
Checkpointing can be implemented at different levels of your workflow.
Application-level
checkpointing is recommended for most Discovery users. You can use the checkpointing tool that is already available in your software application. For example, most software designed for HPC has a checkpointing option, and information on proper usage is often available in the software user manual.
User-level
checkpointing is good if you develop your code or know the application code well enough to integrate checkpointing techniques effectively. We recommend this approach for some Discovery users with advanced proficiency and familiarity with checkpointing mechanisms.
System-level
checkpointing is done on the system side, where the user saves the state of the entire process. This option is less efficient than User-level or Application-level checkpointing as it introduces a lot of redundancy.
Model-level
checkpointing is suitable for saving a model’s internal state (its weights, current learning rate, etc.) so that the framework can resume the training from this point whenever desired. This is often the intent of users doing machine learning on Discovery.
Which checkpoint type should you use?
¶
There are several checkpointing options, depending on your software’s needs.
If your software already includes
built-in checkpointing
, this is often the preferred option, as it is the most optimized and efficient way to checkpoint.
Application-level
checkpointing is the easiest to use, as it exists within your application. It does not require significant script changes and saves only the relevant data for your specific application.
User-level
checkpointing is recommended if you are writing your code. You can use DMTCP or implement checkpointing.
ML Model-level
checkpointing is specific to model training and deployment, as detailed in the
ML
Model-level
_ section.
Note
Some packages can be used to implement checkpointing if you are developing in Python, Matlab, or R. Some examples include
Python PyTorch checkpointing
,
TensorFlow checkpointing
,
MATLAB checkpointing
, and
R checkpointing
. Additionally, many Computational Chemistry and Molecular Dynamics software packages have built-in checkpointing options (e.g.,
GROMACS
and
LAMMPS
).
Implementing checkpointing can be achieved by the following:
Some save-and-load mechanisms of your calculation state.
The use of
Slurm Job Arrays
.
Note
To overcome partition time limits, replace your single long job with multiple shorter jobs. Then, use job arrays to set each job to run one after the other. If checkpointing is used, each job will write a checkpoint file. The following job will use the latest checkpoint file to continue from the latest state of the calculation.
Application-level checkpointing
¶
GROMACS checkpointing example
¶
The following example shows how to implement a 120-hour
GROMACS
job using multiple shorter jobs on the
short
partition. We use Slurm job arrays and the GROMACS built-in checkpointing option to implement checkpointing.
See also
https://manual.gromacs.org/documentation/current/user-guide/managing-simulations.html
The following script
submit_mdrun_array.sh
creates a Slurm job array of 10 individual array jobs:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --constraint=cascadelake
#SBATCH --nodes=1
#SBATCH --time=12:00:00
#SBATCH --job-name=myrun
#SBATCH --ntasks=56
#SBATCH --array=1-10%1  # execute 10 array jobs, 1 at a time
#SBATCH --output=myrun-%A_%a.out
#SBATCH --error=myrun-%A_%a.err
module
load
cuda/10.2
module
load
gcc/7.3.0
module
load
openmpi/4.0.5-skylake-gcc7.3
module
load
gromacs/2020.3-gpu-mpi
source
/shared/centos7/gromacs/2020.3-gcc7.3/bin/GMXRC.bash

srun
--mpi
=
pmi2
-n
$SLURM_NTASKS
gmx_mpi
mdrun
-ntomp
1
-s
myrun.tpr
-v
-dlb
yes
-cpi
state
The script above sets the checkpoint flag
-cpi
state
preceding the filename to dump checkpoints. This directs
mdrun
to the checkpoint in
state.cpt
when loading the state. The Slurm option
--array=1-10%1
creates 10 Slurm array tasks and runs one task job serially for 12 hours. The variable
%A
denotes the main job ID, while
%a
denotes the task ID (i.e., spanning
1-10
).
To submit this array job to the scheduler, use the following command:
sbatch
submit_mdrun_array.bash
DMTCP checkpoint example
¶
Distributed MultiThreaded checkpointing (
DMTCP
) is a tool checkpoint without changing code. It works with most Linux applications, such as Python, Matlab, R, GUI, and MPI.
The program runs in the background of your program, without significant performance loss and saves the process states into checkpoint files. DMTCP is available on the cluster
module
avail
dmtcp
module
show
dmtcp
module
load
dmtcp/2.6.0
Since DMTCP runs in the background, it requires some changes to your shell script. See
examples of checkpointing with DMTCP
, which use DMTCP with a simple C++ program (scripts modified from
RSE-Cambridge
).
Application-level checkpointing tips
¶
What data should you save?
Non-temporary application data
Any application data that has been modified since the last checkpoint
Delete no longer useful checkpoints; keep only the most recent checkpoint file.
How frequently do checkpoints occur?
Too often will slow your calculation; maybe I/O is heavy and memory intensive.
Too infrequently leads to large or long rollback times.
Consider how long it takes to reach a checkpoint and restart your calculation.
In most cases, every 10–15 minutes is okay.
ML Model-level Checkpointing
¶
Model-level checkpointing is a technique used to periodically save the state of a machine learning (ML) model during training, enabling the training process to be resumed from the saved checkpoint in case of interruptions or premature termination. The saved state typically includes the model’s parameters, optimizer state, and essential training information (e.g., the epoch number and loss value). Model checkpoints are especially critical for long-running training jobs.
Why is Checkpointing Important in Deep Learning?
¶
Checkpointing is crucial in deep learning because the training process can be time-consuming and require significant computational resources. Additionally, the training process may be interrupted due to hardware or software issues. Checkpoints solve this problem by saving the model’s current state to resume where it left off.
Moreover, checkpointing also saves the best-performing model, which can be loaded for evaluation. For instance, the model’s performance can vary based on the initialization and optimization algorithms, so checkpointing provides a way to select the best model based on a performance metric.
In summary, checkpointing is essential in deep learning as it provides a way to save progress, resume training, and select the best-performing model.
TensorFlow checkpoint example
¶
The following example demonstrates implementing a longer ML job using the
tf.keras
checkpointing API
and multiple shorter Slurm job arrays on the GPU partition.
The following example of the
submit_tf_array.bash
script:
#!/bin/bash
#SBATCH --job-name=myrun
#SBATCH --time=00:10:00
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --mem=10Gb
#SBATCH --output=%A-%a.out
#SBATCH --error=%A-%a.err
#SBATCH --array=1-10%1  #execute 10 array jobs, 1 at a time.
module
load
miniconda3/2020-09
source
activate
tf_gpu
# Define the number of steps based on the job id:
numOfSteps
=
$((
500
*
SLURM_ARRAY_TASK_ID
))
# Run python and save outputs to a log file corresponding to the current job task ID
python
train_with_checkpoints.py
$numOfSteps
&
>
log.
$SLURM_ARRAY_TASK_ID
The following checkpoint example is given in the program
train_with_checkpoints.py
:
checkpoint_path
=
"training_2/
{epoch:d}
.ckpt"
checkpoint_dir
=
os
.
path
.
dirname
(
checkpoint_path
)
cp_callback
=
tf
.
keras
.
callbacks
.
ModelCheckpoint
(
filepath
=
checkpoint_path
,
verbose
=
1
,
save_weights_only
=
True
,
period
=
5
)
See scripts
, which are modified from
TensorFlow Save and load models
.
The Slurm option
--array=1-10%1
will create 10 Slurm array tasks and run one task at a time. Note that the saved variable
%A
denotes the main job ID, while variable
%a
denotes the task ID (spanning values 1-10). Note that the output/error files are also unique to prevent different jobs from writing to the same files. The Shell variable
SLURM_ARRAY_TASK_ID
holds the unique task ID value and can be used within the Slurm Shell script to point to different files or variables.
To submit this job to the scheduler, use the command:
sbatch
submit_tf_array.bash
PyTorch checkpoint example
¶
See also
GPUs for Deep Learning
Setting up a Sample Project
¶
Next, we will set up a sample project to demonstrate checkpointing in PyTorch. We will create a simple neural network in PyTorch and train it on a sample dataset. The following code creates a simple neural network in PyTorch:
import
torch
import
torch.nn
as
nn
import
torch.nn.functional
as
F
class
Net
(
nn
.
Module
):
def
__init__
(
self
):
super
(
Net
,
self
)
.
__init__
()
self
.
fc1
=
nn
.
Linear
(
28
*
28
,
128
)
self
.
fc2
=
nn
.
Linear
(
128
,
10
)
def
forward
(
self
,
x
):
x
=
x
.
view
(
-
1
,
28
*
28
)
x
=
F
.
relu
(
self
.
fc1
(
x
))
x
=
self
.
fc2
(
x
)
return
x
Once the neural network is defined, we can load a sample dataset and train the model. In this example, we will use the MNIST dataset in the
torchvision
library. The following code loads the MNIST dataset and trains the model:
import
torchvision
import
torchvision.transforms
as
transforms
transform
=
transforms
.
Compose
([
transforms
.
ToTensor
(),
transforms
.
Normalize
((
0.5
,),
(
0.5
,))
])
trainset
=
torchvision
.
datasets
.
MNIST
(
root
=
'./data'
,
train
=
True
,
download
=
True
,
transform
=
transform
)
trainloader
=
torch
.
utils
.
data
.
DataLoader
(
trainset
,
batch_size
=
64
,
shuffle
=
True
)
net
=
Net
()
criterion
=
nn
.
CrossEntropyLoss
()
optimizer
=
torch
.
optim
.
SGD
(
net
.
parameters
(),
lr
=
0.001
,
momentum
=
0.9
)
for
epoch
in
range
(
2
):
running_loss
=
0.0
for
i
,
data
in
enumerate
(
trainloader
,
0
):
inputs
,
labels
=
data
optimizer
.
zero_grad
()
outputs
=
net
(
inputs
)
loss
=
criterion
(
outputs
,
labels
)
loss
.
backward
()
optimizer
.
step
()
running_loss
+=
loss
.
item
()
print
(
'Epoch
%d
loss:
%.3f
'
%
(
epoch
+
1
,
running_loss
/
len
(
trainloader
)))
With this simple project setup, we can demonstrate checkpointing in PyTorch.
What is the model state in PyTorch?
¶
In PyTorch, the model state refers to the values of all the parameters, weights, and biases that define the model’s behavior. This state is stored in the model’s
state_dict
, a dictionary-like object that maps each layer to its parameter tensors. Additionally, the
state_dict
contains information about the model’s architecture and the values of the parameters learned during training.
The
state_dict
can be saved to a file using PyTorch’s
torch.save
function and loaded back into memory using
torch.load
. This allows for checkpointing, which saves the state of a model periodically during training so that it can be recovered in the case of a crash or interruption.
PyTorch also provides the ability to save and load the entire model, including the model’s architecture and optimizer state, using the
torch.save
and
torch.load
functions. This allows for complete checkpointing of a model’s state.
It is important to note that the
state_dict
only contains information about the model’s parameters, not the optimizer or other training-related information. So, to save the optimizer state, you also need to save it separately and load it back when loading the model.
In conclusion, the model state in PyTorch represents the values of all the parameters, weights, and biases that define the model’s behavior. The state is stored in the
state_dict
and can be saved and loaded for checkpointing purposes. By saving the model state periodically during training, you can ensure that your progress is recovered during a crash or interruption.
Saving a PyTorch Model
¶
Saving a Model’s
state_dict
¶
To save the learnable parameters of a PyTorch model and the optimizer, we must save the model’s
state_dict
. We can use the
torch.save
function to pass the state_dict and file names as arguments. The following code demonstrates how to save the state_dict of the model:
checkpoint_path
=
'checkpoint.pth'
state
=
{
'epoch'
:
epoch
+
1
,
'state_dict'
:
net
.
state_dict
(),
'optimizer'
:
optimizer
.
state_dict
()}
torch
.
save
(
state
,
checkpoint_path
)
Saving the Entire Model
¶
In addition to saving the state_dict, we can also save the entire model, which includes the architecture, the learnable parameters, and the optimizer state. We can use the
torch.save()
function to save the entire model by passing the model and a filename as arguments. The following code demonstrates how to save the entire model:
model_path
=
'model.pth'
torch
.
save
(
net
,
model_path
)
Note
When saving the entire model, the custom classes used to define the model must be importable, either as a part of the project or in a separate file that can be imported.
We have seen how to save the state_dict and the entire model in PyTorch. The following section will show how to load a saved model.
Loading a PyTorch Model
¶
Loading a PyTorch
state_dict
¶
We can use the
torch.load
function to load the
state_dict
of a saved model by passing the file name as an argument. After loading the
state_dict
, we can set the
state_dict
of the model using the
model.load_state_dict
method. The following code demonstrates how to load the
state_dict
of a saved model:
codecheckpoint
=
torch
.
load
(
checkpoint_path
)
net
.
load_state_dict
(
checkpoint
[
'state_dict'
])
optimizer
.
load_state_dict
(
checkpoint
[
'optimizer'
])
epoch
=
checkpoint
[
'epoch'
]
Loading the Entire PyTorch Model
¶
To load the entire model, use the
torch.load
function and pass the file name as an argument. The loaded model can then be assigned to a variable, as shown in the following code:
loaded_model
=
torch
.
load
(
model_path
)
In PyTorch, we can load a saved model’s
state_dict
or the entire model. This section will cover how to resume training from a checkpoint.
Resuming Training from a Checkpoint
¶
To resume training from a checkpoint, we must load the model’s
state_dict
and the optimizer’s state (see
Loading a PyTorch Model
). After loading these, we can continue the training process from where it was left off. The following code demonstrates how to resume training from a checkpoint:
checkpoint
=
torch
.
load
(
checkpoint_path
)
net
.
load_state_dict
(
checkpoint
[
'state_dict'
])
optimizer
.
load_state_dict
(
checkpoint
[
'optimizer'
])
epoch
=
checkpoint
[
'epoch'
]
# Continue training
for
epoch
inrange
(
epoch
,
num_epochs
):
# Train the model
# Save the checkpoint
state
=
{
'epoch'
:
epoch
+
1
,
'state_dict'
:
net
.
state_dict
(),
'optimizer'
:
optimizer
.
state_dict
()
}
torch
.
save
(
state
,
checkpoint_path
)
This code loads the
state_dict
and the optimizer’s state from the checkpoint file and resumes training from the
epoch
value saved in the checkpoint file. After each epoch, the model’s
state_dict
and the optimizer’s state are saved in the checkpoint file, as described in Section III. This shows how to checkpoint PyTorch models, save and load the
state_dict
, save and load the entire model, and resume training from a checkpoint.
Model-level tips and tricks
¶
Save only the model’s State_dict
¶
Save only the model’s state_dict and optimizer state, allowing us to save only the information needed to resume training. Doing so reduces the checkpoint file’s size and makes it easier to load the model. So, please don’t forget to avoid unnecessary information in the checkpoint file, such as irrelevant metadata or tensors we can define during training.
Save regularly
¶
To prevent losing progress in case of a crash or interruption, save the checkpoint file regularly (i.e., after each epoch).
Save to multiple locations
¶
Save the checkpoint file to multiple locations, such as a local drive and the cloud, to ensure that the checkpoint is recovered in case of failure.
Use the latest versions of libraries
¶
Using the latest version of PyTorch and other relevant libraries is vital; changes in these libraries may cause compatibility issues with older checkpoints. With these best practices, you can ensure that your PyTorch models are saved efficiently and effectively and that your progress is not lost in the event of a crash or interruption.
Naming conventions
¶
Use a consistent naming convention for checkpoint files; include information such as the date, time, and epoch number in the file name to make tracking multiple checkpoint files easier and ensure you choose the proper checkpoint to load.
Checkpoint Validation
¶
Validate the checkpoint after loading it to ensure that the model’s state_dict and the optimizer’s state are correctly loaded by making a prediction using the loaded model and checking that the results are as expected.
Periodic clean-up
¶
Periodically, remove old checkpoint files to avoid filling up storage. This can be done by keeping only the latest
checkpoint or keeping only checkpoint files from the last few epochs.
Document checkpoints
¶
Document the purpose of each checkpoint and what it contains: the model architecture, the training data, the hyperparameters, and the performance metrics. This will help keep track of the progress and make it easier to compare different checkpoints. With these additional best practices, you can ensure that your checkpointing process is efficient, effective, and well-organized.
Next
Shell Environment on the Cluster
Previous
Scratch Directory Purge
Copyright © 2024, RC
Made with
Furo
On this page
Checkpointing Jobs
The Checkpointing technique
Checkpointing types
Which checkpoint type should you use?
Application-level checkpointing
GROMACS checkpointing example
DMTCP checkpoint example
Application-level checkpointing tips
ML Model-level Checkpointing
Why is Checkpointing Important in Deep Learning?
TensorFlow checkpoint example
PyTorch checkpoint example
Setting up a Sample Project
What is the model state in PyTorch?
Saving a PyTorch Model
Saving a Model’s
state_dict
Saving the Entire Model
Loading a PyTorch Model
Loading a PyTorch
state_dict
Loading the Entire PyTorch Model
Resuming Training from a Checkpoint
Model-level tips and tricks
Save only the model’s State_dict
Save regularly
Save to multiple locations
Use the latest versions of libraries
Naming conventions
Checkpoint Validation
Periodic clean-up
Document checkpoints

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/homequota.html

Home Directory Storage Quota - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Home Directory Storage Quota
¶
There are strict quotas for each
home directory
(i.e.,
/home/<username>
), and staying within the quota is vital for preventing issues on the HPC. This page provides some best practices for keeping within the
quota
. For more information about data storage on the HPC, see
Data Storage Options
.
Important
All commands on this page should be run from a
compute node
because they are CPU-intensive. You can find more information on getting a job on a compute node from
Interactive Jobs: srun Command
.
Utilize /work and /scratch
¶
Use
/work
for long-term storage. PIs can request a folder in
/work
via
New Storage Space Request
and additional storage via
Storage Space Extension Request
. Utilize
/scratch/<username>
for temporary or intermediate files. Then, move files from
/scratch
to
/work
for persistent storage (i.e., the recommended workflow).
Note
Please be mindful of the
/scratch
purge policy, which can be found on the
Research Computing Policy Page
. See
Data Storage Options
for information on
/work
and
/scratch
.
How To Check Your Quotas
¶
You can see exactly how much of your quota is being used in your /work, /scratch, or your /home directories by running the check-quota script from any node in the short partition.
First, launch a job on a node in the short partition.
srun
-p
short
--pty
bash
And then run check-quota with the desired path as follows:
# check your home quota
check-quota
/home/<username>
# check your scratch quota
check-quota
/scratch/<username>
# check a work directory quota
check-quota
/work/<directory>
The output will be of the following form (inode count refers to the number of files):
Directory
<>
has
the
following
quota:
Path:
<directory>
Used
Disk
Space:
0
.0
Tib
Disk
Space
Soft
Limit:
0
.07
Tib
Disk
Space
Hard
Limit:
0
.10
Tib
Used
Inodes:
0
Inodes
Soft
Limit:
2500000
Inodes
Hard
Limit:
5000000
Warning
You will only be able to see quotas of directories to which you have access; attempting to see quotas for directories that you don’t have access to will be logged.
Analyze Disk Usage
¶
To evaluate directory level usage you can use the command
du
. From a compute node, run the following command from your
/home/<username>
directory:
du
-shc
.
[
^.
]
*
~/*
This command will output the size of each file, directory, and hidden directory in your
/home/<username>
space, with the total of your
/home
directory being the last line of the output. After identifying the large files and directories, you can move them to the appropriate location (e.g.,
/work
for research) or back up and delete them if they are no longer required. An example output would look like:
[
<username>@<host>
directory
]
$
du
-shc
.
[
^.
]
*
~/*
39M
.git
106M
discovery-examples
41K
README.md
3
.3M
software-installation
147M
total
;;;{note}
The
du
command can take a few minutes to run in
/home/<username>
Cleaning Directories
Local
We advise against using ‘pip install’ to install packages outside of a conda environment or python virtual environment (for example, while in a JupyterLab Notebook or interactive python session). These installations are placed in your
.local
directory, adding to your
/home
quota. Additionaly, the presence of different packages in
.local
can have a negative impact on the function of applications on the OOD. Please ensure all the packages you need are installed in a conda or virtual python environment.
If there are no activly running processes the entire
.local
directory can be moved to
.local-off
or individual packages can be removed usually from within:
/home/username/.local/lib/pythonXX/site-packages
You can check for running processes via:
squeue
-u
<username>
To move your .local to .local-off
mv
/home/username/.local
/home/username/.local-off
Conda
¶
Note
Conda environments are part of your research and should be stored in your PI’s
/work
directory.
Here are some suggestions to reduce the storage size of the environments for those using the
/home/<username>/.conda
directory.
Remove unused packages and clear caches of Conda by loading an Anaconda module and running the following:
source
activate
<your
environment>
conda
clean
--all
This will only delete unused packages in your
~/.conda/pkgs
directory.
To remove any unused conda environments, run:
conda
env
list
conda
env
remove
--name
<your
environment>
Singularity
¶
If you have pulled any
containers
to the HPC using
Singularity
, you can clean your container cache in your
/home/<username>
directory by running the following command from a
compute node
:
module
load
singularity/3.5.3
singularity
cache
clean
all
To avoid your
~/.singularity
directory filling up, you can set a temporary directory for when you pull a
container
to store the cache in that location; an example of this procedure (where
<project>
is your PI’s
/work
directory) is the following:
mkdir
/work/<project>/singularity_tmp
export
SINGULARITY_TMPDIR
=
/work/<project>/singularity_tmp
Then, pull the container using Singularity as usual.
Cache
¶
The
~/.cache
directory can become large with the general use of HPC and
Open OnDemand
. Make sure you are not running any processes or jobs at the time by running the following:
squeue
-u
<username>
which prints a table with
JOBID
,
PARTITION
,
NAME
,
USER
ST
,
TIME
,
NODES
, and
NODELIST
(REASON)
, which is empty when no jobs are running (i.e., it is safe to remove
~/.cache
when no jobs are running).
Storing research environments
¶
Conda environments
¶
Use conda environments for Python on HPC. To create an environment in
/work
, use the
--prefix
flag as follows: (where
<project>
is your PI’s
/work
directory and
<my
conda
env>
is an empty directory to store your Conda environment):
conda
create
--prefix
=
/work/<project>/<my
conda
env>
Utilize the same conda environment to save storage space and time (i.e., avoid duplicate conda environments). Hence, shared environments can be easily done for a project accessing the same
/work
directory.
More information about creating custom Conda environments.
Singularity containers
¶
Containers pulled, built, and maintained for research work should be stored in your PI’s
/work
directory, not your
/home/<username>
directory.
Next
Work Directory Storage Quota
Previous
Best Practices
Copyright © 2024, RC
Made with
Furo
On this page
Home Directory Storage Quota
Utilize /work and /scratch
How To Check Your Quotas
Analyze Disk Usage
Conda
Singularity
Cache
Storing research environments
Conda environments
Singularity containers

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Parallel-Computing

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Central-Processing-Unit-CPU

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/index.html

Working with GPUs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Working with GPUs
¶
Harnessing the power of Graphics Processing Units (GPUs) can significantly accelerate your computations on our High-Performance Computing (HPC) cluster. This section provides insights into using GPUs effectively, from understanding their capabilities to best practices for optimization.
GPUs on our HPC
Overview of the GPUs on the HPC.
Learn more »
GPUs on the HPC
GPU Access
How to utilize GPUs interactively (srun) and passively (sbatch).
Learn more »
GPU Access
GPU Job Submission
Using CUDA and building deep learning environments.
Learn more »
GPU Job Submission
Access to the Multi-GPU Partition
Get access to the Multi-GPU partition.
Learn more »
Access to Multi-GPU Partition
Research Computing strives to provide an environment where innovation thrives. Explore these topics to understand and leverage our resources to their fullest. Our support team is available at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
if you need assistance.
Next
GPUs on the HPC
Previous
Running Jobs with job-assist
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Overcommitment

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmmonitoringandmanaging.html

Monitoring and Managing Jobs - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Monitoring and Managing Jobs
¶
You can use Slurm commands to check the status of the cluster, check details on the compute nodes, and check on the running jobs you have on the cluster.
Query Partitions:
sinfo
¶
The
sinfo
command will show information about all partitions in the cluster, including the partition name, available nodes, and status. To view information about a specific partition (e.g.,
short
,
gpu
,
long
):
sinfo
-p
<partition_name>
as an example:
sinfo
-p
gpu
PARTITION
AVAIL
TIMELIMIT
NODES
STATE
NODELIST
gpu
up
8
:00:00
5
drain*
c
[
2171
,2184,2188
]
,d
[
1008
,1014
]
gpu
up
8
:00:00
3
down*
c2162,d
[
1006
,1017
]
gpu
up
8
:00:00
1
drain
d1025
gpu
up
8
:00:00
2
resv
c2177,d1029
gpu
up
8
:00:00
50
mix
c
[
2160
,2163-2170,2172-2176,2178-2179,2185-2187,2189-2195,2204-2207
]
,d
[
1001
,1003-1005,1007,1009-1013,1016,1018,1020-1024,1026-1028
]
gpu
up
8
:00:00
3
alloc
d
[
1002
,1015,1019
]
gpu
up
8
:00:00
4
idle
c
[
2180
-2183
]
You can use the
--Format
flag to get more information or a specific format for the output:
sinfo
-p
<partition>
-t
idle
--Format
=
gres,nodes
For more information about sinfo, please review the
sinfo manual
.
Monitoring Jobs:
squeue
¶
The
squeue
command allows you to monitor the state of jobs in the queue. It provides information such as the job ID, the partition it is running on, and the job name.
To monitor all jobs of a specific user, use the following command:
squeue
-u
<username>
For more information about squeue, please review the
squeue manual
.
Canceling Jobs:
scancel
¶
The
scancel
command is used to cancel a running or pending job.
To cancel a specific job, use:
scancel
<job_id>
To cancel all jobs of a specific user:
scancel
-u
<username>
For more information about scancel, please review the
scancel manual
.
Next
Slurm Jobs Array
Previous
Slurm Commands
Copyright © 2024, RC
Made with
Furo
On this page
Monitoring and Managing Jobs
Query Partitions:
sinfo
Monitoring Jobs:
squeue
Canceling Jobs:
scancel

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Interactive-Job

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/fromsource/index.html

From Source - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
From Source
¶
System-Wide Software
¶
Make
CMake
Next
Make
Previous
Spack
Copyright © 2024, RC
Made with
Furo
On this page
From Source
System-Wide Software

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/datamanagement/globus.html

Using Globus - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Using Globus
¶
Globus is a data management system that you can use to transfer and share files. Northeastern has a subscription to Globus, and you can set up a Globus account with your Northeastern credentials. You can link your accounts if you have another account, either personal or through another institution.
To use Globus, you will need to set up an account, as detailed below. Then, as detailed below, you will need to install Globus Connect to create an endpoint on your local computer. After completing these two initial setup procedures, you can use the Globus web app to perform file transfers. See
Using the Northeastern endpoint
for a walkthrough of using the Northeastern endpoint on Globus.
Globus Account Set Up
¶
You can use the following instructions to set up an account with Globus using your Northeastern credentials.
Go to
Globus
.
Click
Log In
.
From the Use your existing organizational login, select
Northeastern University
, and then click
Continue
.
Enter your Northeastern username and password.
If you do not have a previous Globus account, click
Continue
. If you have a previous account, click the Link to an existing account.
Check the agreement checkbox, and then click
Continue
.
Click
Allow
to permit Globus to access your files.
You can then access the
Globus File Manager app
.
Tip
If you received an account identity that includes your NUID number (for example,
000123456
@
northeastern
.
edu
), you can follow the “Creating and linking a new account identity” instructions below to get a different account identity if you want a more user-friendly account identity. You can then link the two accounts together.
Creating and linking a new account identity (Optional)
¶
If you created an account through Northeastern University’s existing organizational login and received a username that included your NUID, you can create a new identity with a different username and link the two accounts together. A username you select instead of one with your NUID can make it easier to remember your login credentials.
Go to
Globus
.
Click
Log In
.
Click
Globus ID
to sign in.
Click
Need a Globus ID? Sign up
.
Enter your Globus ID information.
Enter the verification code that Globus sends to your email.
Click
Link to an existing account
to link this new account with your primary account.
Select Northeastern University from the drop-down box and click
Continue
to be taken to the Northeastern University single sign-on page.
Enter your Northeastern username and password.
You should now see your two accounts linked in the Account section on the
Globus web app
.
Install Globus Connect Personal (GCP)
¶
Use Globus Connect Personal (GCP) as an endpoint for your laptop. You first need to install GCP using the following procedure and be logged in to Globus before you can install GCP.
Go to
Globus File Manager
.
Enter a name for your endpoint in the Endpoint Display Name field.
Click
Generate Setup Key
to generate a setup key for your endpoint.
Click the
Copy
icon next to the generated setup key to copy the key to your clipboard. You will need this key during the installation of GCP in step 6.
Click the appropriate OS icon for your computer to download the installation file.
After downloading the installation file to your computer, double-click on the file to launch the installer.
Accept the defaults on the install wizard. After the installation, you can use your laptop as an endpoint within Globus.
Note
You cannot modify an endpoint after you have created it. If you need an endpoint with different options, you must delete and recreate it. Follow the instructions on the Globus website for
deleting and recreating an endpoint
.
Working with Globus
¶
After you have an account and set up a personal endpoint using Globus Connect personal, you can perform basic file management tasks using the Globus File Manager interface, such as transferring files, renaming files, and creating new folders. You can also download and use the Globus Command Line Interface (CLI) tool. Globus also has extensive documentation and training files for you to practice with.
Using the Northeastern endpoint
¶
To access the Northeastern endpoint on Globus, on the Globus web app, click
File Manager
, then in the
Collection
text box, type Northeastern. The endpoints owned by Northeastern University are displayed in the collection area. The general Northeastern endpoint is
Discovery
Cluster
. This collection is a Managed Mapped Collection (GCS) hosted on dtn-03.rc.northeastern.edu. Using the File Manager interface, you can easily change directories, switch the direction of transferring to and from, and specify options such as transferring only new or changed files. Below is a procedure for transferring files from Discovery to your personal computer, but with the flexibility of the File Manager interface, you can adjust the endpoints, file view, direction of the transfer, and many other options.
To transfer files from Discovery to your personal computer, do the following
Create an endpoint on your computer using the procedure above “Install Globus Connect,” if you have not done so already.
In the File Manager on the Globus web app, in the
Collections
textbox, type Discovery Cluster, then in the collection list, click the
Discovery
Cluster
endpoint.
click
Transfer or Sync to
in the right-pane menu.
Click in the
Search
text box, and then click the name of your endpoint on the
Your Collections
tab. You can now see the list of your files on Discovery on the left and on your personal computer on the right.
Select the file or files from the right-side list of Discovery files that you want to transfer to your personal computer.
Select the destination folder from the left-side list of the files on your computer.
(Optional) Click
Transfer & Sync Options
and select the transfer options you need.
Click
Start
.
Connecting to Google Drive
¶
The version of Globus currently on Discovery allows you to connect to Google Drive by first setting up the connection in GCP. This will add your Google Drive to your current personal endpoint.
Just so you know, you will first need a personal endpoint, as outlined in the procedure above. This procedure is slightly different from using the Google Drive Connector with
Globus version 5.5. You will need your Google Drive
downloaded to your local computer
.
To add Google Drive to your endpoint, do the following
Open the GCP app. Right-click the
G
icon in your taskbar on Windows and select
Options
. Click the
G
icon in the menu bar on Mac and select
Preferences
.
On the
Access
tab, click the + button to open the
Choose a directory
dialog box.
Navigate to your Google Drive on your computer and click
Choose
.
Click the
Shareable
checkbox to make this a shareable folder in Globus File Manager, and then click
Save
.
You can now go to Globus File Manager and see that your Google Drive is available as a folder on your endpoint.
Command Line Interface (CLI)
¶
The Globus Command Line Interface (CLI) tool allows you to access Globus from the command line. It is a stand-alone app that requires a separate download
and installation. Please refer to the
Globus CLI documentation
for working with this app.
Globus documentation and test files
¶
Globus provides detailed instructions on using Globus and has test files for you to practice with. These are free for you to access and use. We would like to encourage you to use the test files to become familiar with the Globus interface. You can access the Globus documentation and training files on the
Globus How To website
.
Next
Software Overview
Previous
Transfer Data
Copyright © 2024, RC
Made with
Furo
On this page
Using Globus
Globus Account Set Up
Creating and linking a new account identity (Optional)
Install Globus Connect Personal (GCP)
Working with Globus
Using the Northeastern endpoint
Connecting to Google Drive
Command Line Interface (CLI)
Globus documentation and test files

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/systemwide/index.html

System-wide Software - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
System-wide Software
¶
Modules
MPI
R
MATLAB
Getting Started Scientific Applications
¶
Research Computing has provides
scripts
to assist you in getting started with different scientific software packages on discovery. Although these scripts may not have all the solutions, they aim to provide users with a starting point to grasp the syntax and procedures used in various applications.
MATLAB
Multiprocessing
Schrodinger
The collection of scripts is continuously growing and the team welcomes any contributions to the
project
.
Next
Using Module
Previous
Software Overview
Copyright © 2024, RC
Made with
Furo
On this page
System-wide Software
Getting Started Scientific Applications

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/connectingtocluster/windows.html

Connecting on Windows - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Connecting on Windows
¶
Before you can connect to the HPC on a Windows computer, you’ll need to download a terminal program,
such as
MobaXterm
or PuTTY. We recommend MobaXterm, as you can also use it for file transfer,
whereas with other SSH programs, you would need a separate file transfer program.
To connect to cluster with MobaXterm
¶
Open MobaXterm.
Click
Session
, then click
SSH
as the connection type.
In
Remote Host
, type
login.discovery.neu.edu
, make sure
Port
is set to 22, and click
OK
.
(OPTIONAL: You can type your Northeastern username and password on MobaXterm, and it will save that information every time you sign in. If you opt to do this, you will be connected to Discovery after you click OK.)
At the prompt, type your Northeastern username and press Enter.
Type your Northeastern password and press Enter. Note that the cursor does not move as you type your password. This is expected behavior.
You are now connected to the cluster’s login node.
Passwordless SSH On Windows Using MobaXterm
¶
You must set up passwordless ssh to ensure that GUI-based applications launch without issues. Please make sure that your keys are added to the authorized.key file in your
~/.ssh
directory. This needs to be done anytime you regenerate your SSH keys. If you are having an issue opening an application that needs X11 forwarding, such as MATLAB or Schrodinger, and you recently regenerated your keys, make sure to add your keys to the authorized.key file.
Sign in to the cluster using MobaXterm.
Type
cd
~/.ssh
to move to your ssh folder.
Type
ssh-keygen
-t
rsa
to generate your key files.
Press
Enter
on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type
Y
.
Type
cat
id_rsa.pub
>>
authorized_keys
. This adds the contents of your public key file to a new line in
~/.ssh/authorized_keys
.
Note
Errors that you can see on Windows when launching a GUI-based program include the following:
Error:
unable
to
open
display
localhost:19.0
Launch
failed:
non-zero
return
code
If you are getting these types of errors, it could be because of the following reasons:
You still need to set up passwordless SSH. If so, you can follow the steps below to set up passwordless SSH.
When requesting a compute node from the login node, you may have forgotten to include the
--x11
option. Please see this example
srun
command.
X11 on Windows
¶
If you use MobaXterm on Windows, X11 forwarding is turned on by default.
Tip
You can test to see if x11 forwarding is working by typing
xeyes
. This will run a small program that makes a pair of eyes appear to follow your cursor.
Next
Connecting on Linux
Previous
Connecting on Mac
Copyright © 2024, RC
Made with
Furo
On this page
Connecting on Windows
To connect to cluster with MobaXterm
Passwordless SSH On Windows Using MobaXterm
X11 on Windows

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Compute-Node

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Package-Manager

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Graphics-Processing-Unit-GPU

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmarray.html

Slurm Jobs Array - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Slurm Jobs Array
¶
Job arrays are a convenient way to submit and manage large numbers of similar jobs quickly. Job arrays are particularly useful when running similar jobs, such as performing the same analysis with different inputs or parameters.
There are several ways to define job arrays, such as specifying the range of indices or providing a list of indices in a file. Slurm also offers various features to manage and track job arrays, such as options to simultaneously suspend, resume, or cancel all jobs in the array.
Note
50 is the maximum number of jobs allowed to be run at once per user-account.
Job Array Examples
¶
The most basic configuration for a job array is as follows:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=job-array-example
#SBATCH --output=out_array_%A_%a.out
#SBATCH --error=err_array_%A_%a.err
#SBATCH --array=1-6
This command runs the same script six times using Slurm job arrays. Each job array has two additional environment variable sets.
SLURM_ARRAY_JOB_ID
(
%A
) is set to the first job ID of the array, and
SLURM_ARRAY_TASK_ID
(
%a
) is set to the job array index value. Both the
SLURM_ARRAY_JOB_ID
(
%A
) and
SLURM_ARRAY_TASK_ID
(
%a
) are referenced when naming outputs in the example so they are not overwritten when a “task” (i.e., one of the executions of the script through the job array) finishes.
Tip
Generally, we want to pass the
SLURM_ARRAY_TASK_ID
as an argument for our script. If you are using R, you can retrieve the former using
task_id
<-
Sys.getenv("SLURM_ARRAY_TASK_ID")
. If you are using job arrays with Python, you can obtain the task ID using the following:
import
sys
taskId
=
sys
.
getenv
(
'SLURM_ARRAY_TASK_ID'
)
Resource Allotment in Job Arrays
¶
When submitting an array and setting its size with many dimensions, please use the
%
symbol to indicate how many jobs run simultaneously. When you specify the memory, number of nodes, number of CPUs, or other specifications, they will be applied to each job for the array. For example, the following code specifies an array of 600 jobs, with 20 jobs running at a time:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=job-array-example
#SBATCH --output=out-array_%A_%a.out
#SBATCH --error=err-array_%A_%a.err
#SBATCH --array=1-600%20
#SBATCH --mem=128G
#SBATCH --nodes=2
Slurm will submit 20 jobs to run simultaneously with each job, represented by a task ID, using the allocated resources for the submission of 2 nodes and 128 GB of RAM.
Job arrays are used for embarrassingly parallel jobs. If your job executed at each job ID does not use any multi-threading libraries, you can use the following header to avoid wasting resources:
#!/bin/bash
#SBATCH --partition=short
#SBATCH --job-name=jarray-example
#SBATCH --output=out/array_%A_%a.out
#SBATCH --error=err/array_%A_%a.err
#SBATCH --array=1-600%50  # 50 is the maximum number
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=2G ## debug prior to know how much RAM
Job Arrays from Commmand Line
¶
You can launch job arrays from the sbatch submission on the command line as follows:
sbatch
--array
=
<indexes>
[
options
]
script_file
Indexes can be listed as
1-5
,
=1,2,3,5,8,13
(i.e., each index listed), or
1-200%5
(i.e., produce a 200 task job array with only 5 tasks active at any given time).
Next
Classroom Resources
Previous
Monitoring and Managing Jobs
Copyright © 2024, RC
Made with
Furo
On this page
Slurm Jobs Array
Job Array Examples
Resource Allotment in Job Arrays
Job Arrays from Commmand Line

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/fromsource/cmake.html

CMake - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
CMake
¶
CMake Example: Serial LAMMPS Build
¶
Note
This is a minimal example using the command line version of CMake to build LAMMPS with no add-on packages enabled and no customization.
To allocate an interactive job on compute node type:
srun
-p
short
--pty
-N
1
-n
28
/bin/bash
For LAMMPS, you will need to create and use the build directory with the following command:
mkdir
build
cd
build/
Load the CMake module and use CMake to generate a build environment in a new directory.
module
load
cmake/3.23.2
cmake
../cmake
Next, you will work on the compilation and linking of all objects, libraries, and executables using the selected build tool.
cmake
--build
.
make
install
The
cmake
--build
command will launch the compilation, which, if successful, will ultimately generate an executable
lmp
inside the
build
folder. Now you can start running LAMMPS using
./lmp
.
CMake Example: Parallel LAMMPS Build
¶
See also
Makefile Example: Installing LAMMPS
The following instructions to build LAMMPS using cmake.
Running LAMMPS in parallel is possible using a domain decomposition parallelization. In order to build the MPI version, first allocate an interactive job on compute node by typing:
srun
-N
1
-n
28
--constraint
=
ib
--pty
/bin/bash
Load the required modules required for building LAMMPS:
module
load
openmpi/4.0.5
module
load
cmake/3.23.2
For LAMMPS, you will need to create and use the build directory with the following command:
mkdir
build
cd
build/
In the build directory, run the following commands with
DBUILD_MPI=yes
to build the MPI version :
cmake
../cmake
-DBUILD_MPI
=
yes
cmake
--build
.
make
install
The instructions above will create a
lmp
inside the
build
directory.
Note
When compiled with MPI, the binaries expect
mpirun
or
mpiexec
with the number of tasks to run.
Now, you can start running LAMMPS using
mpirun
-n
1
./lmp
-h
.
Next
Slurm
Previous
Make
Copyright © 2024, RC
Made with
Furo
On this page
CMake
CMake Example: Serial LAMMPS Build
CMake Example: Parallel LAMMPS Build

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-GPU-Acceleration

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/connectingtocluster/mac.html

Connecting on Mac - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Connecting on Mac
¶
Mac computers come with a Secure Shell (SSH) program called
Terminal
that you use to connect to the HPC using SSH. If you need to use software that uses a GUI, such as Matlab or Maestro, make sure to use the -Y option in the second step below using X11 forwarding.
Note
If you use Mac OS X version 10.8 or higher, and you have
XQuartz
running in the background to do X11 forwarding, you should execute the following command in Terminal once before connecting:
defaults
write
org.macosforge.xquartz.X11
enable_iglx
-bool
true
You should keep XQuartz running in the background. If you close and restart XQuartz, you will need to execute the above command again after restarting. Do not use the Terminal application from within XQuartz to sign in to Discovery. Use the default Terminal program that comes with your Mac (see Step 1 in the procedure below).
Connecting To Cluster On A Mac
¶
Go to Finder > Applications > Utilities, and then double click Terminal.
At the prompt, type
ssh
<username>@login.discovery.neu.edu
, where
<username>
is your Northeastern username. If you need to use X11 forwarding, type
ssh
-Y
<username>@login.discovery.neu.edu
.
Type your Northeastern password and press
Enter
.
You are now connected to Discovery at a login node.
Passwordless SSH On A Mac
¶
You must set up passwordless ssh to ensure that GUI-based applications launch without issues. Please make sure that your keys are added to the authorized.key file in your
~/.ssh
directory. This needs to be done anytime you regenerate your SSH keys. If you are having an issue opening an application that needs X11 forwarding, such as MATLAB or Schrodinger, and you recently regenerated your keys, make sure to add your keys to the authorized.key file.
Note
Ensure you’re on your local computer for steps 1 through 4—type
exit
to return to your local computer if connected to the cluster.
On a Mac, open the Terminal application and type
cd
~/.ssh
. This moves you to the ssh folder on your local computer.
Type
ssh-keygen
-t
rsa
to generate two files,
id_rsa
and
id_rsa.pub
.
Press
Enter
on all the prompts (do not generate a passphrase).
Type
ssh-copy-id
-i
~/.ssh/id_rsa.pub
<yourusername>@login.discovery.neu.edu
to copy
id_rsa.pub
to your
/home/.ssh
folder on Discovery. You can enter your NU password if prompted. This copies the token from
id_rsa.pub
file to the
authorized_keys
file, which will either be generated or appended if it already exists.
Connect to Discovery via
ssh
<yourusername>@login.discovery.neu.edu
. You should now be connected without having to enter your password.
Now on the cluster,
Type
cd
~/.ssh
to move to your ssh folder.
Type
ssh-keygen
-t
rsa
to generate your key files.
Press Enter on all the prompts (do not generate a passphrase). If prompted to overwrite a file, type
Y
.
Type
cat
id_rsa.pub
>>
authorized_keys
. This adds the contents of your public key file to a new line in the
~/.ssh/authorized_keys
file.
X11 on Mac OS
¶
From a Mac Terminal log in using the -Y option (
ssh
-Y
<yourusername>@login.discovery.neu.edu
).
Tip
If you used the -Y option to enable X11 forwarding on your Mac, you can test to see if it is working by typing
xeyes
. This will run a small program that makes a pair of eyes appear to follow your cursor.
Next
Connecting on Windows
Previous
Connecting To Cluster
Copyright © 2024, RC
Made with
Furo
On this page
Connecting on Mac
Connecting To Cluster On A Mac
Passwordless SSH On A Mac
X11 on Mac OS

================================================================================

URL: https://github.com/pradyunsg/furo

GitHub - pradyunsg/furo: A clean customizable documentation theme for Sphinx
Skip to content
Navigation Menu
Toggle navigation
Sign in
Product
GitHub Copilot
Write better code with AI
Security
Find and fix vulnerabilities
Actions
Automate any workflow
Codespaces
Instant dev environments
Issues
Plan and track work
Code Review
Manage code changes
Discussions
Collaborate outside of code
Code Search
Find more, search less
Explore
All features
Documentation
GitHub Skills
Blog
Solutions
By company size
Enterprises
Small and medium teams
Startups
By use case
DevSecOps
DevOps
CI/CD
View all use cases
By industry
Healthcare
Financial services
Manufacturing
Government
View all industries
View all solutions
Resources
Topics
AI
DevOps
Security
Software Development
View all
Explore
Learning Pathways
White papers, Ebooks, Webinars
Customer Stories
Partners
Open Source
GitHub Sponsors
Fund open source developers
The ReadME Project
GitHub community articles
Repositories
Topics
Trending
Collections
Enterprise
Enterprise platform
AI-powered developer platform
Available add-ons
Advanced Security
Enterprise-grade security features
GitHub Copilot
Enterprise-grade AI features
Premium Support
Enterprise-grade 24/7 support
Pricing
Search or jump to...
Search code, repositories, users, issues, pull requests...
Search
Clear
Search syntax tips
Provide feedback
We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
Cancel
Submit feedback
Saved searches
Use saved searches to filter your results more quickly
Name
Query
To see all available qualifiers, see our
documentation
.
Cancel
Create saved search
Sign in
Sign up
Reseting focus
You signed in with another tab or window.
Reload
to refresh your session.
You signed out in another tab or window.
Reload
to refresh your session.
You switched accounts on another tab or window.
Reload
to refresh your session.
Dismiss alert
pradyunsg
/
furo
Public
Notifications
You must be signed in to change notification settings
Fork
317
Star
2.8k
A clean customizable documentation theme for Sphinx
pradyunsg.me/furo/quickstart
License
MIT license
2.8k
stars
317
forks
Branches
Tags
Activity
Star
Notifications
You must be signed in to change notification settings
Code
Issues
24
Pull requests
20
Discussions
Actions
Security
Insights
Additional navigation options
Code
Issues
Pull requests
Discussions
Actions
Security
Insights
pradyunsg/furo
main
Branches
Tags
Go to file
Code
Folders and files
Name
Name
Last commit message
Last commit date
Latest commit
History
1,422 Commits
.github
.github
docs
docs
src/
furo
src/
furo
tests
tests
.git-blame-ignore-revs
.git-blame-ignore-revs
.gitattributes
.gitattributes
.gitignore
.gitignore
.isort.cfg
.isort.cfg
.pre-commit-config.yaml
.pre-commit-config.yaml
.readthedocs.yaml
.readthedocs.yaml
CODE_OF_CONDUCT.md
CODE_OF_CONDUCT.md
LICENSE
LICENSE
README.md
README.md
noxfile.py
noxfile.py
package-lock.json
package-lock.json
package.json
package.json
postcss.config.js
postcss.config.js
pyproject.toml
pyproject.toml
webpack.config.js
webpack.config.js
View all files
Repository files navigation
README
Code of conduct
MIT license
Furo
A clean customisable
Sphinx
documentation theme.
Elevator pitch
Intentionally minimal
--- the most important thing is the content, not the scaffolding around it.
Responsive
--- adapting perfectly to the available screen space, to work on all sorts of devices.
Customisable
--- change the color palette, font families, logo and more!
Easy to navigate
--- with carefully-designed sidebar navigation and inter-page links.
Good looking content
--- through clear typography and well-stylised elements.
Good looking search
--- helps readers find what they want quickly.
Biased for smaller docsets
--- intended for smaller documentation sets, where presenting the entire hierarchy in the sidebar is not overwhelming.
Quickstart
Furo is distributed on
PyPI
. To use the theme in your Sphinx project:
Install Furo in documentation's build environment.
pip install furo
Update the
html_theme
in
conf.py
.
html_theme
=
"furo"
Your Sphinx documentation's HTML pages will now be generated with this theme! 🎉
For more information, visit
Furo's documentation
.
Contributing
Furo is a volunteer maintained open source project, and we welcome contributions of all forms. Please take a look at our
Contributing Guide
for more information.
Acknowledgements
Furo is inspired by (and borrows elements from) some excellent technical documentation themes:
mkdocs-material
for MkDocs
Just the Docs
for Jekyll
GitBook
pdoc3
We use
BrowserStack
to test on real devices and browsers. Shoutout to them for supporting OSS projects!
What's with the name?
I plucked this from the scientific name for
Domesticated Ferrets
: Mustela putorius
furo
.
A ferret is actually a really good spirit animal for this project: cute, small, steals little things from various places, and hisses at you when you try to make it do things it doesn't like.
I plan on commissioning a logo for this project (or making one myself) consisting of a cute ferret. Please reach out if you're interested!
Used By
I'm being told that mentioning who uses
$thing
is a good way to promote
$thing
.
urllib3
-- THE first adopter of Furo
attrs
-- one of the early adopters!
pip
-- what I wrote this for
Python Developer’s Guide
black
License
This project is licensed under the MIT License.
About
A clean customizable documentation theme for Sphinx
pradyunsg.me/furo/quickstart
Topics
theme
documentation
minimal
sphinx
clean
customizable
sphinx-theme
sphinx-doc
three-column
furo
Resources
Readme
License
MIT license
Code of conduct
Code of conduct
Activity
Stars
2.8k
stars
Watchers
23
watching
Forks
317
forks
Report repository
Releases
27
2024.04.27
Latest
Apr 27, 2024
+ 26 releases
Used by
18.5k
+ 18,512
Contributors
49
+ 35 contributors
Languages
Sass
34.0%
HTML
23.5%
Python
19.1%
JavaScript
13.1%
SCSS
10.3%
Footer
© 2024 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information
You can’t perform that action at this time.

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/packagemanagers/index.html

Package Managers - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Package Managers
¶
Package managers allows you to build specific environments.
Conda
Spack
Next
Conda
Previous
Using MATLAB
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Cluster-Manager

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/index.html

Software Overview - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Software Overview
¶
Cluster Software
¶
System-wide Software
Cluster’s nuts and bolts.
Learn more »
System-wide Software
Package Managers
Allow you to build specific environments.
Learn more »
Package Managers
From Source
Using make or cmake.
Learn more »
From Source
The cluster offers you many options for working with software. Two of the easiest and most convenient ways are using the
module
command on the
command line
and the
interactive-ood-apps
web portal.
See also
More about using module.
If you need a specific software package, first check to see if it is already available through one of the preinstalled modules on the cluster. The Research Computing team adds new modules regularly, so use the
module
avail
command to view the most up-to-date list.
Requesting Software Installation Assistance
¶
If the software you need is not a module on the cluster, cannot be installed via Spack, and is not available through another way of self-installation (e.g.,
make
), please submit a
ServiceNow software request ticket
.
Note
Some packages might not be able to be installed on the cluster due to hardware incompatibility issues.
Following these steps, users should be able to install most software packages on their own. However, always refer to the specific software’s documentation, as steps can vary. If you run into issues or need help, please contact support.
Next
System-wide Software
Previous
Using Globus
Copyright © 2024, RC
Made with
Furo
On this page
Software Overview
Cluster Software
Requesting Software Installation Assistance

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/_sources/glossary.rst.txt

=================
Glossary
=================
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.

-------

.. glossary::

   Backfilling
      A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don't impact the completion of larger high-priority jobs.

   Cluster
      A group of computers connected in a way that allows them to function as a single system.

   Cluster Manager
      A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.

   Compute Node
      A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.

   Concurrency
      The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.

   Container
      A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.

   Core
      A processor within a CPU. Each core can execute its tasks.

   Central Processing Unit (CPU)
      The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.

   Fair Share Allocation
      A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.

   Graphics Processing Unit (GPU)
      A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.

   GPU Acceleration
      The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.

   Home Directory
      A user's directory in the cluster where personal files, application settings, and other user-specific data are stored.

   High-Performance Computing (HPC)
      The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It's often used for scientific research, big data analysis, and modeling complex systems.

   InfiniBand (IB)
      A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.

   Interactive Job
      An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.

   Job
      A set of computations a user submits to the HPC cluster for execution.

   Job Dependency
      The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.

   Job Priority
      Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster's scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.

      In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster's scheduler to make decisions on which job to execute next, considering factors such as:

      - **User-defined Priority**: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
      - **Resource Requirements**: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.

      - **Walltime Limit**: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.

      By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.

   Job Script
      A file that contains a series of commands that the HPC cluster will execute.

   Login Node
      A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it's meant for something other than resource-intensive computations.

   Module
      In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user's environment.

   Message Passing Interface (MPI)
      A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.

   Node
      A single machine within a cluster. A node can have multiple processors and its memory and storage.

   Node Allocation
      The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.

   Non-interactive job
      A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.

   Open OnDemand (OOD)
      A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.

   Overcommitment
      Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.

   Package Manager
       A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.

   Parallel Computing
      A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.

   Partition
      A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.

   Quota
      A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.

   Queue
      A waiting line for jobs ready to be executed but waiting for resources to become available.

   Resource Reservation
      The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.

   Scaling efficiency
      Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
.. math::

   S\eta = \frac{T_1}{N \times T_n}

Where:

- :math:`S\eta` is the scaling efficiency
- :math:`T_1` is the execution time on a single processor
- :math:`N` is the number of processors used
- :math:`T_n` is the execution time on N processors

For example, if :math:`T_1 = 100` seconds, :math:`N = 4` processors, and :math:`T_n = 25` seconds, the scaling efficiency would be:

.. math::

   S\eta = \frac{100}{4 \times 25} = 1

A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.



   Scheduling Policy
      A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.

   Scratch Space
      Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.

   Storage Cluster
      A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.

   Scheduler
      A program that manages the cluster's resources and allocates them to jobs based on priority, requested resources, and fair use policies.

   Singularity
      A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.

   Slurm
      An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.

   Task
      A unit of work within a job that can be executed independently. A job can consist of multiple tasks.

   VPN
      A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.

-------

This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#glossary

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-High-Performance-Computing-HPC

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/runningsjob.html

Running Jobs with job-assist - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Running Jobs with job-assist
¶
The
job-assist
command is designed to simplify the process of submitting jobs on the Discovery cluster using SLURM’s
srun
and
sbatch
modes. It provides an intuitive interface for requesting resources and managing job submissions on the cluster.
Features
¶
This command provides three modes of operation:
Mode
Description
Default
To quickly start a job with the default settings.
Interactive
To submit the job interactively with customized resource requests.
Batch
To generate and save
sbatch
scripts with specified parameters.
Getting Started with job-assist Command
¶
Load the job-assist Module
¶
module
load
job-assist
To launch an interactive session, use the following command:
¶
job-assist
This command will display a menu option with different modes of operation.
#Simple Example of the Menu Options
SLURM
Menu:
1
.
Default
mode
(
srun
--pty
/bin/bash
)
2
.
Interactive
Mode
3
.
Batch
Mode
4
.
Exit
Enter
your
option:
Default Mode
¶
This mode runs a basic SLURM job with the command:
srun
--pty
/bin/bash
By default, the resource allocated is short partition with 2 CPUs, 1 task, 1 node, and 2GB of memory for a duration of 4 hours.
Default Mode Example
[
rc.computing@login-00
~
]
$
job-assist
SLURM
Menu:
1
.
Default
mode
(
srun
--pty
/bin/bash
)
2
.
Interactive
Mode
3
.
Batch
Mode
4
.
Exit
Enter
your
option:
1
Running
default
job:
srun
--pty
/bin/bash
srun:
job
43910949
queued
and
waiting
for
resources
srun:
job
43910949
has
been
allocated
resources
[
rc.computing@d1019
~
]
$
Interactive Mode
¶
In this mode,
job-assist
guide you through interactively requesting resources. You will be prompted to provide the following information:
Partition Name
: Choose from available partitions like
debug
,
express
,
short
, or
gpu
.
Number of Nodes
: Specify the number of nodes required.
Number of Tasks
: Specify the number of tasks.
CPUs per Task
: Specify how many CPUs are needed per task.
Time
: Specify the maximum run time for the job in
HH:MM:SS
format.
Memory
: Specify the memory required per node in the correct format (e.g.,
4G
for 4 GB).
If you select the
gpu
partition, you will have the option to choose the type of GPU needed. This mode automatically submits the job to the scheduler and allocates the resources.
Interactive Mode Example
[
rc.computing@login-00
~
]
$
job-assist
SLURM
Menu:
1
.
Default
mode
(
srun
--pty
/bin/bash
)
2
.
Interactive
Mode
3
.
Batch
Mode
4
.
Exit
Enter
your
option:
2
Partitions:
debug
express
short
gpu
Enter
partition
name:
short
Maximum
number
of
nodes
available
for
short
is
2
Enter
number
of
nodes
(
1
-2
)
:
1
Enter
number
of
tasks:
2
Enter
number
of
cpus
per
task:
4
Maximum
time
for
short
partition
is
24
:00:00
Enter
time
(
HH:MM:SS
)
:
01
:00:00
Enter
memory
required
per
node
(
<size>
[
units
])
:
4G
Running
command:
srun
--partition
=
short
--nodes
=
1
--ntasks
=
2
--cpus-per-task
=
4
--time
=
01
:00:00
--mem
=
4G
--pty
/bin/bash
srun:
job
43911332
queued
and
waiting
for
resources
srun:
job
43911332
has
been
allocated
resources
[
rc.computing@d1019
~
]
$
Note
For Default and Interactive Modes, once the resources are allocated, the job will transition from the login node (
login-00
) to a compute node (e.g.,
d1019
).
Batch Mode
¶
This mode allows you to create and save an
sbatch
script with your specific requirements. By default, it will save the batch script in the
/home
directory. If prompted for a specific directory, it will save it in the location you specify.
Batch Mode Example
[
rc.computing@login-00
~
]
$
job-assist
SLURM
Menu:
1
.
Default
mode
(
srun
--pty
/bin/bash
)
2
.
Interactive
Mode
3
.
Batch
Mode
4
.
Exit
Enter
your
option:
3
Partitions:
debug
express
short
gpu
Enter
partition
name:
debug
Enter
number
of
nodes:
2
Enter
number
of
tasks:
4
Enter
number
of
cpus
per
task:
4
Maximum
time
for
debug
partition
is
00
:20:00
Enter
time
(
HH:MM:SS
)
:
00
:20:00
Enter
memory
required
per
node
(
<size>
[
units
])
:
32M
Do
you
want
to
specify
a
directory
to
save
the
batch
script
[
Defaults
to
home
]
?
(
y/n
)
:
n
Enter
the
filename
for
the
batch
script
without
any
extension:
batch-mode
Batch
script
saved
to
/home/<user-name>/batch-mode.sh
[
rc.computing@login-00
~
]
$
To view the content of the batch script:
nano
/home/<user-name>/batch-mode.sh
#Replace <user-name> with your actual username in the path
This creates a basic structure for the batch script to interpret with SLURM. You can edit it to add software, environment modules, and programs to execute.
Tip
Efficient Resource Usage
: Requesting only the resources you need (e.g., nodes, memory, CPUs) can help your job start sooner. Be mindful that requesting more CPUs or longer run times may result in longer queue times.
Next
Working with GPUs
Previous
Interactive and Batch Mode
Copyright © 2024, RC
Made with
Furo
On this page
Running Jobs with job-assist
Features
Getting Started with job-assist Command
Load the job-assist Module
To launch an interactive session, use the following command:
Default Mode
Interactive Mode
Batch Mode

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Cluster

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/classroom/class_guide.html

Course Guide - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Course Guide
¶
We support classroom education at Northeastern University by providing access to computing resources (CPU and GPU) and storage resources for instructors and their students.
We’ve supported courses from many disciplines, including biology, chemistry, civil engineering, machine learning, computer science, mathematics, and physics.
To gain access to HPC resources instructors need to submit a
classroom access form.
Please submit these requests prior to the beginning of each semester (preferred), or at least
one week
prior to the start of when you plan on using the HPC cluster for your class. If you’re requesting a customized application we require
two-weeks
to
one-month
time to complete prior to when you’d like to use it.
Classroom setup
¶
Once access is provided, each course will have a course-specific directory under
/courses/
following this sample file tree. As shown for the course BINF6430.202410 below:
/courses/
└──
BINF6430.202410/
├──
data/
├──
shared/
├──
staff/
└──
students/
The sub-directory
staff/
will be populated with a folder for each of the following: instructors, co-instructors, and TAs. The
students/
sub-directory contains a folder for each student. And the
data/
and
shared/
sub-directories can be populated by those in staff but is read-only for students. Students only have permission to read into their own directories under
students/
and cannot view into another students space.
All those in staff have read-write-execute permissions within the entirety of their courses directory, allowing them to store data, homework assignments, build conda environments, create new directories, etc, as they see fit.
Each course directory gets a default 1TB of storage space. This amount can be increased in the initial application form for classroom access, or requested anytime during an actively running course, by contacting
rchelp
@
northeastern
.
edu
Once the course has ended, and final grades have been submitted, the courses space including all data and shared class files will be archived, and all student personal directories will be deleted. Any students who had access to the HPC cluster only though the course will no longer have access when the course is completed.
Please see our page on
getting-access
if you would like an account that persists through taking courses.
Courses Partitions
¶
We have two partitions dedicated to the use of students and instructors for the duration of their course.
Name
Time Limit (default/max)
Running Jobs (max)
RAM Limit
courses
4 hrs / 24 hrs
50
256 GB
courses-gpu
4 hrs / 8 hrs
2
12 GB
The resources available in the courses/courses-gpu partitions can be queried with the command
sinfo
as run in the command line. We manage the resourses in courses/courses-gpu each term in response to the number of courses and requested usage per course.
sinfo
-p
courses-gpu
--Format
=
nodes,cpus,gres,statecompact
These partitions can be used in the following ways:
sbatch script
¶
An sbatch script can be submitted on the command line via the command
sbatch
scriptname.sh
. Below are some examples of sbatch scripts using the courses and courses-gpu partitions. See
slurm-running-jobs
for more information on running sbatch scripts or run
man
sbatch
for additional sbatch parameters.
courses partition
#!/bin/bash
#SBATCH –nodes=1
#SBATCH –time=4:00:00
#SBATCH –job-name=MyCPUJob
#SBATCH –partition=courses
#SBATCH –mail-type=ALL
#SBATCH –mail-users=username@northeastern.edu
commands to execute
courses-gpu partition
#!/bin/bash
#SBATCH –nodes=1
#SBATCH –time=4:00:00
#SBATCH –job-name=MyGPUJob
#SBATCH –partition=courses-gpu
#SBATCH –gres=gpu:1
#SBATCH –mail-type=ALL
#SBATCH –mail-users=username@northeastern.edu
commands to execute for gpu
srun interactive session
¶
An interactive session can be run on the command line via the
srun
command as shown in the examples below. See
slurm-running-jobs
for more information on using
srun
or run
man
srun
to see additinal parameters that can be set with
srun
.
courses partition
srun
--time
=
4
:00:00
--job-name
=
MyJob
--partition
=
courses
--pty
/bin/bash
courses-gpu partition
srun
--time
=
4
:00:00
--job-name
=
MyJob
--partition
=
courses-gpu
--gres
=
gpu:1
--pty
/bin/bash
Open OnDemand
¶
We have several widely-used applications available on the Open OnDemand (OOD) website including, Jupyterlab Notebook, Rstudio, Matlab, GaussView and more.
access-ood
All of the applications under the “Courses” tab on the dashboard can be set to either the
courses
or
courses-gpu
partitions via the applications specific pull down menus.
Monitoring Jobs
¶
Whichever way you choose to run your jobs, you can monitor their progress with the command
squeue
.
squeue
-u
username
You can also monitor jobs being run on either of the courses partitions.
squeue
-p
courses
squeue
-p
courses-gpu
Jobs can be canceled with the command
scancel
and the slurm job id that is assigned when your job is submitted to the scheduler.
scancel
jobid
Note
A cluster is a collection of shared resources. We highly recommend canceling any jobs that are still running in an interactive session (on the OOD or via srun) when you have completed your work. This frees up the resources for other classmates and instructors.
Software Applications
¶
All courses have access to the
command line
.
We have many software applications installed system wide as modules that are available through the command line via the
“module” command
.
We also support many software applications for courses and have interactive versions on the
Open OnDemand
website including:
Jupyterlab notebook, Rstudio, Matlab, VSCode, Maestro (Schrodinger), and a unix Desktop
Professors should create custom conda environments for their course which can be used in JupyterLab notebook or used in interactive mode (srun) or sbatch scripts on the command line.
Custom Course Applications
¶
At Northeastern University instructors have a great deal of flexibility in how they use the HPC for their classroom, and this is most apparent in the use of software applications.
We encourage professors to perform local software installations via conda environments within the
/courses
directory for their class. These can be used by the students to complete tutorials and homework assignments. Students can also create their own conda environments in their
/courses/course.code/students/username
directory to complete their own projects.
Conda environments
can be used to install a variety of research software and are not only useful for coding in python.
For most courses, the instructor is able to create a shared conda environment in their
/courses
directory that can provide all the necessary packages for the class.
In other cases, where specialized software is needed, please book a classroom consultation with one of the
RC team members
to discuss what is needed. Please allow at least
one month
for specialized app development and testing. In some cases we may be unable to provide the exact specifications requested. We will work with the instructor to find a suitable solution.
Next
Courses Cheatsheet
Previous
Classroom Resources
Copyright © 2024, RC
Made with
Furo
On this page
Course Guide
Classroom setup
Courses Partitions
sbatch script
srun interactive session
Open OnDemand
Monitoring Jobs
Software Applications
Custom Course Applications

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Open-OnDemand-OOD

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/scratchpurge.html

Scratch Directory Purge - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Scratch Directory Purge
¶
The space assigned to every user in
/scratch/<username>
is not meant for persistant storage and is purged every month by the Research Computing team. Files are not backed up in /scratch and as such important data or scripts need to be transfered quickly to /work or /home to be retained.
Important
The /scratch space is intended to provide temporary storage for jobs that produce a lot of output, not all of which will be retained.
There are several things you can do to prepare for a purge of /scratch.
Transfer any materials that you need to save to /work or /home.
Example using
srun
srun
--
pty
/
bin
/
bash
mv
/
scratch
/<
username
>/
file_to_keep
.
out
/
home
/<
username
>/
Example moving files via an
sbatch
script.
You will need to copy the script below to a text file (e.g., filemover.sh) and submit it to the scheduler with the command
sbatch
filemover.sh
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --time=05:00:00
#SBATCH --partition=short
#SBATCH --job-name=moveit
#SBATCH --ntasks=1
# if statement to check if tar.gz file exists, delete the output path
mv
/
scratch
/
file_to_keep
/
work
/<
groupname
>/
myimportantdata
On the day of the /scratch purge you will not be able to write job outputs to /scratch. Please edit your sbatch scripts to write outputs to /work or to /home.
If you have jobs that continually write output to scratch and run for long periods of time, please make sure you are
checkpointing
. This will allow the resumption of your jobs around the /scratch purge.
If you wish to retain entire directories that were generated in /scratch as part of a job output, you can tar the directory first and move the compressed file to your /home or /work. We recommmend only doing this if the directory will not be opened often as for large or many files taring and untaring can be time consuming.
# First get on a compute node
srun
--
pty
/
bin
/
bash
tar
-
czvf
name_of_output
.
tar
.
gz
/
scratch
/<
username
>/
directory
mv
/
scratch
/<
username
>/
name_of_output
.
tar
.
gz
/
work
/<
groupname
>/
files_to_keep
The code above can also be run in an
sbatch
job.
Note
Taring and compressing files can take time for large directories.
What happens during a purge of /scratch ?
¶
Files and directories are removed during a /scratch purge. This saves space for the proper function of the filesystem for all users.
How do I know /scratch is usable again following the /scratch purge?
¶
We will message via email when the scratch purge is finished. You can also check for updates on the Research Computing
website
. Additionaly, you can also check if the below commands work without error:
cd
/
scratch
/<
username
>
touch
test
-
scratch
My files are too big to transfer to /home
¶
Home has a limit of 75GBs. If your usage in /scratch is greater than than for the files that you want to keep, please apply for a space in
/work
or if you are a student request that your PI applies for /work. Do this well in advance of the /scratch purge to ensure your files are saved.
Next
Checkpointing Jobs
Previous
Work Directory Storage Quota
Copyright © 2024, RC
Made with
Furo
On this page
Scratch Directory Purge
What happens during a purge of /scratch ?
How do I know /scratch is usable again following the /scratch purge?
My files are too big to transfer to /home

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#furo-main-content

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/best-practices/index.html

Best Practices - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Best Practices
¶
Utilizing High-Performance Computing (HPC) resources effectively requires adherence to best practices. In this section, you will find essential guidance for managing your directory storage, checkpointing jobs, and cluster usage.
Home Directory Storage Quota
Learn about storage quotas and how to manage your home directory effectively.
Learn more »
Home Directory Storage Quota
Work Directory Storage Quota
Learn about storage quotas and how to manage your work directory effectively.
Learn more »
Work Directory Storage Quota
Scratch Directory Purge
Learn how to use /scratch effectively, before and after a purge.
Learn more »
Scratch Directory Purge
Checkpointing Jobs
Discover how to save progress within long-running jobs, preventing loss of data.
Learn more »
Checkpointing Jobs
Shell Environment on the Cluster
Best practices for using the shell environment on Discovery.
Learn more »
Shell Environment on the Cluster
Cluster Usage
Guidelines about where tasks should be performed and the bots that monitor them.
Learn more »
Cluster Usage
Applying these best practices can greatly enhance your HPC experience. They ensure efficient use of resources, contribute to robust job execution, and facilitate optimal performance. If you need additional assistance, our support team is reachable at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Happy computing!
Next
Home Directory Storage Quota
Previous
Singularity on Discovery
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/containers/index.html

Containers on HPC - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Containers on HPC
¶
A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. (
Reference
)
Containers allow reproducibility and portability in scientific workflows.
They ensure that scientific experiments can be reproduced on any system, such as our laptop, our friend’s laptop, HPC as well as public cloud-based environments. Containers achieve this by encapsulating the entire software environment of the specific analysis or simulation. This eliminates the “it works on my machine” problem by providing a consistent runtime environment across various platforms.
This consistency also allows for more collaboration, allowing researchers to share their work without encountering compatibility issues, thereby speeding up scientific workflows.
Containers also simplify software dependency issues by isolating software and its dependencies from the underlying HPC environment. This reduces project conflicts and makes managing different software versions much more accessible.
We recommend using containers on the HPC because they provide isolation from the HPC environment and complete control of the software environment to the researcher. Many technologies exist that could be used for containerization, including Docker, Podman, and Singularity/Apptainer. On Discovery, we use Singularity as the containerization engine, however, Singularity can work with containers built with Docker or Podman.
Singularity on Discovery
See how to use containers on HPC
Singularity on Discovery
Next
Singularity on Discovery
Previous
CPS Class Instructions
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Node-Allocation

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Quota

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/connectingtocluster/index.html

Connecting To Cluster - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Connecting To Cluster
¶
The following sections on this page will guide you on how to connect to the cluster using command-line access using the terminal and the web interface of Open OnDemand. Guides are provided if your personal computer is running Windows, macOS, or Linux.
Using the Terminal
¶
You connect to the HPC using a
secure shell
program to initiate an SSH session to
sign in to the HPC.
Connecting with Mac
Using the Terminal to connect to Discovery.
Learn more »
Connecting on Mac
Connecting with Windows
Using MobaXTerm to connect to Discvoery.
Learn more »
Connecting on Windows
Connecting with Linux
Using the Terminal to connect to Discovery.
Learn more »
Connecting on Linux
Using Open OnDemand
¶
To learn more about connecting to the cluster using Open OnDemand, please see the Research Computing website on utilizing
Open OnDemand
.
Next
Connecting on Mac
Previous
Home
Copyright © 2024, RC
Made with
Furo
On this page
Connecting To Cluster
Using the Terminal
Using Open OnDemand

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Message-Passing-Interface-MPI

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Non-interactive-job

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/datamanagement/discovery_storage.html

Data Storage Options - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Data Storage Options
¶
RC is responsible for procuring and maintaining several data storage options, including active and archive storage solutions. If you are affiliated with Northeastern, you can request one or more storage solutions to meet your needs. If you anticipate needing storage as part of a grant requirement, please
schedule a storage consultation
with an RC staff member to understand what storage options best meet your research needs.
Active Storage
¶
Two main storage systems are connected to Northeastern’s HPC cluster:
/home
and
/scratch
; these options have specific quotas and limitations. The list below details the storage options available on the HPC cluster if you have an account. Every individual with an account has a
/home
and
/scratch
. While research groups can request additional storage on the
/work
storage system,
/work
storage is not currently provisioned for individuals.
Important
The
/scratch
space is only for temporary storage; this storage is not backed up. Please review the
/scratch
policy on our
Policy page
.
$HOME:
/home/<username>
where
username
is your NU login, e.g.,
/home/j.smith
Description:
All users are automatically given a
/home
when their account is created. This storage is mainly intended for storing relatively small files such as script files, source code, and software installation files. While
/home
is permanent storage backed up and replicated,
/home
is not permanent storage.
/home
also has a small quota, so you should frequently check your space usage (use a command such as
du
-h
/home/<yourusername>
where
<yourusername>
is your username to see the total space usage).
Quota:
75GB
Scratch:
/scratch/<username>
Description:
All users are automatically given a
/scratch
when their account is created.
Scratch space
is a shared space for all users and is meant for temporary storage.
It is not backed up.
Data on
/scratch
should be moved to another location for permanent storage as soon as possible.
Quota:
N/A
Work:
/work/<groupname>
Description:
Research groups can request additional storage on
/work
. A PI can request this extra storage through the
New Storage Space Request
. This is permanent, persistent, and long-term storage for storing data actively used for research. It can be accessed by all members of the research group who have the necessary access permissions, facilitating collaboration and seamless sharing of data within the group.
Quota:
Each PI can request up to
35TB
of complimentary storage summed across all
/work
they own.
Access Request:
Students with research groups can request access to the PI’s storage on
/work
. To expedite the request process, we recommend that you inform the group owner that they will be receiving an email requesting their permission to grant you access to
/work
before you submit the request.
To request access to
/work
, students can either create a
ServiceNow ticket with RC
or email
rchelp
@
northeastern
.
edu
to automatically generate a ticket in ServiceNow. Please include both the storage space name and the PI’s name.
Once you have been added to the Unix group for the space on
/work
, please close all open connections to the HPC and log in again for the changes to reflect on your end. As you know, UNIX groups are assigned at login time, and this step ensures that your access privileges are updated accordingly. To confirm you have been added to the group, you can run the command
groups
.
Default Permission:
By default, users are given read and write access when added to
/work
. However, specific permissions might be granted at the PI’s request.
Attention
The
/research
storage tier is no longer provided. Please contact Research Computing if you are a former user of
/research
and have questions or issues related to
/research
by
submitting a ticket
. Other storage options include
/work
,
Sharepoint
, and
OneDrive
.
Archival Storage
¶
Important
If you are not connected to the campus internet, you must be connected to the university’s
VPN
(i.e., GlobalProtect) before accessing the
/nese
system. You can find detailed information about downloading and using the GlobalProtect VPN in the
FAQ: VPN and remote access
.
NAME:
/nese
Description:
This is archival, non-permanent storage intended for researchers needing a long-term storage option for their data.
Quota:
A PI can request this storage through the
New Storage Space Request
.
Next
Transfer Data
Previous
Data Management
Copyright © 2024, RC
Made with
Furo
On this page
Data Storage Options
Active Storage
Archival Storage

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/software/systemwide/r.html

Using R - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Using R
¶
R
is available as a
modules
and it is also an interactive app on
Open OnDemand
(OOD). You can also use R with Anaconda.
See also
Our
Conda
page and
official Anaconda documentation
.
Using R on Open OnDemand
¶
The Open OnDemand application offers several different versions of R accessed through an interactive session with RStudio server in a
Rocker Container
. Each version of R is available as a different
flavor
whereby different packages are pre-installed. We host three flavors whose package-lists build on one another in the following order: RStudio < Tidyverse < Geospatial.
In addition to different R packages you will find additional package requirements (e.g, compilers) also present in the three flavors in increasing order.
Important
If you have tried to install a package in the
RStudio
or
Tidyverse
flavors and recieve an error message saying a necessary compiler is missing (e.g., glibc, CMAKE, zlib) or other “compilation failed” message. Please try to install the package again in the
Geospatial
flavor. If this still returns an error reach out to rchelp@northeastern.edu
Creating a Packrat Environment
¶
If you work with R packages, using a
Packrat environment
can be a helpful way to access packages across different sessions in the Open OnDemand app, between the Open OnDemand app and the command line, or between the different R flavors. Use the procedure below to create a Packrat environment on Discovery.
After you create a new directory for your R project, you can then use Packrat to store your package dependencies inside it.
We recommend making your Packrat directory in /work (preferred) or /home
Connect to Discovery via ssh.
Type
module
load
R/4.2.1
.
Create a new directory for your R project by typing,
mkdir
/work/<groupname>/<username>/<directoryname>
where
<groupname>
is your group name,
<username>
is your username, and
<directoryname>
is the name of the directory you want to create for your R project. For example,
/work/coolsciencegroup/j.smith/packrat_r
.
Create a new directory for your R project by typing,
mkdir
<directoryname>
Open the R interface and install Packrat:
install
.
packages
(
"packrat"
)
# install in a local directory, as you cannot install as root
Initialize the environment where R packages will be written to:
packrat
::
init
(
"/work/<groupname>/<yourusername>/<directoryname>"
)
You can then install R packages that you need. For example, to install a package called
rtracklayer
, type the following:
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("rtracklayer")
When using RStudio in the OOD App:
The instructions below can be applied on any RStudio “flavor” available (i.e., RStudio, Geospatial, and Tidyverse). Once a Packrat snapshot is created it can easily be transferred between flavors and even machines (e.g., personal laptop, Discovery).
Launch an RStudio instance on the OOD. Specify the flavor and other parameters as usual.
In the RStudio console type:
install.packages("packrat")
.
Note
This will install by default in
$HOME/R/x86_64-pc-linux-gnu-library/<version>/
as long as you don’t have previous environments or those have been turned off (see below). For Packrat installation, it is best to specify a “project folder” in your
$HOME
,
/scratch
or
/work
directory (if you do not have
/work
please see
Data Storage Options
). The location
/tmp/Rtmp8CbQCA/downloaded_packages
would not work because
/tmp
corresponds to the compute node that you were on while running the R session. Optimally, you would like to have the Packrat location in a persistent place so that all packages and libraries are available to you at all times regardless of the compute node you are on.
Create a Packrat project directory at the desired location by selecting “New Folder” in the “Files” tab in the lower right hand side of the RStudio screen. Alternatively, use
mkdir
in the terminal tab on the lower left-hand side of the RStudio screen. For example:
mkdir
projectfolder
.
In the RStudio console, initialize the Packrat. If your current directory is the project folder (i.e.,
getwd()
== Packrat project folder) you can omit the path here.
packrat
::
init
(
"<path-to-project-folder>"
)
You can now record all the currently installed packages to your Packrat with the snapshot command. This may take some time if you have installed a lot of packages.
packrat
::
snapshot
()
And now you can check on the status of your Packrat with:
packrat
::
status
()
Now turn Packrat on. Packrat will now stay on for all your RStudio sessions and across the RStudio flavors (RStudio, geospatial, and tidyverse).
packrat
::
on
()
You can now install packages as normal. You should see the installation location for your Packrat project folder. For example: “Installing package into
/work/groupname/username/packrat_R/
”
install
.
packages
(
"viridis"
)
Packrat Tips
¶
At any time you can check the status of your Packrat with
packrat::status()
.
Packrat can be toggled on and off with
packrat::on()
and
packrat::off()
, respectively.
To disconnect Packrat and allow for package installation outside your packrat project folder:
packrat::disable(project
=
NULL,
restart
=
TRUE)
, where
project
refers to the current Packrat project folder, and
restart
=
TRUE
will restart the R session.
To re-initialize Packrat run:
packrat::init("<path-to-packrat-project-folder>")
. This will automatically restart your R session.
A package can be removed from Packrat via:
remove.packages("viridis)
, but will remain in your Packrat snapshot and can be restored with:
Packrat::restore()
.
The function
packrat::clean(dry.run=T)
will list any unused packages that were installed in your snapshot. You can remove them with:
packrat::clean()
.
Note
For most cases, having a single Packrat directory is sufficient, unless you notice specific package conflicts or need different versions of the same package. A single Packrat directory also saves from having to install the same dependencies multiple times in different locations.
If the installation location is not setting to your project folder you may need to turn off these environments. In some cases, these folders could also be present in your
/work/groupname/<project-name>
directory.
mv
~/.rstudio
~/.rstudio-off
mv
~/.local
~/.local-off
mv
~/ondemand
~/ondemand.off
mv
~/.Rprofile
~/.Rprofile.off
mv
~/.Rhistory
~/.Rhistory.off
Next
Using MATLAB
Previous
Using MPI
Copyright © 2024, RC
Made with
Furo
On this page
Using R
Using R on Open OnDemand
Creating a Packrat Environment
Packrat Tips

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/gpus/accessinggpus.html

GPU Access - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
GPU Access
¶
Running Jobs
¶
Use
srun
for interactive and
sbatch
for batch mode. The
srun
example below is requesting 1 node and 1 GPU with 4GB of memory in the
gpu
partition. You must use the
--gres=
option to request a gpu:
srun
--partition
=
gpu
--nodes
=
1
--pty
--gres
=
gpu:v100-pcie:1
--ntasks
=
1
--mem
=
4GB
--time
=
01
:00:00
/bin/bash
Note
On the
gpu
partition, requesting more than 1 GPU (
--gres=gpu:1
) will cause your request to fail. Additionally, one cannot request all the CPUs on that gpu node as they are reserved for other GPUs.
The
sbatch
example below is similar to the
srun
example above, but it submits the job in the background, gives it a name, and directs the output to a file:
#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --gres=gpu:v100-pcie:1
#SBATCH --time=01:00:00
#SBATCH --job-name=gpu_run
#SBATCH --mem=4GB
#SBATCH --ntasks=1
#SBATCH --output=myjob.%j.out
#SBATCH --error=myjob.%j.err
## <your code>
Specifying a GPU type
¶
You can add a specific type of GPU to the
--gres=
option (with either
srun
or
sbatch
). For a list of available GPU types, refer to the GPU Types column in
gpu-table
, that are listed as
Public
.
Command to request one v100 GPU.
¶
--gres
=
gpu:v100-pcie:1
Note
Requesting a specific type of GPU could result in longer wait times, based on GPU availability at that time.
Next
GPU Job Submission
Previous
GPUs on the HPC
Copyright © 2024, RC
Made with
Furo
On this page
GPU Access
Running Jobs
Specifying a GPU type

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/slurmguide/slurmcommands.html

Slurm Commands - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Slurm Commands
¶
Download
Cheatsheet
[
Source
]
¶
Next
Monitoring and Managing Jobs
Previous
Slurm
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/glossary.html#term-Job

Glossary - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Glossary
¶
This glossary provides definitions for terms and abbreviations you may encounter when using our HPC cluster.
Backfilling
¶
A scheduling technique that allows smaller jobs to be scheduled ahead of larger jobs, as long as they don’t impact the completion of larger high-priority jobs.
Cluster
¶
A group of computers connected in a way that allows them to function as a single system.
Cluster Manager
¶
A software system responsible for monitoring and managing the health, status, and communication among nodes in a cluster.
Compute Node
¶
A component within a cluster that performs the actual computational tasks. It typically consists of multiple processors, memory, and storage resources, where the primary computation and data processing occur.
Concurrency
¶
The ability of the system to handle multiple tasks or jobs simultaneously, without waiting for each task to complete before starting another.
Container
¶
A lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Containers provide consistency and portability across different computing environments.
Core
¶
A processor within a CPU. Each core can execute its tasks.
Central Processing Unit (CPU)
¶
The primary component of a computer that performs most processing inside the computer. CPUs can have multiple cores.
Fair Share Allocation
¶
A scheduling policy that ensures all users receive a fair share of cluster resources over time, regardless of job size or priority.
Graphics Processing Unit (GPU)
¶
A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.
GPU Acceleration
¶
The use of GPUs to offload computation-intensive tasks from the CPU, leading to faster processing of tasks like simulations and data analysis.
Home Directory
¶
A user’s directory in the cluster where personal files, application settings, and other user-specific data are stored.
High-Performance Computing (HPC)
¶
The use of parallel processing for running advanced application programs efficiently, reliably, and quickly. It’s often used for scientific research, big data analysis, and modeling complex systems.
InfiniBand (IB)
¶
A high-speed networking technology commonly used in HPC clusters. It provides high bandwidth and low latency communication between nodes in the cluster, facilitating fast data transfer for parallel processing.
Interactive Job
¶
An interactive job lets you interact directly with a compute node in real-time. This is helpful for testing, debugging, or running applications that require user input. When you submit an interactive job, you gain access to a shell on the compute node, where you can execute commands and run scripts just as you would on your local machine. This is particularly helpful for iterative testing and development tasks that benefit from immediate feedback and adjustments.
Job
¶
A set of computations a user submits to the HPC cluster for execution.
Job Dependency
¶
The condition where one job relies on the successful completion of another job before it can start, ensuring proper sequencing of tasks.
Job Priority
¶
Refers to the relative importance or urgency assigned to a specific computational task or job within a High-Performance Computing (HPC) cluster’s scheduling system. Job priority determines the order in which jobs are executed and the allocation of computing resources.
In an HPC environment, different jobs may have varying degrees of importance or resource requirements. Job priority allows the cluster’s scheduler to make decisions on which job to execute next, considering factors such as:
User-defined Priority
: Users can often assign priority values to their jobs, indicating the relative importance of their tasks. Higher priority values typically result in faster job execution.
Resource Requirements
: Jobs with greater resource demands, such as more CPUs, memory, or GPUs, may receive higher priority, ensuring they receive the necessary resources for efficient execution.
Walltime Limit
: Jobs with shorter estimated execution times may be assigned higher priority, as they are more likely to finish quickly and free up resources for other pending jobs.
By adjusting job priorities, users and administrators can optimize resource allocation, meet project deadlines, and promptly process critical tasks within the HPC cluster. Job priority management is an essential aspect of efficient cluster operation.
Job Script
¶
A file that contains a series of commands that the HPC cluster will execute.
Login Node
¶
A gateway or access point to an HPC cluster. Users connect to the login node to submit jobs, manage files, and interact with the cluster. However, it’s meant for something other than resource-intensive computations.
Module
¶
In the context of HPC, a module is a bundle of software that can be loaded or unloaded in the user’s environment.
Message Passing Interface (MPI)
¶
A standardized and portable message-passing system used to enable communication between nodes in a parallel computing environment.
Node
¶
A single machine within a cluster. A node can have multiple processors and its memory and storage.
Node Allocation
¶
The process of reserving a set of nodes for a specific job, ensuring that the required resources are available for successful execution.
Non-interactive job
¶
A non-interactive job (sbatch job) is a task submitted to a compute cluster that runs without the need for real-time user input. These jobs are defined by scripts that specify all the parameters and commands required for execution and do not provide a direct interface for user interaction during execution.
Open OnDemand (OOD)
¶
A web-based interface for accessing and managing HPC resources. It provides users a user-friendly way to submit jobs, manage files, and utilize cluster resources through a web browser.
Overcommitment
¶
Allowing more resources to be allocated to jobs than physically available, relying on intelligent scheduling and efficient resource management.
Package Manager
¶
A collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer in a consistent manner.
Parallel Computing
¶
A type of computation in which multiple calculations or processes are carried out simultaneously to solve a problem faster.
Partition
¶
A division of the cluster resources. Each partition can have different configurations, such as different types of nodes and different access policies.
Quota
¶
A quota limits the storage or computing resources allocated to a user or a project within an HPC cluster. Quotas help manage resource usage and prevent resource exhaustion.
Queue
¶
A waiting line for jobs ready to be executed but waiting for resources to become available.
Resource Reservation
¶
The process of specifying resources required for a job in advance to ensure availability and prevent resource conflicts.
Scaling efficiency
¶
Scaling efficiency measures how effectively a computing task can be parallelized across multiple processors or GPUs. It is calculated by comparing the execution time of a task on a single core with the time taken when using multiple cores. The formula for calculating scaling efficiency is:
\[S\eta = \frac{T_1}{N \times T_n}\]
Where:
\(S\eta\)
is the scaling efficiency
\(T_1\)
is the execution time on a single processor
\(N\)
is the number of processors used
\(T_n\)
is the execution time on N processors
For example, if
\(T_1 = 100\)
seconds,
\(N = 4\)
processors, and
\(T_n = 25\)
seconds, the scaling efficiency would be:
\[S\eta = \frac{100}{4 \times 25} = 1\]
A scaling efficiency of 1 (or 100%) indicates perfect linear scaling, which is the ideal case where using N processors reduces the execution time by a factor of N.
Scheduling Policy
A set of rules and algorithms used by the scheduler to determine the order in which jobs are executed based on their priority, resource requirements, and other factors.
Scratch Space
Temporary storage that allows users to store intermediate data during job execution. Data in scratch space is not preserved between jobs.
Storage Cluster
A set of networked storage devices used to provide centralized and scalable storage solutions for the HPC environment.
Scheduler
A program that manages the cluster’s resources and allocates them to jobs based on priority, requested resources, and fair use policies.
Singularity
A containerization platform commonly used in HPC environments. It allows users to create and run containers focusing on security and compatibility, making it suitable for running scientific applications.
Slurm
An open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small HPC clusters.
Task
A unit of work within a job that can be executed independently. A job can consist of multiple tasks.
VPN
A technology that creates a secure and encrypted connection over a public network, such as the Internet. It often provides remote access to HPC clusters, ensuring data privacy and security during remote cluster interactions.
This glossary is not exhaustive. If you come across a term not listed here, please check the specific section of the documentation or ask in our User Community and Forums.
Next
Frequently Asked Questions (FAQs)
Previous
Cluster Usage
Copyright © 2024, RC
Made with
Furo

================================================================================

URL: https://rc-docs.northeastern.edu/en/latest/runningjobs/jobscheduling.html

Job Scheduling Policies and Priorities - NURC RTD
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark, in light mode
Auto light/dark, in dark mode
Hide navigation sidebar
Hide table of contents sidebar
Skip to content
Toggle site navigation sidebar
NURC RTD
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Visit NURC Homepage
User Guides
Connecting To Cluster
Toggle navigation of Connecting To Cluster
Mac
Windows
Linux
Running Jobs
Toggle navigation of Running Jobs
Understanding the Queuing System
Job Scheduling Policies and Priorities
Interactive and Batch Mode
Running Jobs with job-assist
Working with GPUs
Toggle navigation of Working with GPUs
GPUs on the HPC
GPU Access
GPU Job Submission
Access to Multi-GPU Partition
Data Management
Toggle navigation of Data Management
Data Storage Options
Transfer Data
Using Globus
Software
Toggle navigation of Software
System Wide
Toggle navigation of System Wide
Modules
MPI
R
Matlab
Package Managers
Toggle navigation of Package Managers
Conda
Spack
From Source
Toggle navigation of From Source
Make
CMake
Slurm
Toggle navigation of Slurm
Slurm Commands
Monitoring and Managing Jobs
Slurm Jobs Array
HPC for the Classroom
Toggle navigation of HPC for the Classroom
Course Guide
Courses Cheatsheet
CPS Class Instructions
Containers on HPC
Toggle navigation of Containers on HPC
Singularity on Discovery
Best Practices
Toggle navigation of Best Practices
Home Directory Storage Quota
Work Directory Storage Quota
Scratch Directory Purge
Checkpointing Jobs
Shell Environment on the Cluster
Cluster Usage
Glossary
Frequently Asked Questions (FAQs)
Back to top
View this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Job Scheduling Policies and Priorities
¶
In an HPC environment, efficient job scheduling is crucial for allocating computing resources and ensuring optimal cluster utilization. Job scheduling policies and priorities determine the order in which jobs are executed and the resources they receive. Understanding these policies is essential for maximizing job efficiency and minimizing wait times.
Scheduling Policies
¶
FIFO (First-In-First-Out)
¶
Jobs are executed in the order they are submitted. Although simple, this policy may lead to long wait times for large, resource-intensive jobs if smaller jobs are constantly being submitted.
Fair Share
¶
This policy ensures that all users receive a fair share of cluster resources over time. Users with high resource usage may experience reduced priority, allowing others to access resources more regularly.
Priority-Based
¶
Jobs are assigned priorities based on user-defined criteria or system-wide rules. Higher-priority jobs are executed before lower-priority ones, allowing for resource allocation based on user requirements.
Job Priorities
¶
User Priority
¶
Users can assign priority values to their jobs. Higher values result in increased job priority and faster access to resources.
Resource Requirements
¶
Jobs with larger resource requirements may be assigned higher priority, as they require more significant resources to execute efficiently.
Walltime Limit
¶
Jobs with shorter estimated execution times may receive higher priority, ensuring they are executed promptly and freeing up resources for other jobs.
Balancing Policies
¶
Backfilling
¶
This policy allows smaller jobs to “backfill” into available resources ahead of larger jobs, optimizing resource utilization and reducing wait times.
Preemption
¶
Higher-priority jobs can preempt lower-priority ones, temporarily pausing the lower-priority job’s execution to make resources available for the higher-priority job.
Best Practices
¶
Set Realistic Priorities
: Assign accurate priorities to your jobs to reflect their importance and resource requirements.
Use Resource Quotas
: Be mindful of the resources you request to prevent over- or underutilization.
Leverage
Backfilling
: Submit smaller, shorter jobs that can backfill into available resources while waiting for larger jobs to start.
Understanding these scheduling policies and priorities empowers you to make informed decisions when submitting jobs, ensuring that your computational tasks are executed efficiently and promptly. If you need further guidance on selecting the right scheduling policy for your job or optimizing your resource usage, our support team is available at
rchelp
@
northeastern
.
edu
or consult our
Frequently Asked Questions (FAQs)
.
Optimize your job execution by maximizing our cluster’s scheduling capabilities. Happy computing!
Next
Interactive and Batch Mode
Previous
Understanding the Queuing System
Copyright © 2024, RC
Made with
Furo
On this page
Job Scheduling Policies and Priorities
Scheduling Policies
FIFO (First-In-First-Out)
Fair Share
Priority-Based
Job Priorities
User Priority
Resource Requirements
Walltime Limit
Balancing Policies
Backfilling
Preemption
Best Practices

================================================================================

